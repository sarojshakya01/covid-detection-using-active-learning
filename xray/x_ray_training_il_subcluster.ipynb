{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os, math, random\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from statistics import mode\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n",
    "from sklearn.metrics import auc, roc_curve, davies_bouldin_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 1 #int(input(\"Enter the number for: \\n 1) VGG16 \\n 2) Resnet101  \\n 3) Densenet169 \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For traning speed, define DATASET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open extracted feature in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../pickle_files/al/x_ray/\"\n",
    "if selected_model == 1:\n",
    "  filename = \"x_ray_pca_vgg16.pickle\"\n",
    "elif selected_model == 2:\n",
    "  filename = \"x_ray_resnet101.pickle\"\n",
    "elif selected_model == 3:\n",
    "  filename = \"x_ray_densenet169.pickle\"\n",
    "\n",
    "file = filepath + filename\n",
    "with open(file, 'rb') as handle:\n",
    "  all_ft_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the data and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset: 4400\n",
      "Sample feature dataset Covid: {'id': 3094, 'filepath': '../../dataset/xray/resized\\\\Noncovid\\\\NORMAL(447).png', 'image': array([ 6.2504688e+02, -4.0303204e+01,  9.9337494e+01, ...,\n",
      "        7.3082373e-11,  1.7737714e-10,  7.5504554e-11], dtype=float32), 'label': 0}\n",
      "Sample feature dataset Non-Covid: {'id': 4033, 'filepath': '../../dataset/xray/resized\\\\Noncovid\\\\PNEUMONIA(392).jpg', 'image': array([ 2.9877567e+02, -2.0280376e+02, -2.0804192e+02, ...,\n",
      "        7.2032998e-11,  1.8084165e-10,  7.4162385e-11], dtype=float32), 'label': 0}\n",
      "Selected Dataset: 4400\n"
     ]
    }
   ],
   "source": [
    "# suffle the data\n",
    "random.seed(42)\n",
    "all_ft_dataset = all_ft_dataset[:4400]\n",
    "shuffle(all_ft_dataset)\n",
    "\n",
    "print(\"Total Dataset: {}\".format(len(all_ft_dataset)))\n",
    "print(\"Sample feature dataset Covid: {}\".format(all_ft_dataset[0]))\n",
    "print(\"Sample feature dataset Non-Covid: {}\".format(all_ft_dataset[2325]))\n",
    "\n",
    "# shrink for minimize training time\n",
    "original_data_size = len(all_ft_dataset)\n",
    "new_data_size = int(original_data_size * DATASET_SIZE)\n",
    "ft_dataset = all_ft_dataset[:new_data_size]\n",
    "\n",
    "print(\"Selected Dataset: {}\".format(len(ft_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the mean of each subclusters. (required as mean is the representative of that subcluster)\n",
    "def mean_features(c_pos_features, c_neg_features):\n",
    "  mpos_features = np.array([np.mean(i, axis=0) for i in c_pos_features])  # Mean of all positive subclusters\n",
    "  mneg_features = np.array([np.mean(i, axis=0) for i in c_neg_features])  # Mean of all negative subclusters\n",
    "  return mpos_features, mneg_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update subcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that updates the subcluster by concatenating the test data sample to the most similar subcluster.\n",
    "def update_subclusters(query, closest_labels_from_model, id_pred, label_pred, n_neighbours, c_features, distances_of_data_to_cc, cluster_index):\n",
    "  nearest_subcluster_index = np.argmin(distances_of_data_to_cc) # find nearest subcluster of the query\n",
    "  # add query data to the nearest/most-similar subcluster\n",
    "  c_features[nearest_subcluster_index] = np.concatenate((c_features[nearest_subcluster_index], np.expand_dims(query[\"image\"], axis=0)), axis=0)\n",
    "\n",
    "  id_pred[cluster_index].append(query[\"id\"])\n",
    "  # closest_labels_from_model.count(1)/n_neighbours --> Percentage that the model predict the data as positive (required to calculate AUC ROC value)\n",
    "  label_pred[cluster_index].append((query['id'], closest_labels_from_model.count(1)/n_neighbours))\n",
    "  return c_features, id_pred, label_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the class (pos or neg) from max frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_with_max_freq(closest_labels_from_model):\n",
    "    return mode(closest_labels_from_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to correct mispredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the model predicted label with the ground truth and corrects only if it is a mistake\n",
    "def correct_mispredictions(query, closest_labels_from_model, c_pos_features, c_neg_features, distances_of_data_to_neg_cc, distances_of_data_to_pos_cc, data_frame_mistake, mistake_index, corrected_count):\n",
    "  if get_label_with_max_freq(closest_labels_from_model) != query[\"label\"]:  # Misclassification: if model's decision is different than the ground truth.\n",
    "    corrected_count += 1\n",
    "    data_frame_mistake[\"Image name\"].append(query[\"filepath\"].split(\"/\")[-1])  # Recording to save it as csv file\n",
    "    data_frame_mistake[\"Mistake ID\"].append(query['id'])\n",
    "    data_frame_mistake[\"Original label\"].append(query['label'])\n",
    "    data_frame_mistake[\"Predicted label\"].append(get_label_with_max_freq(closest_labels_from_model))\n",
    "    data_frame_mistake[\"Mistake index\"].append(mistake_index)\n",
    "    if query[\"label\"] == 0:\n",
    "      c_neg_features.append(np.expand_dims(query[\"image\"], axis=0))  # Appending negative image to negative cluster\n",
    "    else:\n",
    "      c_pos_features.append(np.expand_dims(query[\"image\"], axis=0))  # Appending positive image to positive cluster\n",
    "\n",
    "  else: # Correct classification by model: concatenating the feature to the closest subsample.\n",
    "    if query['label'] == 0:\n",
    "      nearest_subcluster_index = np.argmin(distances_of_data_to_neg_cc)  # find nearest subcluster of the query\n",
    "      # add query data to the nearest/most-similar subcluster\n",
    "      c_neg_features[nearest_subcluster_index] = np.concatenate((c_neg_features[nearest_subcluster_index], np.expand_dims(query[\"image\"], axis=0)), axis=0)\n",
    "    else:\n",
    "      nearest_subcluster_index = np.argmin(distances_of_data_to_pos_cc)  # find nearest subcluster of the query\n",
    "      # add query data to the nearest/most-similar subcluster\n",
    "      c_pos_features[nearest_subcluster_index] = np.concatenate((c_pos_features[nearest_subcluster_index], np.expand_dims(query[\"image\"], axis=0)), axis=0)\n",
    "\n",
    "  return corrected_count, data_frame_mistake, c_pos_features, c_neg_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calcuate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: raw dictionary (from pickle file)\n",
    "# cluster_centers_dict: dictionary of {0: [], 1:[]} ==> {0: cc_neg_features, 1: cc_pos_features}\n",
    "# distance_type: 1. Eucliddean, 2. Manhattan, 3. Cosine\n",
    "# label_pred: predicted label\n",
    "# c_pos_features\n",
    "# c_neg_features\n",
    "# n_neighbours: no. of neighbour\n",
    "# corrected_count: count of coorection of mispredictions\n",
    "# mistake_index: index of data to track the mistaken data\n",
    "# data_frame_mistake: to save data_frame in CSV\n",
    "# mentored_data: if mentored data or not\n",
    "def distance(query,\n",
    "             cluster_centers_dict,\n",
    "             distance_type, id_pred,\n",
    "             label_pred,\n",
    "             c_pos_features,\n",
    "             c_neg_features,\n",
    "             n_neighbours,\n",
    "             corrected_count,\n",
    "             mistake_index,\n",
    "             data_frame_mistake,\n",
    "             mentored_data):\n",
    "  expnd_query = np.expand_dims(query['image'], axis=0)\n",
    "  distances_of_data_to_pos_cc, distances_of_data_to_neg_cc = [], []\n",
    "\n",
    "  # len(cluster_centers_dict[0]) should have at least the number of sub-cluster\n",
    "  # Calculating the distance using numpy (axis=1) to calculate all at ones\n",
    "  if distance_type == 1: # Euclidean distance\n",
    "    if len(cluster_centers_dict[0]) > 1:\n",
    "      distances_of_data_to_neg_cc = np.linalg.norm(query['image'] - cluster_centers_dict[0], axis=1)\n",
    "    elif len(cluster_centers_dict[0]) == 1:\n",
    "      distances_of_data_to_neg_cc = [np.linalg.norm(query['image'] - cluster_centers_dict[0], axis=1)]\n",
    "    if len(cluster_centers_dict[1]) > 1:\n",
    "      distances_of_data_to_pos_cc = np.linalg.norm(query['image'] - cluster_centers_dict[1], axis=1)\n",
    "    elif len(cluster_centers_dict[1]) == 1:\n",
    "      distances_of_data_to_pos_cc = [np.linalg.norm(query['image'] - cluster_centers_dict[1], axis=1)]\n",
    "\n",
    "  elif distance_type == 2: # Manhattan distance\n",
    "    if len(cluster_centers_dict[0]) > 1:\n",
    "      distances_of_data_to_neg_cc = np.squeeze(manhattan_distances(cluster_centers_dict[0], expnd_query))\n",
    "    elif len(cluster_centers_dict[0]) == 1:\n",
    "      distances_of_data_to_neg_cc = [np.squeeze(manhattan_distances(cluster_centers_dict[0], expnd_query))]\n",
    "    if len(cluster_centers_dict[1]) > 1:\n",
    "      distances_of_data_to_pos_cc = np.squeeze(manhattan_distances(cluster_centers_dict[1], expnd_query))\n",
    "    elif len(cluster_centers_dict[1]) == 1:\n",
    "      distances_of_data_to_pos_cc = [np.squeeze(manhattan_distances(cluster_centers_dict[1], expnd_query))]\n",
    "\n",
    "  elif distance_type == 3: # Cosine distance\n",
    "    if len(cluster_centers_dict[0]) > 1:\n",
    "      distances_of_data_to_neg_cc = np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[0]))\n",
    "    elif len(cluster_centers_dict[0]) == 1:\n",
    "      distances_of_data_to_neg_cc = [np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[0]))]\n",
    "    if len(cluster_centers_dict[1]) > 1:\n",
    "      distances_of_data_to_pos_cc = np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[1]))\n",
    "    elif len(cluster_centers_dict[1]) == 1:\n",
    "      distances_of_data_to_pos_cc = [np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[1]))]\n",
    "\n",
    "  pos_ditances_tup_list, neg_distances_tup_list = [], []\n",
    "  for dist_single in distances_of_data_to_pos_cc:\n",
    "    pos_ditances_tup_list.append((dist_single, 1))\n",
    "\n",
    "  for dist_single in distances_of_data_to_neg_cc:\n",
    "    neg_distances_tup_list.append((dist_single, 0))\n",
    "\n",
    "  # concat all distances\n",
    "  pos_ditances_tup_list.extend(neg_distances_tup_list)\n",
    "  # sort distances from min to max result: ((0.1, 1), (0.2, 1), (0.3, 0), (0.4, 0), (0.5, 1))\n",
    "  all_distances_tup = sorted(pos_ditances_tup_list)[:n_neighbours]\n",
    "\n",
    "  # filter only n_neighbours elements\n",
    "  # all_distances_tup = all_distances_tup[:n_neighbours]\n",
    "\n",
    "  closest_labels_from_model = [label for (distance, label) in all_distances_tup]\n",
    "\n",
    "  if mentored_data:\n",
    "    (corrected_count,\n",
    "     data_frame_mistake,\n",
    "     c_pos_features,\n",
    "     c_neg_features) = correct_mispredictions(query,\n",
    "                                              closest_labels_from_model,\n",
    "                                              c_pos_features,\n",
    "                                              c_neg_features,\n",
    "                                              distances_of_data_to_neg_cc,\n",
    "                                              distances_of_data_to_pos_cc,\n",
    "                                              data_frame_mistake,\n",
    "                                              mistake_index,\n",
    "                                              corrected_count\n",
    "                                              )\n",
    "\n",
    "  else:\n",
    "    # label from model is negative\n",
    "    if len(closest_labels_from_model) > 0 and get_label_with_max_freq(closest_labels_from_model) == 0:\n",
    "      c_neg_features, id_pred, label_pred = update_subclusters(query,\n",
    "                                                               closest_labels_from_model,\n",
    "                                                               id_pred,\n",
    "                                                               label_pred,\n",
    "                                                               n_neighbours,\n",
    "                                                               # neg params\n",
    "                                                               c_neg_features,\n",
    "                                                               distances_of_data_to_neg_cc,\n",
    "                                                               cluster_index=0\n",
    "                                                              )\n",
    "    else:\n",
    "      # label from model is positive\n",
    "      c_pos_features, id_pred, label_pred = update_subclusters(query,\n",
    "                                                               closest_labels_from_model,\n",
    "                                                               id_pred,\n",
    "                                                               label_pred,\n",
    "                                                               n_neighbours,\n",
    "                                                               # pos params\n",
    "                                                               c_pos_features,\n",
    "                                                               distances_of_data_to_pos_cc,\n",
    "                                                               cluster_index=1\n",
    "                                                              )\n",
    "\n",
    "  return data_frame_mistake, corrected_count, id_pred, label_pred, c_pos_features, c_neg_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate the classification metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrices(id_gt, id_pred):\n",
    "  TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "  # TP --> when correctly classified covid\n",
    "  for tp in id_pred[1]:\n",
    "    if tp in id_gt[1]:\n",
    "      TP += 1\n",
    "\n",
    "  # TN --> when correctly classified healthy (non-covid)\n",
    "  for tn in id_pred[0]:\n",
    "    if tn in id_gt[0]:\n",
    "      TN += 1\n",
    "\n",
    "  # FP --> when incorrectly classified healthy (Classified healthy as covid)\n",
    "  for fp in id_pred[1]:\n",
    "    if fp in id_gt[0]:\n",
    "      FP += 1\n",
    "\n",
    "  # FN --> when missed covid classification (Covid cases missed)\n",
    "  for fn in id_pred[0]:\n",
    "    if fn in id_gt[1]:\n",
    "      FN += 1\n",
    "\n",
    "  accuracy = round((TP + TN) / (TP + TN + FP + FN), 3)\n",
    "  if (TN + FP) > 0:\n",
    "    specificity = round(TN / (TN + FP), 3)\n",
    "  else:\n",
    "    specificity = 0 # Infinity\n",
    "\n",
    "  if (TP + FN) > 0:\n",
    "    sensitivity = round((TP) / (TP + FN), 3)\n",
    "  else:\n",
    "    sensitivity = 0 # Infinity\n",
    "\n",
    "  # f1_score = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "  print(\"TP: {}  FP: {}\".format(TP, FP))\n",
    "  print(\"FN: {}  TN: {}\".format(FN, TN))\n",
    "\n",
    "  return accuracy, specificity, sensitivity, TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_curve(label_gt, label_pred):\n",
    "  # contains (id, labels) tuple of binary class\n",
    "  gt_labels = sorted(label_gt[0] + label_gt[1])\n",
    "\n",
    "  # contains (id, labels) tuple of binary class --> sorted to match each element in gt_labels and pred_labels\n",
    "  pred_labels = sorted(label_pred[0] + label_pred[1])\n",
    "  y_test = [y for (x,y) in gt_labels] # Get only the labels\n",
    "  y_scores = [y for (x,y) in pred_labels]\n",
    "  fpr, tpr, threshold = roc_curve(y_test, y_scores)\n",
    "  roc_auc = round(auc(fpr, tpr), 3)\n",
    "  return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate Cluster metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_metrices(neg_features, pos_features):\n",
    "  print(\"Calculating Dunn's index...\")\n",
    "  dunn_index, davies_bouldin_index, silhouette_index = \"NA\", \"NA\", \"NA\"\n",
    "  if len(neg_features) > 0 and len(pos_features) > 0:\n",
    "    intra_dist1 = euclidean_distances(neg_features).max()\n",
    "    intra_dist2 = euclidean_distances(pos_features).max()\n",
    "    inter_dist = euclidean_distances(neg_features, pos_features).min()\n",
    "\n",
    "    if intra_dist1 > intra_dist2:\n",
    "      max_intra_dist= intra_dist1\n",
    "    else:\n",
    "      max_intra_dist = intra_dist2\n",
    "\n",
    "    dunn_index = round(inter_dist / max_intra_dist, 3)\n",
    "\n",
    "  print(\"dunn_index: \", dunn_index)\n",
    "\n",
    "  # Davies Bouldin and Silhouette score from sklearn library.\n",
    "  if len(neg_features) > 0 and len(pos_features) > 0:\n",
    "    feature_all = np.concatenate((neg_features, pos_features))\n",
    "    neg_labels = np.zeros(shape=(len(neg_features)),dtype=int)\n",
    "    pos_labels = np.ones(shape=(len(pos_features)),dtype=int)\n",
    "\n",
    "    label_all = np.concatenate((neg_labels, pos_labels))\n",
    "    print(\"Calculating Davies Bouldin index...\")\n",
    "    davies_bouldin_index = round(davies_bouldin_score(feature_all, label_all), 3)\n",
    "    print(\"davies_bouldin_index: \", davies_bouldin_index)\n",
    "\n",
    "    print(\"Calculating Silhouette index...\")\n",
    "    silhouette_index = round(silhouette_score(feature_all, label_all), 3)\n",
    "    print(\"silhouette_index: \", silhouette_index)\n",
    "\n",
    "  return dunn_index, davies_bouldin_index, silhouette_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to flatter the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to calculate the clustering indices\n",
    "def flatten_features(features):\n",
    "  all_features = []\n",
    "  for feature in features:\n",
    "    for index in feature:\n",
    "      all_features.append(index)\n",
    "  return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find sub cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create subclusters\n",
    "def sub_clusters(features, n_clusters=5):\n",
    "  # Number of cluster defined from elbow method\n",
    "  # kmeans = KMeans(n_clusters=int(70*DATASET_SIZE), random_state=0, n_init=\"auto\").fit(features)\n",
    "  kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(features)\n",
    "  # list of labels for elements occuring in each cluster\n",
    "  out_labels = kmeans.labels_\n",
    "  # Form clusters of deep features of image\n",
    "  # clusters = [np.squeeze(np.array(features)[[np.where(out_labels == i)[0]]], axis=0) for i in range(len(np.unique(out_labels)))]\n",
    "  clusters = [np.array(features)[np.where(out_labels == i)[0]] for i in range(len(np.unique(out_labels)))]\n",
    "  return kmeans.cluster_centers_, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to load dataset into three different segment (k-way n-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to return three sets (n, 1500, 3000)  of labeled dataset for experiment\n",
    "def data_loader(dataset, n):\n",
    "  labeled_data, unlabeled_data = [], []\n",
    "\n",
    "  l_data = dataset[:n]                          # First case (0-40) // labeled + mentored\n",
    "  ul_data = dataset[n:]                         # First case (40-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[0] => dataset[0-40]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[0] => dataset[40-1000]\n",
    "\n",
    "  size_second_set = int(1500 * DATASET_SIZE) # 1500 * 0.1 = 150\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Second case (150-190) // labeled + mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Second case (0-150) + (190-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[1] => dataset[150-190]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[1] => dataset[0-150] + dataset[190-1000]\n",
    "\n",
    "  size_second_set = int(3000 * DATASET_SIZE) # 3000 * 0.1 = 300\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Third case (300-340) // labeled + mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Third case (0-300) + (340-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[2] => dataset[300-340]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[2] => dataset[0-300] + dataset[340-1000]\n",
    "  return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to separate data into positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that selects number of data samples and removes the selected data from the dataset. (required to select balanced positive and negative samples)\n",
    "def data_separation2(dataset, taken_data_idx, label=None, data_sample=100):\n",
    "  add_data = []\n",
    "  for i, data in enumerate(dataset):\n",
    "    if i in taken_data_idx:\n",
    "      continue\n",
    "    if label > -1 and dataset[i][\"label\"] == label:\n",
    "      add_data.append(data)\n",
    "      taken_data_idx.append(i)\n",
    "    else:\n",
    "      add_data.append(data)\n",
    "      taken_data_idx.append(i)\n",
    "    if len(add_data) == data_sample:\n",
    "      break\n",
    "  return add_data, taken_data_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative function to separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that selects number of data samples and removes the selected data from the dataset. (required to select balanced positive and negative samples)\n",
    "def data_separation(dataset, label, data_sample=100):\n",
    "  add_data = []\n",
    "  for i, data in enumerate(dataset):\n",
    "    if data[\"label\"] == label:\n",
    "      add_data.append(data['image'])\n",
    "      del dataset[i]\n",
    "    if len(add_data) == data_sample:\n",
    "      break\n",
    "\n",
    "  return add_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance type (Euclidean Manhattan or Consine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_type = 3 # int(input(\"Enter the number for: \\n 1) Euclidean  \\n 2) Manhattan \\n 3) Cosine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Distance Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model == 1:\n",
    "  s_model = 'vgg16'\n",
    "elif selected_model == 2:\n",
    "  s_model = 'resnet101'\n",
    "elif selected_model == 3:\n",
    "  s_model = 'densenet169'\n",
    "\n",
    "if distance_type == 1:\n",
    "  s_distance = 'euclidean'\n",
    "elif distance_type == 2:\n",
    "  s_distance = 'manhattan'\n",
    "elif distance_type == 3:\n",
    "  s_distance = 'cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** training with 200 size of labled data***************\n",
      "============================== 1/12 ==============================\n",
      "labeled data: 200, unlabled data: 4200\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 160\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 160 data ###############\n",
      " ############### Mentoring 160 data DONE!!! ###############\n",
      " ############### Training 4200 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4200/4200 [00:28<00:00, 148.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4200 unlabeled data DONE!!! ###############\n",
      "TP: 2010  FP: 249\n",
      "FN: 19  TN: 1922\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.162\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.599\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.124\n",
      "Dataset: d_1 \t\t\t Labeled image: 200 \t\t Corrected count: 8\n",
      "Accuracy: 0.936 \t\t Specificity: 0.885 \t\t Sensitivity: 0.991\n",
      "Dunn index: 0.16200000047683716\n",
      "Davies Bouldin: 2.599\n",
      "Silhouette index: 0.12399999797344208\n",
      "AUC: 0.94\n",
      "============================== 2/12 ==============================\n",
      "labeled data: 200, unlabled data: 4200\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 160\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 160 data ###############\n",
      " ############### Mentoring 160 data DONE!!! ###############\n",
      " ############### Training 4200 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4200/4200 [00:28<00:00, 148.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4200 unlabeled data DONE!!! ###############\n",
      "TP: 2001  FP: 225\n",
      "FN: 22  TN: 1952\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.131\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.605\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.123\n",
      "Dataset: d_2 \t\t\t Labeled image: 200 \t\t Corrected count: 8\n",
      "Accuracy: 0.941 \t\t Specificity: 0.897 \t\t Sensitivity: 0.989\n",
      "Dunn index: 0.13099999725818634\n",
      "Davies Bouldin: 2.605\n",
      "Silhouette index: 0.12300000339746475\n",
      "AUC: 0.946\n",
      "============================== 3/12 ==============================\n",
      "labeled data: 200, unlabled data: 4200\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 160\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 160 data ###############\n",
      " ############### Mentoring 160 data DONE!!! ###############\n",
      " ############### Training 4200 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4200/4200 [00:29<00:00, 144.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4200 unlabeled data DONE!!! ###############\n",
      "TP: 2004  FP: 37\n",
      "FN: 21  TN: 2138\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.121\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.659\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.114\n",
      "Dataset: d_3 \t\t\t Labeled image: 200 \t\t Corrected count: 4\n",
      "Accuracy: 0.986 \t\t Specificity: 0.983 \t\t Sensitivity: 0.99\n",
      "Dunn index: 0.12099999934434891\n",
      "Davies Bouldin: 2.659\n",
      "Silhouette index: 0.11400000005960464\n",
      "AUC: 0.5\n",
      "*************** training with 400 size of labled data***************\n",
      "============================== 4/12 ==============================\n",
      "labeled data: 400, unlabled data: 4000\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 360\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 360 data ###############\n",
      " ############### Mentoring 360 data DONE!!! ###############\n",
      " ############### Training 4000 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:28<00:00, 141.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4000 unlabeled data DONE!!! ###############\n",
      "TP: 1910  FP: 53\n",
      "FN: 27  TN: 2010\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.0\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.652\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.114\n",
      "Dataset: d_1 \t\t\t Labeled image: 400 \t\t Corrected count: 15\n",
      "Accuracy: 0.98 \t\t Specificity: 0.974 \t\t Sensitivity: 0.986\n",
      "Dunn index: 0.0\n",
      "Davies Bouldin: 2.652\n",
      "Silhouette index: 0.11400000005960464\n",
      "AUC: 0.995\n",
      "============================== 5/12 ==============================\n",
      "labeled data: 400, unlabled data: 4000\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 360\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 360 data ###############\n",
      " ############### Mentoring 360 data DONE!!! ###############\n",
      " ############### Training 4000 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:28<00:00, 142.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4000 unlabeled data DONE!!! ###############\n",
      "TP: 1904  FP: 65\n",
      "FN: 23  TN: 2008\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.035\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.646\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.115\n",
      "Dataset: d_2 \t\t\t Labeled image: 400 \t\t Corrected count: 19\n",
      "Accuracy: 0.978 \t\t Specificity: 0.969 \t\t Sensitivity: 0.988\n",
      "Dunn index: 0.03500000014901161\n",
      "Davies Bouldin: 2.646\n",
      "Silhouette index: 0.11500000208616257\n",
      "AUC: 0.997\n",
      "============================== 6/12 ==============================\n",
      "labeled data: 400, unlabled data: 4000\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 360\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 360 data ###############\n",
      " ############### Mentoring 360 data DONE!!! ###############\n",
      " ############### Training 4000 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:28<00:00, 138.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4000 unlabeled data DONE!!! ###############\n",
      "TP: 1901  FP: 140\n",
      "FN: 27  TN: 1932\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.162\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.628\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.119\n",
      "Dataset: d_3 \t\t\t Labeled image: 400 \t\t Corrected count: 13\n",
      "Accuracy: 0.958 \t\t Specificity: 0.932 \t\t Sensitivity: 0.986\n",
      "Dunn index: 0.16200000047683716\n",
      "Davies Bouldin: 2.628\n",
      "Silhouette index: 0.11900000274181366\n",
      "AUC: 0.992\n",
      "*************** training with 800 size of labled data***************\n",
      "============================== 7/12 ==============================\n",
      "labeled data: 800, unlabled data: 3600\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 760\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 760 data ###############\n",
      " ############### Mentoring 760 data DONE!!! ###############\n",
      " ############### Training 3600 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3600/3600 [00:26<00:00, 133.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 3600 unlabeled data DONE!!! ###############\n",
      "TP: 1724  FP: 65\n",
      "FN: 18  TN: 1793\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.133\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.652\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.115\n",
      "Dataset: d_1 \t\t\t Labeled image: 800 \t\t Corrected count: 21\n",
      "Accuracy: 0.977 \t\t Specificity: 0.965 \t\t Sensitivity: 0.99\n",
      "Dunn index: 0.13300000131130219\n",
      "Davies Bouldin: 2.652\n",
      "Silhouette index: 0.11500000208616257\n",
      "AUC: 0.995\n",
      "============================== 8/12 ==============================\n",
      "labeled data: 800, unlabled data: 3600\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 760\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 760 data ###############\n",
      " ############### Mentoring 760 data DONE!!! ###############\n",
      " ############### Training 3600 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3600/3600 [00:27<00:00, 130.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 3600 unlabeled data DONE!!! ###############\n",
      "TP: 1728  FP: 51\n",
      "FN: 12  TN: 1809\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.035\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.65\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.115\n",
      "Dataset: d_2 \t\t\t Labeled image: 800 \t\t Corrected count: 26\n",
      "Accuracy: 0.983 \t\t Specificity: 0.973 \t\t Sensitivity: 0.993\n",
      "Dunn index: 0.03500000014901161\n",
      "Davies Bouldin: 2.65\n",
      "Silhouette index: 0.11500000208616257\n",
      "AUC: 0.998\n",
      "============================== 9/12 ==============================\n",
      "labeled data: 800, unlabled data: 3600\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 760\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 760 data ###############\n",
      " ############### Mentoring 760 data DONE!!! ###############\n",
      " ############### Training 3600 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3600/3600 [00:28<00:00, 127.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 3600 unlabeled data DONE!!! ###############\n",
      "TP: 1708  FP: 84\n",
      "FN: 23  TN: 1785\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.0\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.64\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.116\n",
      "Dataset: d_3 \t\t\t Labeled image: 800 \t\t Corrected count: 22\n",
      "Accuracy: 0.97 \t\t Specificity: 0.955 \t\t Sensitivity: 0.987\n",
      "Dunn index: 0.0\n",
      "Davies Bouldin: 2.64\n",
      "Silhouette index: 0.11599999666213989\n",
      "AUC: 0.997\n",
      "*************** training with 1550 size of labled data***************\n",
      "============================== 10/12 ==============================\n",
      "labeled data: 1550, unlabled data: 2850\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 1510\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 1510 data ###############\n",
      " ############### Mentoring 1510 data DONE!!! ###############\n",
      " ############### Training 2850 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2850/2850 [00:25<00:00, 112.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 2850 unlabeled data DONE!!! ###############\n",
      "TP: 1357  FP: 17\n",
      "FN: 22  TN: 1454\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.128\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.667\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.112\n",
      "Dataset: d_1 \t\t\t Labeled image: 1550 \t\t Corrected count: 38\n",
      "Accuracy: 0.986 \t\t Specificity: 0.988 \t\t Sensitivity: 0.984\n",
      "Dunn index: 0.12800000607967377\n",
      "Davies Bouldin: 2.667\n",
      "Silhouette index: 0.1120000034570694\n",
      "AUC: 0.999\n",
      "============================== 11/12 ==============================\n",
      "labeled data: 1550, unlabled data: 2850\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 1510\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 1510 data ###############\n",
      " ############### Mentoring 1510 data DONE!!! ###############\n",
      " ############### Training 2850 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2850/2850 [00:25<00:00, 112.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 2850 unlabeled data DONE!!! ###############\n",
      "TP: 1377  FP: 20\n",
      "FN: 12  TN: 1441\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.129\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.667\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.113\n",
      "Dataset: d_2 \t\t\t Labeled image: 1550 \t\t Corrected count: 38\n",
      "Accuracy: 0.989 \t\t Specificity: 0.986 \t\t Sensitivity: 0.991\n",
      "Dunn index: 0.1289999932050705\n",
      "Davies Bouldin: 2.667\n",
      "Silhouette index: 0.11299999803304672\n",
      "AUC: 0.999\n",
      "============================== 12/12 ==============================\n",
      "labeled data: 1400, unlabled data: 3000\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 1360\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 1360 data ###############\n",
      " ############### Mentoring 1360 data DONE!!! ###############\n",
      " ############### Training 3000 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:28<00:00, 106.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 3000 unlabeled data DONE!!! ###############\n",
      "TP: 1414  FP: 16\n",
      "FN: 20  TN: 1550\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.111\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.673\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.112\n",
      "Dataset: d_3 \t\t\t Labeled image: 1550 \t\t Corrected count: 41\n",
      "Accuracy: 0.988 \t\t Specificity: 0.99 \t\t Sensitivity: 0.986\n",
      "Dunn index: 0.11100000143051147\n",
      "Davies Bouldin: 2.673\n",
      "Silhouette index: 0.1120000034570694\n",
      "AUC: 0.999\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME =  str(round(datetime.datetime.now().timestamp()))\n",
    "os.mkdir(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}\")\n",
    "n_neighbours = int(15 * DATASET_SIZE)\n",
    "labeled_size = [200, 400, 800, 1550]\n",
    "labeled_size = [int(size * DATASET_SIZE) for size in labeled_size]\n",
    "data_frame_metrix = {\n",
    "  \"Labeled data\": [],\n",
    "  \"Dataset\": [],\n",
    "  \"Accuracy\": [],\n",
    "  \"Specificity\": [],\n",
    "  \"Sensitivity\": [],\n",
    "  \"AUC\":[],\n",
    "  \"Dunn index\": [],\n",
    "  \"Davies Bouldin\": [],\n",
    "  \"Silhouette index\":[],\n",
    "  \"TP\":[],\n",
    "  \"TN\":[],\n",
    "  \"FP\":[],\n",
    "  \"FN\":[],\n",
    "  \"pos_labeled_img\":[],\n",
    "  \"neg_labeled_img\":[],\n",
    "  \"corrected_count\":[]\n",
    "}\n",
    "\n",
    "global_count = 0\n",
    "\n",
    "for size in labeled_size:\n",
    "  print(\"{} training with {} size of labled data{}\".format('*'*15, size, '*'*15))\n",
    "  labeled_data_sets, unlabeled_data_sets = data_loader(ft_dataset, size)\n",
    "\n",
    "  # labeled_data_sets ==> three sets: [d1, d2, d3] ==> eg: [0-40, 320-360, 640-680]\n",
    "  for dataset_type, labeled_data in enumerate(labeled_data_sets):\n",
    "    global_count += 1\n",
    "    print(f\"============================== {global_count}/{len(labeled_size) * len(labeled_data_sets)} ==============================\")\n",
    "    data_frame_mistake = {\n",
    "      \"Image name\": [],\n",
    "      \"Mistake index\": [],\n",
    "      \"Mistake ID\": [],\n",
    "      \"Original label\": [],\n",
    "      \"Predicted label\": []\n",
    "    }\n",
    "\n",
    "    pos_img, neg_img = 0, 0\n",
    "\n",
    "    # collect the ground truth (label) of all the predicting images =>> key: 0 & 1 (class), value: tuple (data['id'], data['label']), required to calulate TP, FP, FN, TN\n",
    "    label_gt = {0: [], 1: []}\n",
    "    # collect the ground truth (id) of all the predicting images =>> key: 0 & 1 (class), value: ground truth id\n",
    "    id_gt = {0: [], 1: []}\n",
    "\n",
    "    # collect the predicted label for all the images =>> key: 0 & 1 (class), value: tuple(query['id'], decision_list.count(1)/n_neighbours)\n",
    "    # Percentage of predicted positive class, required to calculate AUC/ROC value\n",
    "    label_pred = {0: [], 1: []}\n",
    "    # collect the predicted id for all the images =>> key: 0 & 1 (class), value: predicted ids, required to calulate TP, FP, FN, TN\n",
    "    id_pred = {0: [],  1: []}\n",
    "\n",
    "    # feature label =>> key: 0 & 1 (class), value: deep feature of image\n",
    "    cluster_centers_dict = {0: [], 1: []}\n",
    "\n",
    "    print(f\"labeled data: {len(labeled_data)}, unlabled data: {len(unlabeled_data_sets[dataset_type])}\")\n",
    "\n",
    "    neg_labeled_img, pos_labeled_img = 0, 0\n",
    "    for data in labeled_data:\n",
    "        if data['label'] == 0:\n",
    "            neg_labeled_img += 1\n",
    "        else:\n",
    "            pos_labeled_img += 1\n",
    "\n",
    "    # select balanced labeled data (50% from positive and 50% from negative)\n",
    "    sample_size = int(20 * DATASET_SIZE) # sample size of balanced_data\n",
    "    fpositive = data_separation(labeled_data, 1, sample_size)  # Get the 'sample_size' positive features from 'labeled_data'\n",
    "    fnegative = data_separation(labeled_data, 0, sample_size)  # Get the 'sample_size' negative features from 'labeled_data'\n",
    "\n",
    "    print(f\"balanced data: {2 * sample_size}, fpositive: {len(fpositive)}, fnegative: {len(fnegative)}\")\n",
    "    print(f\"mentored data: {len(labeled_data)}\")\n",
    "\n",
    "    n_sub_clusters = math.ceil(5 * DATASET_SIZE) if DATASET_SIZE > 0.5 else 2\n",
    "    print(\"Number of subclusters: {}\".format(n_sub_clusters))\n",
    "    cc_neg_features, c_neg_features = sub_clusters(fnegative, n_sub_clusters)  # Get the cluster center and negative clusters (Using K-means algorithm)\n",
    "    cc_pos_features, c_pos_features = sub_clusters(fpositive, n_sub_clusters)  # Get the cluster center and positive clusters (Using K-means algorithm)\n",
    "\n",
    "    corrected_count, mistake_index = 0, 2 * sample_size\n",
    "\n",
    "    print(f\" {'#' * 15} Mentoring {len(labeled_data)} data {'#' * 15}\")\n",
    "    # loop is for the mentored data --> Notice mentored_data=True in argument of the function call distance.\n",
    "    for data in labeled_data:\n",
    "      cluster_centers_dict = {0: cc_neg_features, 1: cc_pos_features}\n",
    "\n",
    "      (data_frame_mistake,\n",
    "       corrected_count,\n",
    "       _,\n",
    "       label_pred,\n",
    "       c_pos_features,\n",
    "       c_neg_features) = distance(data,\n",
    "                                 cluster_centers_dict,\n",
    "                                 distance_type,\n",
    "                                 id_pred, # not being used in this case\n",
    "                                 label_pred,\n",
    "                                 c_pos_features,\n",
    "                                 c_neg_features,\n",
    "                                 n_neighbours,\n",
    "                                 corrected_count,\n",
    "                                 mistake_index,\n",
    "                                 data_frame_mistake,\n",
    "                                 mentored_data=True)\n",
    "\n",
    "      cc_pos_features, cc_neg_features = mean_features(c_pos_features, c_neg_features)  # Get the mean of the features\n",
    "      mistake_index += 1\n",
    "\n",
    "    print(f\" {'#' * 15} Mentoring {len(labeled_data)} data DONE!!! {'#' * 15}\")\n",
    "\n",
    "    data_f_mistake = pd.DataFrame.from_dict(data_frame_mistake)\n",
    "    data_f_mistake.to_csv(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}/mistake_{size}_d{dataset_type + 1}.csv\", index=False)\n",
    "\n",
    "    print(f\" {'#' * 15} Training {len(unlabeled_data_sets[dataset_type])} unlabeled data {'#' * 15}\")\n",
    "    # loop is for the test data --> Notice mentored_data=False in argument of the function call distance.\n",
    "    for data in tqdm(unlabeled_data_sets[dataset_type]):\n",
    "      if data[\"label\"] == 1:\n",
    "        id_gt[1].append(data['id'])\n",
    "        label_gt[1].append((data['id'], data['label'])) # Required to calulate TP, FP, FN, TN\n",
    "      else:\n",
    "        id_gt[0].append(data['id'])\n",
    "        label_gt[0].append((data['id'], data['label']))\n",
    "\n",
    "      cluster_centers_dict = {0: cc_neg_features, 1: cc_pos_features}\n",
    "\n",
    "      (_,\n",
    "       _,\n",
    "       id_pred,\n",
    "       label_pred,\n",
    "       c_pos_features,\n",
    "       c_neg_features) = distance(data,\n",
    "                                 cluster_centers_dict,\n",
    "                                 distance_type,\n",
    "                                 id_pred,\n",
    "                                 label_pred,\n",
    "                                 c_pos_features,\n",
    "                                 c_neg_features,\n",
    "                                 n_neighbours,\n",
    "                                 corrected_count, # not being used in this case\n",
    "                                 mistake_index,\n",
    "                                 data_frame_mistake,\n",
    "                                 mentored_data=False)\n",
    "\n",
    "      cc_pos_features, cc_neg_features = mean_features(c_pos_features, c_neg_features)   # Get the mean of the features\n",
    "\n",
    "    print(f\" {'#' * 15} Training {len(unlabeled_data_sets[dataset_type])} unlabeled data DONE!!! {'#' * 15}\")\n",
    "\n",
    "    accuracy, specificity, sensitivity, TP, TN, FP, FN = classification_metrices(id_gt, id_pred)\n",
    "\n",
    "    # Flattened as required to calculate clustering indices\n",
    "    flattened_neg_features = flatten_features(c_neg_features)\n",
    "    flattened_pos_features = flatten_features(c_pos_features)\n",
    "\n",
    "    dunn_index, davies_bouldin_index, silhouette_index = cluster_metrices(flattened_pos_features, flattened_neg_features)\n",
    "    cl_auc = roc_auc_curve(label_gt, label_pred)\n",
    "\n",
    "    data_frame_metrix[\"Labeled data\"].append(size)\n",
    "    data_frame_metrix[\"Dataset\"].append(f\"d_{dataset_type + 1}\")\n",
    "    data_frame_metrix[\"Accuracy\"].append(accuracy)\n",
    "    data_frame_metrix[\"Specificity\"].append(specificity)\n",
    "    data_frame_metrix[\"Sensitivity\"].append(sensitivity)\n",
    "    data_frame_metrix[\"AUC\"].append(cl_auc)\n",
    "    data_frame_metrix[\"Dunn index\"].append(dunn_index)\n",
    "    data_frame_metrix[\"Davies Bouldin\"].append(davies_bouldin_index)\n",
    "    data_frame_metrix[\"Silhouette index\"].append(silhouette_index)\n",
    "    data_frame_metrix[\"TP\"].append(TP)\n",
    "    data_frame_metrix[\"TN\"].append(TN)\n",
    "    data_frame_metrix[\"FP\"].append(FP)\n",
    "    data_frame_metrix[\"FN\"].append(FN)\n",
    "    data_frame_metrix[\"neg_labeled_img\"].append(neg_labeled_img)\n",
    "    data_frame_metrix[\"pos_labeled_img\"].append(pos_labeled_img)\n",
    "    data_frame_metrix[\"corrected_count\"].append(corrected_count)\n",
    "\n",
    "    print(f\"Dataset: d_{dataset_type + 1} \\t\\t\\t Labeled image: {size} \\t\\t Corrected count: {corrected_count}\")\n",
    "    print(f\"Accuracy: {accuracy} \\t\\t Specificity: {specificity} \\t\\t Sensitivity: {sensitivity}\")\n",
    "    print(f\"Dunn index: {dunn_index}\")\n",
    "    print(f\"Davies Bouldin: {davies_bouldin_index}\")\n",
    "    print(f\"Silhouette index: {silhouette_index}\")\n",
    "    print(f\"AUC: {cl_auc}\")\n",
    "\n",
    "data_f_matrix = pd.DataFrame.from_dict(data_frame_metrix)\n",
    "data_f_matrix.to_csv(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}/model_evaluation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
