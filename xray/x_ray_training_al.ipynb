{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "from statistics import mode\n",
    "\n",
    "# OpenCV\n",
    "import cv2\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import manhattan_distances, cosine_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 2 #int(input(\"Enter the number for: \\n 1) VGGNET16 \\n 2) Resnet101  \\n 3) Densenet169 \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For traning speed, define DATASET_SHRINK_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SHRINK_FACTOR = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open extracted feature in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../pickle_files/al/x_ray/\"\n",
    "if selected_model == 1:\n",
    "  filename = \"x_ray_vggnet16.pickle\"\n",
    "elif selected_model == 2:\n",
    "  filename = \"x_ray_resnet101.pickle\"\n",
    "elif selected_model == 3:\n",
    "  filename = \"x_ray_densenet169.pickle\"\n",
    "\n",
    "file = filepath + filename\n",
    "with open(file, 'rb') as handle:\n",
    "  all_ft_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the data and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset: 4400\n",
      "Sample feature dataset Covid: {'id': 3094, 'filepath': '../dataset/xrays/Noncovid/normal/NORMAL(48).png', 'image': array([0.       , 0.       , 0.       , ..., 0.       , 7.4171534,\n",
      "       0.       ], dtype=float32), 'label': 0}\n",
      "Sample feature dataset Non-Covid: {'id': 3176, 'filepath': '../dataset/xrays/Noncovid/normal/NORMAL(1150).png', 'image': array([ 0.      ,  0.      ,  0.      , ...,  0.      , 10.997936,\n",
      "        0.      ], dtype=float32), 'label': 0}\n",
      "Selected Dataset: 440\n"
     ]
    }
   ],
   "source": [
    "# suffle the data\n",
    "random.seed(42)\n",
    "shuffle(all_ft_dataset)\n",
    "\n",
    "print(\"Total Dataset: {}\".format(len(all_ft_dataset)))\n",
    "print(\"Sample feature dataset Covid: {}\".format(all_ft_dataset[0]))\n",
    "print(\"Sample feature dataset Non-Covid: {}\".format(all_ft_dataset[2125]))\n",
    "\n",
    "# shrink for minimize training time\n",
    "original_data_size = len(all_ft_dataset)\n",
    "new_data_size = int(original_data_size * DATASET_SHRINK_FACTOR)\n",
    "ft_dataset = all_ft_dataset[:new_data_size]\n",
    "\n",
    "print(\"Selected Dataset: {}\".format(len(ft_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to correct mispredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_mispredictions(query, feat_label, train_label, train_id, index_data, decision, data_frame_mistake, count):\n",
    "  if mode(decision) != query[\"label\"]:\n",
    "    count += 1\n",
    "    data_frame_mistake[\"Image name\"].append(query[\"filepath\"].split(\"/\")[-1])\n",
    "    data_frame_mistake[\"Mistake ID\"].append(query['id'])\n",
    "    data_frame_mistake[\"Original label\"].append(query['label'])\n",
    "    data_frame_mistake[\"Predicted label\"].append(mode(decision))\n",
    "    data_frame_mistake[\"Mistake index\"].append(index_data)\n",
    "    feat_label[query['label']].append(query[\"image\"])\n",
    "    train_label[query['label']].append(query[\"label\"])\n",
    "    train_id[query['label']].append(query['id'])\n",
    "\n",
    "  else:\n",
    "    feat_label[query['label']].append(query[\"image\"])\n",
    "    train_label[query['label']].append(query[\"label\"])\n",
    "    train_id[query['label']].append(query['id'])\n",
    "  return count, data_frame_mistake, feat_label, train_label, train_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calcuate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: raw dictionary (from pickle file)\n",
    "# feat_label: dictionary of {0: [], 1:[]}\n",
    "# distance_type: 1. Eucliddean, 2. Manhattan, 3. Cosine\n",
    "# id_pred: predicted id\n",
    "# label_pred: predicted label\n",
    "# n_neighbours: no. of neighbour\n",
    "# count:\n",
    "# train_label: training label\n",
    "# train_id: trainin id\n",
    "# index_data: index of data to track the mistaken data\n",
    "# data_frame_mistake: to save data_frame_metrix in CSV\n",
    "# supervised_data: if mentored data or not\n",
    "def distance(query, feat_label, distance_type, id_pred, label_pred, n_neighbours, count, train_label, train_id, index_data, data_frame_mistake, supervised_data):\n",
    "  expnd_query = np.expand_dims(query['image'], axis=0)\n",
    "  pos_tup, neg_tup = [], []\n",
    "  pos_dist, neg_dist = [], []\n",
    "\n",
    "  # Calculating the distance using numpy (axis=1) to calculate all at ones\n",
    "  if distance_type == 1: # Euclidean distance\n",
    "    if len(feat_label[0]) > 1:\n",
    "      neg_dist = np.linalg.norm(query['image'] - feat_label[0], axis=1)\n",
    "    elif len(feat_label[0]) == 1:\n",
    "      neg_dist = [np.linalg.norm(query['image'] - feat_label[0], axis=1)]\n",
    "    if len(feat_label[1]) > 1:\n",
    "      pos_dist = np.linalg.norm(query['image'] - feat_label[1], axis=1)\n",
    "    elif len(feat_label[1]) == 1:\n",
    "      pos_dist = [np.linalg.norm(query['image'] - feat_label[1], axis=1)]\n",
    "\n",
    "  elif distance_type == 2: # Manhattan distance\n",
    "    if len(feat_label[0]) > 1:\n",
    "      neg_dist = np.squeeze(manhattan_distances(feat_label[0], expnd_query))\n",
    "    elif len(feat_label[0]) == 1:\n",
    "      neg_dist = [np.squeeze(manhattan_distances(feat_label[0], expnd_query))]\n",
    "    if len(feat_label[1]) > 1:\n",
    "      pos_dist = np.squeeze(manhattan_distances(feat_label[1], expnd_query))\n",
    "    elif len(feat_label[1]) == 1:\n",
    "      pos_dist = [np.squeeze(manhattan_distances(feat_label[1], expnd_query))]\n",
    "\n",
    "  elif distance_type == 3: # Cosine distance\n",
    "    if len(feat_label[0]) > 1:\n",
    "      neg_dist = np.squeeze(cosine_distances(expnd_query, feat_label[0]))\n",
    "    elif len(feat_label[0]) == 1:\n",
    "      neg_dist = [np.squeeze(cosine_distances(expnd_query, feat_label[0]))]\n",
    "    if len(feat_label[1]) > 1:\n",
    "      pos_dist = np.squeeze(cosine_distances(expnd_query, feat_label[1]))\n",
    "    elif len(feat_label[1]) == 1:\n",
    "      pos_dist = [np.squeeze(cosine_distances(expnd_query, feat_label[1]))]\n",
    "\n",
    "  for dist_single in pos_dist:\n",
    "    pos_tup.append((dist_single, 1))\n",
    "\n",
    "  for dist_single in neg_dist:\n",
    "    neg_tup.append((dist_single, 0))\n",
    "\n",
    "  pos_tup.extend(neg_tup)\n",
    "  tup_dist = sorted(pos_tup)[:n_neighbours]\n",
    "\n",
    "  decision = [y for (x,y) in tup_dist]\n",
    "  if supervised_data:\n",
    "    (count,\n",
    "     data_frame_mistake,\n",
    "     feat_label,\n",
    "     train_label,\n",
    "     train_id) = correct_mispredictions(query,\n",
    "                                        feat_label,\n",
    "                                        train_label,\n",
    "                                        train_id,\n",
    "                                        index_data,\n",
    "                                        decision,\n",
    "                                        data_frame_mistake,\n",
    "                                        count)\n",
    "\n",
    "  else:\n",
    "    if len(decision) > 0 and mode(decision) == 0:\n",
    "      feat_label[0].append(query[\"image\"])\n",
    "      id_pred[0].append(query[\"id\"])\n",
    "      label_pred[0].append((query['id'], decision.count(1)/n_neighbours))\n",
    "    else:\n",
    "      feat_label[1].append(query[\"image\"])\n",
    "      id_pred[1].append(query[\"id\"])\n",
    "      label_pred[1].append((query['id'],decision.count(1)/n_neighbours))\n",
    "\n",
    "  return feat_label, id_pred, label_pred, data_frame_mistake, count, train_label, train_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate the classification metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrices(id_gt, id_pred):\n",
    "  TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "  # TP --> when correctly classified covid\n",
    "  for tp in id_pred[1]:\n",
    "    if tp in id_gt[1]:\n",
    "      TP += 1\n",
    "\n",
    "  # TN --> when correctly classified healthy (non-covid)\n",
    "  for tn in id_pred[0]:\n",
    "    if tn in id_gt[0]:\n",
    "      TN += 1\n",
    "\n",
    "  # FP --> when incorrectly classified healthy (Classified healthy as covid)\n",
    "  for fp in id_pred[1]:\n",
    "    if fp in id_gt[0]:\n",
    "      FP += 1\n",
    "\n",
    "  # FN --> when missed covid classification (Covid cases missed)\n",
    "  for fn in id_pred[0]:\n",
    "    if fn in id_gt[1]:\n",
    "      FN += 1\n",
    "\n",
    "  accuracy = round((TP + TN) / (TP + TN + FP + FN), 3)\n",
    "  if (TN + FP) > 0:\n",
    "    specificity = round(TN / (TN + FP), 3)\n",
    "  else:\n",
    "    specificity = 0 # Infinity\n",
    "\n",
    "  if (TP + FN) > 0:\n",
    "    sensitivity = round((TP) / (TP + FN), 3)\n",
    "  else:\n",
    "    sensitivity = 0 # Infinity\n",
    "\n",
    "  # f1_score = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "  print(\"TP: {}  FP: {}\".format(TP, FP))\n",
    "  print(\"FN: {}  TN: {}\".format(FN, TN))\n",
    "\n",
    "  return accuracy, specificity, sensitivity, TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_curve(label_gt, label_pred):\n",
    "  # contains (id, labels) tuple of binary class\n",
    "  gt_labels = sorted(label_gt[0] + label_gt[1])\n",
    "\n",
    "  # contains (id, labels) tuple of binary class --> sorted to match each element in gt_labels and pred_labels\n",
    "  pred_labels = sorted(label_pred[0] + label_pred[1])\n",
    "  y_test = [y for (x,y) in gt_labels] # Get only the labels\n",
    "  y_scores = [y for (x,y) in pred_labels]\n",
    "  fpr, tpr, threshold = roc_curve(y_test, y_scores)\n",
    "  roc_auc = round(auc(fpr, tpr), 3)\n",
    "  return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate Cluster metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_metrices(feat_label, train_label, id_pred):\n",
    "  print(\"Calculating Dunn's index...\")\n",
    "  dunn_index, davies_bouldin_index, silhouette_index = \"NA\", \"NA\", \"NA\"\n",
    "  if len(feat_label[0]) > 0 and len(feat_label[1]) > 0:\n",
    "    intra_dist1 = euclidean_distances(feat_label[0]).max()\n",
    "    intra_dist2 = euclidean_distances(feat_label[1]).max()\n",
    "    inter_dist = euclidean_distances(feat_label[0], feat_label[1]).min()\n",
    "\n",
    "    if intra_dist1 > intra_dist2:\n",
    "      max_intra_dist= intra_dist1\n",
    "    else:\n",
    "      max_intra_dist = intra_dist2\n",
    "\n",
    "    dunn_index = round(inter_dist / max_intra_dist, 3)\n",
    "\n",
    "  print(\"dunn_index: \", dunn_index)\n",
    "\n",
    "  # Davies Bouldin and Silhouette score from sklearn library.\n",
    "  class_0 = np.concatenate((np.zeros(shape=(len(train_label[0])), dtype=int), np.zeros(shape=(len(id_pred[0])), dtype=int)))\n",
    "  class_1 = np.concatenate((np.ones(shape=(len(train_label[1])), dtype=int), np.ones(shape=(len(id_pred[1])), dtype=int)))\n",
    "  class_all = np.concatenate((class_0, class_1))\n",
    "  if len(feat_label[0]) > 0 and len(feat_label[1]) > 0:\n",
    "    feature_all = np.concatenate((feat_label[0], feat_label[1]))\n",
    "    print(\"Calculating Davies Bouldin index...\")\n",
    "    davies_bouldin_index = round(davies_bouldin_score(feature_all, class_all), 3)\n",
    "    print(\"davies_bouldin_index: \", davies_bouldin_index)\n",
    "\n",
    "    print(\"Calculating Silhouette index...\")\n",
    "    silhouette_index = round(silhouette_score(feature_all, class_all), 3)\n",
    "    print(\"silhouette_index: \", silhouette_index)\n",
    "\n",
    "\n",
    "  return dunn_index, davies_bouldin_index, silhouette_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to load dataset into three different segment (k-way n-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to return three sets (n, 1500, 3000)  of labeled dataset for experiment\n",
    "def data_loader(dataset, n):\n",
    "  labeled_data, unlabeled_data = [], []\n",
    "\n",
    "  l_data = dataset[:n]                          # First case (0-40) // labeled + mentored\n",
    "  ul_data = dataset[n:]                         # First case (40-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[0] => dataset[0-40]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[0] => dataset[40-1000]\n",
    "\n",
    "  size_second_set = int(1500 * DATASET_SHRINK_FACTOR) # 1500 * 0.1 = 150\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Second case (150-190) // labeled + mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Second case (0-150) + (190-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[1] => dataset[150-190]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[1] => dataset[0-150] + dataset[190-1000]\n",
    "\n",
    "  size_second_set = int(3000 * DATASET_SHRINK_FACTOR) # 3000 * 0.1 = 300\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Third case (300-340) // labeled + mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Third case (0-300) + (340-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[2] => dataset[300-340]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[2] => dataset[0-300] + dataset[340-1000]\n",
    "  return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to separate data into positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that selects number of data samples and removes the selected data from the dataset. (required to select balanced positive and negative samples)\n",
    "def data_separation(dataset, taken_data_idx, label=None, data_sample=100):\n",
    "  add_data = []\n",
    "  for i, data in enumerate(dataset):\n",
    "    if i in taken_data_idx:\n",
    "      continue\n",
    "    if label > -1 and dataset[i][\"label\"] == label:\n",
    "      add_data.append(data)\n",
    "      taken_data_idx.append(i)\n",
    "    else:\n",
    "      add_data.append(data)\n",
    "      taken_data_idx.append(i)\n",
    "    if len(add_data) == data_sample:\n",
    "      break\n",
    "  return add_data, taken_data_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance type (Euclidean Manhattan or Consine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_type = 3 # int(input(\"Enter the number for: \\n 1) Euclidean  \\n 2) Manhattan \\n 3) Cosine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Distance Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model == 1:\n",
    "  s_model = 'vggnet16'\n",
    "elif selected_model == 2:\n",
    "  s_model = 'resnet101'\n",
    "elif selected_model == 3:\n",
    "  s_model = 'densenet169'\n",
    "\n",
    "if distance_type == 1:\n",
    "  s_distance = 'euclidean'\n",
    "elif distance_type == 2:\n",
    "  s_distance = 'manhattan'\n",
    "elif distance_type == 3:\n",
    "  s_distance = 'cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== 1/12 ==============================\n",
      "training data: 20\n",
      "feature neg: 9\n",
      "feature pos: 11\n",
      "supervised data: 20\n",
      "unlabeled data: 420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 420/420 [00:25<00:00, 16.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 188  FP: 1\n",
      "FN: 10  TN: 221\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.104\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.089\n",
      "Dataset: d_0 \t\t Labeled image: 20 \t\t Corrected count: 0\n",
      "Accuracy: 0.974 \t\t Specificity: 0.995 \t\t Sensitivity: 0.949\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.104 \t Silhouette index: 0.08900000154972076\n",
      "AUC: 0.9894553644553644\n",
      "============================== 2/12 ==============================\n",
      "training data: 20\n",
      "feature neg: 13\n",
      "feature pos: 7\n",
      "supervised data: 20\n",
      "unlabeled data: 420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 420/420 [00:23<00:00, 17.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 190  FP: 1\n",
      "FN: 12  TN: 217\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.114\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.089\n",
      "Dataset: d_1 \t\t Labeled image: 20 \t\t Corrected count: 4\n",
      "Accuracy: 0.969 \t\t Specificity: 0.995 \t\t Sensitivity: 0.941\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.114 \t Silhouette index: 0.08900000154972076\n",
      "AUC: 0.9869425015896085\n",
      "============================== 3/12 ==============================\n",
      "training data: 20\n",
      "feature neg: 11\n",
      "feature pos: 9\n",
      "supervised data: 20\n",
      "unlabeled data: 420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 420/420 [00:22<00:00, 18.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 194  FP: 1\n",
      "FN: 6  TN: 219\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.111\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.09\n",
      "Dataset: d_2 \t\t Labeled image: 20 \t\t Corrected count: 2\n",
      "Accuracy: 0.983 \t\t Specificity: 0.995 \t\t Sensitivity: 0.97\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.111 \t Silhouette index: 0.09000000357627869\n",
      "AUC: 0.9918181818181817\n",
      "============================== 4/12 ==============================\n",
      "training data: 40\n",
      "feature neg: 21\n",
      "feature pos: 19\n",
      "supervised data: 40\n",
      "unlabeled data: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:25<00:00, 15.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 181  FP: 1\n",
      "FN: 9  TN: 209\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.084\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.091\n",
      "Dataset: d_0 \t\t Labeled image: 40 \t\t Corrected count: 0\n",
      "Accuracy: 0.975 \t\t Specificity: 0.995 \t\t Sensitivity: 0.953\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.084 \t Silhouette index: 0.09099999815225601\n",
      "AUC: 0.9890601503759399\n",
      "============================== 5/12 ==============================\n",
      "training data: 40\n",
      "feature neg: 24\n",
      "feature pos: 16\n",
      "supervised data: 40\n",
      "unlabeled data: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:24<00:00, 16.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 181  FP: 1\n",
      "FN: 12  TN: 206\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.108\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.09\n",
      "Dataset: d_1 \t\t Labeled image: 40 \t\t Corrected count: 2\n",
      "Accuracy: 0.968 \t\t Specificity: 0.995 \t\t Sensitivity: 0.938\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.108 \t Silhouette index: 0.09000000357627869\n",
      "AUC: 0.986383319566469\n",
      "============================== 6/12 ==============================\n",
      "training data: 40\n",
      "feature neg: 21\n",
      "feature pos: 19\n",
      "supervised data: 40\n",
      "unlabeled data: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:24<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 183  FP: 1\n",
      "FN: 7  TN: 209\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.094\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.091\n",
      "Dataset: d_2 \t\t Labeled image: 40 \t\t Corrected count: 1\n",
      "Accuracy: 0.98 \t\t Specificity: 0.995 \t\t Sensitivity: 0.963\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.094 \t Silhouette index: 0.09099999815225601\n",
      "AUC: 0.9941604010025062\n",
      "============================== 7/12 ==============================\n",
      "training data: 40\n",
      "feature neg: 21\n",
      "feature pos: 19\n",
      "supervised data: 40\n",
      "unlabeled data: 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 360/360 [00:20<00:00, 17.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 165  FP: 0\n",
      "FN: 7  TN: 188\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.548\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.083\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.09\n",
      "Dataset: d_0 \t\t Labeled image: 80 \t\t Corrected count: 0\n",
      "Accuracy: 0.981 \t\t Specificity: 1.0 \t\t Sensitivity: 0.959\n",
      "Dunn index: 0.5479999780654907 \t Davies Bouldin: 3.083 \t Silhouette index: 0.09000000357627869\n",
      "AUC: 0.9910935180603662\n",
      "============================== 8/12 ==============================\n",
      "training data: 40\n",
      "feature neg: 24\n",
      "feature pos: 16\n",
      "supervised data: 40\n",
      "unlabeled data: 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 360/360 [00:20<00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 166  FP: 1\n",
      "FN: 12  TN: 181\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.121\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.089\n",
      "Dataset: d_1 \t\t Labeled image: 80 \t\t Corrected count: 2\n",
      "Accuracy: 0.964 \t\t Specificity: 0.995 \t\t Sensitivity: 0.933\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.121 \t Silhouette index: 0.08900000154972076\n",
      "AUC: 0.9878534386961353\n",
      "============================== 9/12 ==============================\n",
      "training data: 40\n",
      "feature neg: 21\n",
      "feature pos: 19\n",
      "supervised data: 40\n",
      "unlabeled data: 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 360/360 [00:20<00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 167  FP: 1\n",
      "FN: 4  TN: 188\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.064\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.092\n",
      "Dataset: d_2 \t\t Labeled image: 80 \t\t Corrected count: 1\n",
      "Accuracy: 0.986 \t\t Specificity: 0.995 \t\t Sensitivity: 0.977\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.064 \t Silhouette index: 0.09200000017881393\n",
      "AUC: 0.9935486865311427\n",
      "============================== 10/12 ==============================\n",
      "training data: 40\n",
      "feature neg: 21\n",
      "feature pos: 19\n",
      "supervised data: 40\n",
      "unlabeled data: 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 285/285 [00:13<00:00, 21.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 123  FP: 0\n",
      "FN: 8  TN: 154\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.576\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.068\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.091\n",
      "Dataset: d_0 \t\t Labeled image: 155 \t\t Corrected count: 0\n",
      "Accuracy: 0.972 \t\t Specificity: 1.0 \t\t Sensitivity: 0.939\n",
      "Dunn index: 0.5759999752044678 \t Davies Bouldin: 3.068 \t Silhouette index: 0.09099999815225601\n",
      "AUC: 0.9805690492713394\n",
      "============================== 11/12 ==============================\n",
      "training data: 40\n",
      "feature neg: 24\n",
      "feature pos: 16\n",
      "supervised data: 40\n",
      "unlabeled data: 285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 285/285 [00:13<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 135  FP: 1\n",
      "FN: 9  TN: 140\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.076\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.091\n",
      "Dataset: d_1 \t\t Labeled image: 155 \t\t Corrected count: 2\n",
      "Accuracy: 0.965 \t\t Specificity: 0.993 \t\t Sensitivity: 0.938\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.076 \t Silhouette index: 0.09099999815225601\n",
      "AUC: 0.9882042947202522\n",
      "============================== 12/12 ==============================\n",
      "training data: 40\n",
      "feature neg: 21\n",
      "feature pos: 19\n",
      "supervised data: 40\n",
      "unlabeled data: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:14<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 136  FP: 1\n",
      "FN: 3  TN: 160\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.544\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.099\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.091\n",
      "Dataset: d_2 \t\t Labeled image: 155 \t\t Corrected count: 1\n",
      "Accuracy: 0.987 \t\t Specificity: 0.994 \t\t Sensitivity: 0.978\n",
      "Dunn index: 0.5440000295639038 \t Davies Bouldin: 3.099 \t Silhouette index: 0.09099999815225601\n",
      "AUC: 0.995665579337772\n"
     ]
    }
   ],
   "source": [
    "n_neighbours = 5 # 31\n",
    "labeled_size = [200, 400, 800, 1550]\n",
    "labeled_size = [int(size * DATASET_SHRINK_FACTOR) for size in labeled_size]\n",
    "data_frame_metrix = {\n",
    "  \"Labeled data\": [],\n",
    "  \"Dataset\": [],\n",
    "  \"Accuracy\": [],\n",
    "  \"Specificity\": [],\n",
    "  \"Sensitivity\": [],\n",
    "  \"AUC\":[],\n",
    "  \"Dunn index\": [],\n",
    "  \"Davies Bouldin\": [],\n",
    "  \"Silhouette index\":[],\n",
    "  \"TP\":[],\n",
    "  \"TN\":[],\n",
    "  \"FP\":[],\n",
    "  \"FN\":[],\n",
    "  \"pos_labeled_img\":[],\n",
    "  \"neg_labeled_img\":[],\n",
    "  \"corrected_count\":[]\n",
    "}\n",
    "\n",
    "global_count = 0\n",
    "\n",
    "for size in labeled_size:\n",
    "  labeled_data_sets, unlabeled_data_sets = data_loader(ft_dataset, size)\n",
    "\n",
    "  # labeled_data_sets ==> three sets: [d1, d2, d3] ==> eg: [0-40, 320-360, 640-680]\n",
    "  for dataset_type, labeled_data in enumerate(labeled_data_sets):\n",
    "    global_count += 1\n",
    "    print(f\"============================== {global_count}/{len(labeled_size) * len(labeled_data_sets)} ==============================\")\n",
    "    data_frame_mistake = {\n",
    "      \"Image name\": [],\n",
    "      \"Mistake index\": [],\n",
    "      \"Mistake ID\": [],\n",
    "      \"Original label\": [],\n",
    "      \"Predicted label\": [],\n",
    "    }\n",
    "\n",
    "    pos_img, neg_img = 0, 0\n",
    "\n",
    "    # collect the ground truth (label) of all the predicting images =>> key: 0 & 1 (class), value: tuple (data['id'], data['label']), required to calulate TP, FP, FN, TN\n",
    "    label_gt = {0: [], 1: []}\n",
    "    # collect the ground truth (id) of all the predicting images =>> key: 0 & 1 (class), value: ground truth id\n",
    "    id_gt = {0: [], 1: []}\n",
    "\n",
    "    # collect the predicted label for all the images =>> key: 0 & 1 (class), value: tuple(query['id'], decision.count(1)/n_neighbours) --> decision.count(1)/n_neighbours: Percentage of predicted positive class, required to calculate AUC/ROC value\n",
    "    label_pred = {0: [], 1: []}\n",
    "    # collect the predicted id for all the images =>> key: 0 & 1 (class), value: predicted ids, required to calulate TP, FP, FN, TN\n",
    "    id_pred = {0: [],  1: []}\n",
    "\n",
    "    # feature label =>> key: 0 & 1 (class), value: deep feature of image\n",
    "    feat_label = {0: [], 1: []}\n",
    "\n",
    "    # train id =>> key: 0 & 1 (class), value: id of images --> Not required for, but scared to delete. \n",
    "    train_id = {0: [], 1: []}\n",
    "    # train lable =>> key: 0 & 1 (class), value: ground truth labels\n",
    "    train_label = {0: [], 1: []}\n",
    "\n",
    "    # # select balanced labeled data (50% from positive and 50% from negative) and initialize training data from a few sample and rest data as supervised data\n",
    "    # sample_data_size = int(40 * DATASET_SHRINK_FACTOR) # 40\n",
    "    # training_data_pos, taken_data_idx = data_separation(labeled_data, [], 0, int(sample_data_size/2))\n",
    "    # training_data_neg, taken_data_idx = data_separation(labeled_data, taken_data_idx, 1, int(sample_data_size/2))\n",
    "    # training_data = training_data_pos + training_data_neg\n",
    "\n",
    "    # sample_data_size_supervised = len(labeled_data) - sample_data_size\n",
    "    # supervised_data, taken_data_idx = data_separation(labeled_data, taken_data_idx, -1, sample_data_size_supervised)\n",
    "\n",
    "    training_data, supervised_data = labeled_data[:40], labeled_data[:40]\n",
    "\n",
    "    print(f\"training data: {len(training_data)}\")\n",
    "\n",
    "    for data in training_data:\n",
    "      if data[\"label\"] == 1:\n",
    "        feat_label[1].append(data['image'])\n",
    "        train_id[1].append(data['id'])\n",
    "        train_label[1].append((data['id'],data['label']))\n",
    "        pos_img += 1\n",
    "      else:\n",
    "        feat_label[0].append(data['image'])\n",
    "        train_id[0].append(data['id'])\n",
    "        train_label[0].append((data['id'],data['label']))\n",
    "        neg_img += 1\n",
    "\n",
    "\n",
    "    print(f\"feature neg: {neg_img}\")\n",
    "    print(f\"feature pos: {pos_img}\")\n",
    "\n",
    "    count, index_data = 0, 200\n",
    "\n",
    "    print(f\"supervised data: {len(supervised_data)}\")\n",
    "\n",
    "    # loop is for the mentored data --> Notice mentored_data=True in argument of the function call distance.\n",
    "    for data in supervised_data:\n",
    "      (feat_label,\n",
    "       id_pred,\n",
    "       label_pred,\n",
    "       data_frame_mistake,\n",
    "       count, train_label,\n",
    "       train_id) = distance(data,\n",
    "                           feat_label,\n",
    "                           distance_type,\n",
    "                           id_pred,\n",
    "                           label_pred,\n",
    "                           n_neighbours,\n",
    "                           count,\n",
    "                           train_label,\n",
    "                           train_id,\n",
    "                           index_data,\n",
    "                           data_frame_mistake,\n",
    "                           supervised_data=True)\n",
    "      index_data += 1\n",
    "\n",
    "    data_f_mistake = pd.DataFrame.from_dict(data_frame_mistake)\n",
    "    data_f_mistake.to_csv(f\"../test/x_ray/{s_model}_{s_distance}_mistake_{size}_{dataset_type}.csv\", index=False)\n",
    "\n",
    "    print(f\"unlabeled data: {len(unlabeled_data_sets[dataset_type])}\")\n",
    "\n",
    "    # loop is for the test data --> Notice mentored_data=False in argument of the function call distance.\n",
    "    for data in tqdm(unlabeled_data_sets[dataset_type]):\n",
    "      if data[\"label\"] == 1:\n",
    "        id_gt[1].append(data['id'])\n",
    "        label_gt[1].append((data['id'], data['label']))\n",
    "      else:\n",
    "        id_gt[0].append(data['id'])\n",
    "        label_gt[0].append((data['id'], data['label']))\n",
    "\n",
    "      feat_label, id_pred, label_pred, _, _, _, _ = distance(data,\n",
    "                                                             feat_label,\n",
    "                                                             distance_type,\n",
    "                                                             id_pred,\n",
    "                                                             label_pred,\n",
    "                                                             n_neighbours,\n",
    "                                                             count=None,\n",
    "                                                             train_label=None,\n",
    "                                                             train_id=None,\n",
    "                                                             index_data=None,\n",
    "                                                             data_frame_mistake=None,\n",
    "                                                             supervised_data=False)\n",
    "\n",
    "    accuracy, specificity, sensitivity, TP, TN, FP, FN = classification_metrices(id_gt, id_pred)\n",
    "    dunn_index, davies_bouldin_index, silhouette_index = cluster_metrices(feat_label, train_label, id_pred)\n",
    "    cl_auc = roc_auc_curve(label_gt, label_pred)\n",
    "\n",
    "    data_frame_metrix[\"Labeled data\"].append(size)\n",
    "    data_frame_metrix[\"Dataset\"].append(f\"d_{dataset_type}\")\n",
    "    data_frame_metrix[\"Accuracy\"].append(accuracy)\n",
    "    data_frame_metrix[\"Specificity\"].append(specificity)\n",
    "    data_frame_metrix[\"Sensitivity\"].append(sensitivity)\n",
    "    data_frame_metrix[\"AUC\"].append(cl_auc)\n",
    "    data_frame_metrix[\"Dunn index\"].append(dunn_index)\n",
    "    data_frame_metrix[\"Davies Bouldin\"].append(davies_bouldin_index)\n",
    "    data_frame_metrix[\"Silhouette index\"].append(silhouette_index)\n",
    "    data_frame_metrix[\"TP\"].append(TP)\n",
    "    data_frame_metrix[\"TN\"].append(TN)\n",
    "    data_frame_metrix[\"FP\"].append(FP)\n",
    "    data_frame_metrix[\"FN\"].append(FN)\n",
    "    data_frame_metrix[\"pos_labeled_img\"].append(pos_img)\n",
    "    data_frame_metrix[\"neg_labeled_img\"].append(neg_img)\n",
    "    data_frame_metrix[\"corrected_count\"].append(count)\n",
    "\n",
    "    print(f\"Dataset: d_{dataset_type} \\t\\t Labeled image: {size} \\t\\t Corrected count: {count}\")\n",
    "    print(f\"Accuracy: {accuracy} \\t\\t Specificity: {specificity} \\t\\t Sensitivity: {sensitivity}\")\n",
    "    print(f\"Dunn index: {dunn_index} \\t Davies Bouldin: {davies_bouldin_index} \\t Silhouette index: {silhouette_index}\")\n",
    "    print(f\"AUC: {cl_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f=pd.DataFrame.from_dict(data_frame_metrix)\n",
    "data_f.to_csv(f\"../test/x_ray/{s_model}_{s_distance}_dist.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
