{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os, math, copy\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from statistics import mode\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_curve, auc, davies_bouldin_score, silhouette_score # cluster metrices\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix # agent metrices\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Base model to be used for image features, select distance type and define dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 3 #int(input(\"Enter the number for: \\n 1) VGG16 \\n 2) Resnet101  \\n 3) Densenet169 \"))\n",
    "\n",
    "distance_type = 3 # int(input(\"Enter the number for: \\n 1) Euclidean  \\n 2) Manhattan \\n 3) Cosine\"))\n",
    "\n",
    "DATASET_SIZE = 1 # (max: 1, min: 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open extracted feature in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../pickle_files/al/ct_scan/\"\n",
    "if selected_model == 1:\n",
    "  filename = \"ct_scan_pca_5000_vgg16.pickle\"\n",
    "elif selected_model == 2:\n",
    "  filename = \"ct_scan_pca_5000_resnet101.pickle\"\n",
    "elif selected_model == 3:\n",
    "  filename = \"ct_scan_pca_5000_densenet169.pickle\"\n",
    "\n",
    "file = filepath + filename\n",
    "with open(file, 'rb') as handle:\n",
    "  all_ft_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the data and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset: 5000\n",
      "Sample feature dataset Covid: {'id': 1383, 'filepath': '../../dataset/ctscan/3A_images_resized/all\\\\Normal\\\\Normal_1679_838_0035.png', 'image': array([-2.6356245e+01,  7.2206085e+01, -5.9000652e+01, ...,\n",
      "       -9.3539524e-01, -3.2258956e+00,  1.8179201e-04], dtype=float32), 'label': 0}\n",
      "Sample feature dataset Non-Covid: {'id': 3811, 'filepath': '../../dataset/ctscan/3A_images_resized/all\\\\Covid19\\\\NCP_218_1580_0100.png', 'image': array([ 7.2411385e+01, -5.6367859e+01, -4.5803837e+01, ...,\n",
      "       -4.3743055e-02,  6.7281947e-02,  1.8183325e-04], dtype=float32), 'label': 1}\n",
      "Selected Dataset: 5000\n",
      "Number of features in a image: 5000\n"
     ]
    }
   ],
   "source": [
    "# suffle the data\n",
    "random.seed(42)\n",
    "shuffle(all_ft_dataset)\n",
    "\n",
    "print(\"Total Dataset: {}\".format(len(all_ft_dataset)))\n",
    "print(\"Sample feature dataset Covid: {}\".format(all_ft_dataset[0]))\n",
    "print(\"Sample feature dataset Non-Covid: {}\".format(all_ft_dataset[1200]))\n",
    "\n",
    "# shrink for minimize training time\n",
    "original_data_size = len(all_ft_dataset)\n",
    "new_data_size = int(original_data_size * DATASET_SIZE)\n",
    "ft_dataset = all_ft_dataset[:new_data_size]\n",
    "\n",
    "print(\"Selected Dataset: {}\".format(len(ft_dataset)))\n",
    "print(\"Number of features in a image: {}\".format(ft_dataset[0]['image'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom class inherited from KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKMeans(KMeans):\n",
    "  def __init__(self, n_clusters, random_state=0, n_init=\"auto\"):\n",
    "    KMeans.__init__(self, n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n",
    "    self.cluster_members_ = []\n",
    "\n",
    "  def __update_members(self, new_members):\n",
    "    self.cluster_members_ = new_members\n",
    "\n",
    "  def __update_centers(self, new_cluster_centers):\n",
    "    self.cluster_centers_ = new_cluster_centers\n",
    "\n",
    "  def __update_labels(self, new_labels):\n",
    "    self.labels_ = new_labels\n",
    "\n",
    "  def merge_subclusters(self, subclusters):\n",
    "    members = []\n",
    "    for subcluster in subclusters:\n",
    "      members.extend(subcluster)\n",
    "    return members\n",
    "\n",
    "  def update(self, centers_neg, centers_pos, clusters_neg, clusters_pos):\n",
    "    c_centers_neg = np.mean(centers_neg, axis=0, dtype=np.float64)\n",
    "    c_centers_pos = np.mean(centers_pos, axis=0, dtype=np.float64)\n",
    "\n",
    "    new_centers = np.array([c_centers_neg, c_centers_pos])\n",
    "    self.__update_centers(new_centers)\n",
    "\n",
    "    c_members_neg = self.merge_subclusters(clusters_neg)\n",
    "    c_members_pos = self.merge_subclusters(clusters_pos)\n",
    "\n",
    "    new_members = [c_members_neg, c_members_pos]\n",
    "    self.__update_members(new_members)\n",
    "\n",
    "    new_labels = self.predict(c_members_neg + c_members_pos)\n",
    "    model_cluster.__update_labels(new_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defina a class for human expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanExpert():\n",
    "  def __init__(self, labeled_data):\n",
    "    self.labeled_data = labeled_data\n",
    "\n",
    "  def policy(self, instance):\n",
    "    query = next(data for data in self.labeled_data if data[\"id\"] == instance)\n",
    "    return [query['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a class for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "  def __init__(self):\n",
    "    self.clf = self.classifier()\n",
    "    self.reward = 0\n",
    "    self.avg_reward = 0\n",
    "    self.reward_list = []\n",
    "\n",
    "  def classifier(self):\n",
    "    # param_grid = {'C': [0.1,1,10,100], 'gamma': [0.0001,0.001,0.1,1], 'kernel': ['rbf','linear']}\n",
    "    clf = SVC(kernel=\"linear\")\n",
    "    # clf = GridSearchCV(clf, param_grid)\n",
    "    return clf\n",
    "\n",
    "  def imitate_human(self, demo_data):\n",
    "    shuffle(demo_data)\n",
    "    X_train, y_train = [x for (x, y) in demo_data], [y for (x, y) in demo_data]\n",
    "    self.clf.fit(X_train, y_train)\n",
    "\n",
    "  def policy(self, states):\n",
    "    actions = self.clf.predict(states)\n",
    "    return actions\n",
    "\n",
    "  def update_reward(self, pred, gt, n_obs):\n",
    "    if pred == gt:\n",
    "      self.reward += 1\n",
    "      self.avg_reward = round(self.reward/n_obs, 3)\n",
    "\n",
    "  def update_reward_list(self):\n",
    "    self.reward_list.append(self.avg_reward)\n",
    "\n",
    "  def reset_reward(self):\n",
    "    self.reward = self.avg_reward = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate the classification metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrices(id_gt, id_pred):\n",
    "  TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "  # TP --> when correctly classified covid\n",
    "  for tp in id_pred[1]:\n",
    "    if tp in id_gt[1]:\n",
    "      TP += 1\n",
    "\n",
    "  # TN --> when correctly classified healthy (non-covid)\n",
    "  for tn in id_pred[0]:\n",
    "    if tn in id_gt[0]:\n",
    "      TN += 1\n",
    "\n",
    "  # FP --> when incorrectly classified healthy (Classified healthy as covid)\n",
    "  for fp in id_pred[1]:\n",
    "    if fp in id_gt[0]:\n",
    "      FP += 1\n",
    "\n",
    "  # FN --> when missed covid classification (Covid cases missed)\n",
    "  for fn in id_pred[0]:\n",
    "    if fn in id_gt[1]:\n",
    "      FN += 1\n",
    "\n",
    "  accuracy = round((TP + TN) / (TP + TN + FP + FN), 3)\n",
    "  if (TN + FP) > 0:\n",
    "    specificity = round(TN / (TN + FP), 3)\n",
    "  else:\n",
    "    specificity = 0 # Infinity\n",
    "\n",
    "  if (TP + FN) > 0:\n",
    "    sensitivity = round((TP) / (TP + FN), 3)\n",
    "  else:\n",
    "    sensitivity = 0 # Infinity\n",
    "\n",
    "  # f1_score = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "  print(\"TP: {}  FP: {}\".format(TP, FP))\n",
    "  print(\"FN: {}  TN: {}\".format(FN, TN))\n",
    "\n",
    "  return accuracy, specificity, sensitivity, TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_curve(label_gt, label_pred):\n",
    "  # contains (id, labels) tuple of binary class\n",
    "  gt_labels = sorted(label_gt[0] + label_gt[1])\n",
    "\n",
    "  # contains (id, labels) tuple of binary class --> sorted to match each element in gt_labels and pred_labels\n",
    "  pred_labels = sorted(label_pred[0] + label_pred[1])\n",
    "  y_test = [y for (x,y) in gt_labels] # Get only the labels\n",
    "  y_scores = [y for (x,y) in pred_labels]\n",
    "  fpr, tpr, threshold = roc_curve(y_test, y_scores)\n",
    "  roc_auc = round(auc(fpr, tpr), 3)\n",
    "  return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate Cluster metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_metrices(neg_features, pos_features):\n",
    "  print(\"Calculating Dunn's index...\")\n",
    "  dunn_index, davies_bouldin_index, silhouette__score = \"NA\", \"NA\", \"NA\"\n",
    "  if len(neg_features) > 0 and len(pos_features) > 0:\n",
    "    intra_dist1 = euclidean_distances(neg_features).max()\n",
    "    intra_dist2 = euclidean_distances(pos_features).max()\n",
    "    inter_dist = euclidean_distances(neg_features, pos_features).min()\n",
    "\n",
    "    if intra_dist1 > intra_dist2:\n",
    "      max_intra_dist= intra_dist1\n",
    "    else:\n",
    "      max_intra_dist = intra_dist2\n",
    "\n",
    "    dunn_index = round(inter_dist / max_intra_dist, 3)\n",
    "\n",
    "  print(\"dunn_index: \", dunn_index)\n",
    "\n",
    "  # Davies Bouldin and Silhouette score from sklearn library.\n",
    "  if len(neg_features) > 0 and len(pos_features) > 0:\n",
    "    feature_all = np.concatenate((neg_features, pos_features))\n",
    "    neg_labels = np.zeros(shape=(len(neg_features)),dtype=int)\n",
    "    pos_labels = np.ones(shape=(len(pos_features)),dtype=int)\n",
    "\n",
    "    label_all = np.concatenate((neg_labels, pos_labels))\n",
    "    print(\"Calculating Davies Bouldin index...\")\n",
    "    davies_bouldin_index = round(davies_bouldin_score(feature_all, label_all), 3)\n",
    "    print(\"davies_bouldin_index: \", davies_bouldin_index)\n",
    "\n",
    "    print(\"Calculating Silhouette index...\")\n",
    "    silhouette__score = round(silhouette_score(feature_all, label_all), 3)\n",
    "    print(\"silhouette_score: \", silhouette_score)\n",
    "\n",
    "  return dunn_index, davies_bouldin_index, silhouette__score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to flatten the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to calculate the clustering indices\n",
    "def flatten_features(features):\n",
    "  all_features = []\n",
    "  for feature in features:\n",
    "    for index in feature:\n",
    "      all_features.append(index)\n",
    "  return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Model for the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_to_clusters(features, n_clusters=2):\n",
    "  kmeans = MyKMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\")\n",
    "  kmeans.fit(features)\n",
    "  # list of cluster elements => [neg_data_list, pos_data_list]\n",
    "  clusters = [np.array(features)[np.where(kmeans.labels_ == i)[0]] for i in range(len(np.unique(kmeans.labels_)))]\n",
    "  kmeans.cluster_members_ = clusters\n",
    "  return kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to subcluster the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create subclusters\n",
    "def sub_clusters(features, n_clusters=5):\n",
    "  # number of cluster defined from elbow method\n",
    "  kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\")\n",
    "  kmeans.fit(features)\n",
    "  # list of subcluster elements => [sublist, sublist2...]\n",
    "  clusters = [np.array(features)[np.where(kmeans.labels_ == i)[0]] for i in range(len(np.unique(kmeans.labels_)))]\n",
    "  return kmeans.cluster_centers_, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to load dataset into three different segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to return three sets (n, 1500, 3000)  of mentored dataset for experiment\n",
    "def data_loader(dataset, n):\n",
    "  mentored_data, unlabeled_data = [], []\n",
    "\n",
    "  l_data = dataset[:n]                          # First case (0-40) // mentored\n",
    "  ul_data = dataset[n:]                         # First case (40-1000) // unlabeled\n",
    "  mentored_data.append(l_data)                   # mentored_data[0] => dataset[0-40]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[0] => dataset[40-1000]\n",
    "\n",
    "  size_second_set = int(1500 * DATASET_SIZE)    # 1500 * 0.1 = 150\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Second case (150-190) // mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Second case (0-150) + (190-1000) // unlabeled\n",
    "  mentored_data.append(l_data)                   # mentored_data[1] => dataset[150-190]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[1] => dataset[0-150] + dataset[190-1000]\n",
    "\n",
    "  size_second_set = int(3000 * DATASET_SIZE)    # 3000 * 0.1 = 300\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Third case (300-340) // mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Third case (0-300) + (340-1000) // unlabeled\n",
    "  mentored_data.append(l_data)                   # mentored_data[2] => dataset[300-340]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[2] => dataset[0-300] + dataset[340-1000]\n",
    "  return mentored_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to separate data into positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that selects number of data samples and removes the selected data from the dataset.\n",
    "# (required to select balanced neg and pos samples)\n",
    "def data_separation(dataset, label, data_sample=100):\n",
    "  add_data = []\n",
    "  for i, data in enumerate(dataset):\n",
    "    if data[\"label\"] == label:\n",
    "      add_data.append(data['image'])\n",
    "      del dataset[i]\n",
    "    if len(add_data) == data_sample:\n",
    "      break\n",
    "\n",
    "  return add_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Distance Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model == 1:\n",
    "  s_model = 'vgg16'\n",
    "elif selected_model == 2:\n",
    "  s_model = 'resnet101'\n",
    "elif selected_model == 3:\n",
    "  s_model = 'densenet169'\n",
    "\n",
    "if distance_type == 1:\n",
    "  s_distance = 'euclidean'\n",
    "elif distance_type == 2:\n",
    "  s_distance = 'manhattan'\n",
    "elif distance_type == 3:\n",
    "  s_distance = 'cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the new cluster centers of each subclusters.\n",
    "# (required as mean is the representative of that subcluster)\n",
    "def get_updated_cc(c_neg_features, c_pos_features):\n",
    "  cc_neg = np.array([np.mean(i, axis=0) for i in c_neg_features])  # Mean of all neg subclusters\n",
    "  cc_pos = np.array([np.mean(i, axis=0) for i in c_pos_features])  # Mean of all pos subclusters\n",
    "  return cc_neg, cc_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the class (neg or pos) from max frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_with_max_freq(closest_labels_from_model):\n",
    "    return mode(closest_labels_from_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update subcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that updates the subcluster by concatenating the test data sample to the most similar subcluster.\n",
    "def update_subclusters(query,\n",
    "                       closest_labels_from_model,\n",
    "                       id_pred, label_pred,\n",
    "                       n_neighbours,\n",
    "                       c_features,\n",
    "                       distances_of_data_to_cc,\n",
    "                       cluster_index):\n",
    "  nearest_subcluster_index = np.argmin(distances_of_data_to_cc) # find nearest subcluster of the query\n",
    "  # add query data to the nearest/most-similar subcluster\n",
    "  c_features[nearest_subcluster_index] = np.concatenate((c_features[nearest_subcluster_index], np.expand_dims(query[\"image\"], axis=0)), axis=0)\n",
    "\n",
    "  id_pred[cluster_index].append(query[\"id\"])\n",
    "  # closest_labels_from_model.count(1)/n_neighbours --> Percentage that the model predict the data as positive (required to calculate AUC ROC value)\n",
    "  label_pred[cluster_index].append((query['id'], closest_labels_from_model.count(1)/n_neighbours))\n",
    "  return c_features, id_pred, label_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to correct mispredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the model predicted label with the ground truth and corrects only if it is a mistake\n",
    "def correct_mispredictions(query, human_expert, closest_labels_from_model, c_neg_features, c_pos_features, distances_of_data_to_neg_cc, distances_of_data_to_pos_cc, data_frame_mistake, mistake_index, corrected_count):\n",
    "  expert_label = human_expert.policy(query['id'])[0]\n",
    "  model_pred = get_label_with_max_freq(closest_labels_from_model)\n",
    "  if model_pred != expert_label:  # Misclassification: if model's decision is different than the ground truth.\n",
    "    corrected_count += 1\n",
    "    data_frame_mistake[\"Image name\"].append(query[\"filepath\"].split(\"/\")[-1])  # Recording to save it as csv file\n",
    "    data_frame_mistake[\"Mistake ID\"].append(query['id'])\n",
    "    data_frame_mistake[\"Original label\"].append(query['label'])\n",
    "    data_frame_mistake[\"Predicted label\"].append(get_label_with_max_freq(closest_labels_from_model))\n",
    "    data_frame_mistake[\"Mistake index\"].append(mistake_index)\n",
    "    if query[\"label\"] == 0:\n",
    "      c_neg_features.append(np.expand_dims(query[\"image\"], axis=0))  # Appending negative image to negative cluster\n",
    "    else:\n",
    "      c_pos_features.append(np.expand_dims(query[\"image\"], axis=0))  # Appending positive image to positive cluster\n",
    "\n",
    "  else: # Correct classification by model: concatenating the feature to the closest subsample.\n",
    "    if model_pred == 0:\n",
    "      nearest_subcluster_index = np.argmin(distances_of_data_to_neg_cc)  # find nearest subcluster of the query\n",
    "      # add query data to the nearest/most-similar subcluster\n",
    "      c_neg_features[nearest_subcluster_index] = np.concatenate((c_neg_features[nearest_subcluster_index], np.expand_dims(query[\"image\"], axis=0)), axis=0)\n",
    "    else:\n",
    "      nearest_subcluster_index = np.argmin(distances_of_data_to_pos_cc)  # find nearest subcluster of the query\n",
    "      # add query data to the nearest/most-similar subcluster\n",
    "      c_pos_features[nearest_subcluster_index] = np.concatenate((c_pos_features[nearest_subcluster_index], np.expand_dims(query[\"image\"], axis=0)), axis=0)\n",
    "\n",
    "  return corrected_count, data_frame_mistake, c_neg_features, c_pos_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calcuate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: raw dictionary (from pickle file)\n",
    "# cluster_centers_dict: dictionary of {0: [], 1:[]} ==> {0: cc_neg_features, 1: cc_pos_features}\n",
    "# distance_type: 1. Eucliddean, 2. Manhattan, 3. Cosine\n",
    "# label_pred: predicted label\n",
    "# c_neg_features\n",
    "# c_pos_features\n",
    "# n_neighbours: no. of neighbour\n",
    "# corrected_count: count of coorection of mispredictions\n",
    "# mistake_index: index of data to track the mistaken data\n",
    "# data_frame_mistake: to save data_frame in CSV\n",
    "# mentored_data: if mentored data or not\n",
    "def distance(query,\n",
    "             cluster_centers_dict,\n",
    "             distance_type, id_pred,\n",
    "             label_pred,\n",
    "             c_neg_features,\n",
    "             c_pos_features,\n",
    "             n_neighbours,\n",
    "             corrected_count,\n",
    "             mistake_index,\n",
    "             data_frame_mistake,\n",
    "             human_expert,\n",
    "             agent,\n",
    "             mentored_data):\n",
    "  expnd_query = np.expand_dims(query['image'], axis=0)\n",
    "  distances_of_data_to_neg_cc, distances_of_data_to_pos_cc = [], []\n",
    "\n",
    "  # len(cluster_centers_dict[0]) should have at least the number of sub-cluster\n",
    "  # Calculating the distance using numpy (axis=1) to calculate all at ones\n",
    "  if distance_type == 1: # Euclidean distance\n",
    "    if len(cluster_centers_dict[0]) > 1:\n",
    "      distances_of_data_to_neg_cc = np.linalg.norm(query['image'] - cluster_centers_dict[0], axis=1)\n",
    "    elif len(cluster_centers_dict[0]) == 1:\n",
    "      distances_of_data_to_neg_cc = [np.linalg.norm(query['image'] - cluster_centers_dict[0], axis=1)]\n",
    "    if len(cluster_centers_dict[1]) > 1:\n",
    "      distances_of_data_to_pos_cc = np.linalg.norm(query['image'] - cluster_centers_dict[1], axis=1)\n",
    "    elif len(cluster_centers_dict[1]) == 1:\n",
    "      distances_of_data_to_pos_cc = [np.linalg.norm(query['image'] - cluster_centers_dict[1], axis=1)]\n",
    "\n",
    "  elif distance_type == 2: # Manhattan distance\n",
    "    if len(cluster_centers_dict[0]) > 1:\n",
    "      distances_of_data_to_neg_cc = np.squeeze(manhattan_distances(cluster_centers_dict[0], expnd_query))\n",
    "    elif len(cluster_centers_dict[0]) == 1:\n",
    "      distances_of_data_to_neg_cc = [np.squeeze(manhattan_distances(cluster_centers_dict[0], expnd_query))]\n",
    "    if len(cluster_centers_dict[1]) > 1:\n",
    "      distances_of_data_to_pos_cc = np.squeeze(manhattan_distances(cluster_centers_dict[1], expnd_query))\n",
    "    elif len(cluster_centers_dict[1]) == 1:\n",
    "      distances_of_data_to_pos_cc = [np.squeeze(manhattan_distances(cluster_centers_dict[1], expnd_query))]\n",
    "\n",
    "  elif distance_type == 3: # Cosine distance\n",
    "    if len(cluster_centers_dict[0]) > 1:\n",
    "      distances_of_data_to_neg_cc = np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[0]))\n",
    "    elif len(cluster_centers_dict[0]) == 1:\n",
    "      distances_of_data_to_neg_cc = [np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[0]))]\n",
    "    if len(cluster_centers_dict[1]) > 1:\n",
    "      distances_of_data_to_pos_cc = np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[1]))\n",
    "    elif len(cluster_centers_dict[1]) == 1:\n",
    "      distances_of_data_to_pos_cc = [np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[1]))]\n",
    "\n",
    "  neg_distances_tup_list, pos_ditances_tup_list = [], []\n",
    "  for dist_single in distances_of_data_to_neg_cc:\n",
    "    neg_distances_tup_list.append((dist_single, 0))\n",
    "\n",
    "  for dist_single in distances_of_data_to_pos_cc:\n",
    "    pos_ditances_tup_list.append((dist_single, 1))\n",
    "\n",
    "  # concat all distances\n",
    "  pos_ditances_tup_list.extend(neg_distances_tup_list)\n",
    "  # sort distances from min to max result: ((0.1, 1), (0.2, 1), (0.3, 0), (0.4, 0), (0.5, 1))\n",
    "  all_distances_tup = sorted(pos_ditances_tup_list)[:n_neighbours]\n",
    "\n",
    "  # filter only n_neighbours elements\n",
    "  # all_distances_tup = all_distances_tup[:n_neighbours]\n",
    "\n",
    "  closest_labels_from_model = [label for (distance, label) in all_distances_tup]\n",
    "\n",
    "  if mentored_data:\n",
    "    (corrected_count,\n",
    "     data_frame_mistake,\n",
    "     c_neg_features,\n",
    "     c_pos_features) = correct_mispredictions(query,\n",
    "                                              human_expert,\n",
    "                                              closest_labels_from_model,\n",
    "                                              c_neg_features,\n",
    "                                              c_pos_features,\n",
    "                                              distances_of_data_to_neg_cc,\n",
    "                                              distances_of_data_to_pos_cc,\n",
    "                                              data_frame_mistake,\n",
    "                                              mistake_index,\n",
    "                                              corrected_count\n",
    "                                              )\n",
    "\n",
    "  else:\n",
    "    # label from model is cat\n",
    "    pred = get_label_with_max_freq(closest_labels_from_model)\n",
    "    expert_label = agent.policy(query['image'].reshape(1, -1))[0]\n",
    "    # if prediction is correct\n",
    "    if pred == expert_label:\n",
    "      final_label = pred\n",
    "    else:\n",
    "      corrected_count += 1\n",
    "      final_label = expert_label\n",
    "\n",
    "    if len(closest_labels_from_model) > 0 and final_label == 0:\n",
    "      c_neg_features, id_pred, label_pred = update_subclusters(query,\n",
    "                                                               closest_labels_from_model,\n",
    "                                                               id_pred,\n",
    "                                                               label_pred,\n",
    "                                                               n_neighbours,\n",
    "                                                               # negative params\n",
    "                                                               c_neg_features,\n",
    "                                                               distances_of_data_to_neg_cc,\n",
    "                                                               cluster_index=0\n",
    "                                                              )\n",
    "    else:\n",
    "      # label from model is positive\n",
    "      c_pos_features, id_pred, label_pred = update_subclusters(query,\n",
    "                                                               closest_labels_from_model,\n",
    "                                                               id_pred,\n",
    "                                                               label_pred,\n",
    "                                                               n_neighbours,\n",
    "                                                               # positive params\n",
    "                                                               c_pos_features,\n",
    "                                                               distances_of_data_to_pos_cc,\n",
    "                                                               cluster_index=1\n",
    "                                                              )\n",
    "\n",
    "  return data_frame_mistake, corrected_count, id_pred, label_pred, c_neg_features, c_pos_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active + Imitation Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*************** Training with 200 size of labled data ***************\n",
      "\n",
      "==================== 1/12 ====================\n",
      "mentored data: 200, test data: 50, unlabled data: 4750\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 150\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 150 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:04<00:00, 36.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 150 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [0.987]\n",
      " ############### Labeling 4750 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4750/4750 [00:44<00:00, 107.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 4750 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 2343  FP: 0\n",
      "FN: 44  TN: 2363\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.148\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.618\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.991\n",
      "AUC: 0.958\n",
      "\n",
      "==================== 2/12 ====================\n",
      "mentored data: 200, test data: 50, unlabled data: 4750\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 150\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 150 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:03<00:00, 38.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 150 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [0.987, 1.0]\n",
      " ############### Labeling 4750 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4750/4750 [00:42<00:00, 112.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 4750 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 2332  FP: 0\n",
      "FN: 47  TN: 2371\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.167\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.616\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.99\n",
      "AUC: 0.5\n",
      "\n",
      "==================== 3/12 ====================\n",
      "mentored data: 200, test data: 50, unlabled data: 4750\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 150\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 150 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:04<00:00, 36.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 150 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [0.987, 1.0, 0.947]\n",
      " ############### Labeling 4750 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4750/4750 [00:44<00:00, 107.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 4750 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 2279  FP: 0\n",
      "FN: 98  TN: 2373\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.157\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.616\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.979\n",
      "AUC: 0.977\n",
      "\n",
      "\n",
      "*************** Training with 400 size of labled data ***************\n",
      "\n",
      "==================== 4/12 ====================\n",
      "mentored data: 400, test data: 100, unlabled data: 4500\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 350\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 350 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 350/350 [00:23<00:00, 14.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 350 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [1.0]\n",
      " ############### Labeling 4500 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4500/4500 [00:43<00:00, 102.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 4500 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 2254  FP: 1\n",
      "FN: 9  TN: 2236\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.276\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.622\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.998\n",
      "AUC: 0.99\n",
      "\n",
      "==================== 5/12 ====================\n",
      "mentored data: 400, test data: 100, unlabled data: 4500\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 350\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 350 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 350/350 [00:23<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 350 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [1.0, 0.986]\n",
      " ############### Labeling 4500 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4500/4500 [00:44<00:00, 102.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 4500 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 2206  FP: 0\n",
      "FN: 40  TN: 2254\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.284\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.619\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.991\n",
      "AUC: 0.989\n",
      "\n",
      "==================== 6/12 ====================\n",
      "mentored data: 400, test data: 100, unlabled data: 4500\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 350\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 350 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 350/350 [00:21<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 350 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [1.0, 0.986, 0.994]\n",
      " ############### Labeling 4500 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4500/4500 [00:42<00:00, 105.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 4500 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 2243  FP: 0\n",
      "FN: 21  TN: 2236\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.148\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.616\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.995\n",
      "AUC: 0.966\n",
      "\n",
      "\n",
      "*************** Training with 800 size of labled data ***************\n",
      "\n",
      "==================== 7/12 ====================\n",
      "mentored data: 800, test data: 200, unlabled data: 4000\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 750\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 750 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 750/750 [02:19<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 750 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [0.952]\n",
      " ############### Labeling 4000 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:43<00:00, 92.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 4000 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 1848  FP: 0\n",
      "FN: 169  TN: 1983\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.206\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.666\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.958\n",
      "AUC: 0.996\n",
      "\n",
      "==================== 8/12 ====================\n",
      "mentored data: 800, test data: 200, unlabled data: 4000\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 750\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 750 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 750/750 [02:03<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 750 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [0.952, 0.967]\n",
      " ############### Labeling 4000 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:43<00:00, 92.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 4000 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 1889  FP: 0\n",
      "FN: 113  TN: 1998\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.147\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.615\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.972\n",
      "AUC: 0.998\n",
      "\n",
      "==================== 9/12 ====================\n",
      "mentored data: 800, test data: 200, unlabled data: 4000\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 750\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 750 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 750/750 [01:58<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 750 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [0.952, 0.967, 0.968]\n",
      " ############### Labeling 4000 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:42<00:00, 93.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 4000 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 1926  FP: 0\n",
      "FN: 96  TN: 1978\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.176\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.605\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.976\n",
      "AUC: 0.995\n",
      "\n",
      "\n",
      "*************** Training with 1600 size of labled data ***************\n",
      "\n",
      "==================== 10/12 ====================\n",
      "mentored data: 1600, test data: 400, unlabled data: 3000\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 1550\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 1550 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1550/1550 [10:07<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Mentoring and Demonstrating 1550 examples to Agent by Human Expert DONE!!! ###############\n",
      "Reward_list: [0.965]\n",
      " ############### Labeling 3000 unlabeled data with the help of Agent ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:36<00:00, 82.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Labeling 3000 unlabeled data with the help of Agent DONE!!! ###############\n",
      "TP: 1402  FP: 0\n",
      "FN: 95  TN: 1503\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.174\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.637\n",
      "Calculating Silhouette index...\n",
      "silhouette_score:  <function silhouette_score at 0x00000288C4D86550>\n",
      "Accuracy: 0.968\n",
      "AUC: 0.999\n",
      "\n",
      "==================== 11/12 ====================\n",
      "mentored data: 1600, test data: 400, unlabled data: 3000\n",
      "balanced data: 50, fpositive: 25, fnegative: 25\n",
      "mentored data: 1550\n",
      "Number of subclusters: 5\n",
      "Initial Demo data to agent: 50\n",
      "############### Mentoring and Demonstrating 1550 examples to Agent by Human Expert ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████▌   | 1481/1550 [09:47<00:59,  1.15it/s]"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME =  str(round(datetime.datetime.now().timestamp()))\n",
    "os.mkdir(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}\")\n",
    "n_neighbours = int(15 * DATASET_SIZE)\n",
    "INITAL_SAMPLE_SIZE = int(50 * DATASET_SIZE)\n",
    "mentored_size = [200, 400, 800, 1600] # 80 percent\n",
    "test_data_size = [50, 100, 200, 400] # 20 percent\n",
    "\n",
    "mentored_size = [int(size * DATASET_SIZE) for size in mentored_size]\n",
    "test_data_size = [int(size * DATASET_SIZE) for size in test_data_size]\n",
    "\n",
    "data_frame_metrix = {\n",
    "  \"Mentored Data\": [],\n",
    "  \"Dataset\": [],\n",
    "  \"Mentored Covid-ve\": [],\n",
    "  \"Mentored Covid+ve\": [],\n",
    "  \"Unlabeled Data\": [],\n",
    "  \"Accuracy\": [],\n",
    "  \"Specificity\": [],\n",
    "  \"Sensitivity\": [],\n",
    "  \"AUC\": [],\n",
    "  \"Dunn Index\": [],\n",
    "  \"Davies Bouldin Index\": [],\n",
    "  \"Silhouette Score\": [],\n",
    "  \"TP\": [],\n",
    "  \"TN\": [],\n",
    "  \"FP\": [],\n",
    "  \"FN\": [],\n",
    "  \"Human Corrected Count\": [],\n",
    "  \"Agent Corrected Count\": []\n",
    "}\n",
    "\n",
    "data_frame_agent = {\n",
    "  \"Accuracy\": [],\n",
    "  \"Precision\": [],\n",
    "  \"Recall\": [],\n",
    "  \"F1 Score\": [],\n",
    "  \"Confusion Matrix\": [],\n",
    "  \"Agent Reward\": []\n",
    "}\n",
    "\n",
    "global_count = 0\n",
    "\n",
    "for ds_index, size in enumerate(mentored_size):\n",
    "  agent = Agent()\n",
    "  print(\"\\n\\n{} Training with {} size of labled data {}\".format('*'*15, size, '*'*15))\n",
    "\n",
    "  test_data_set = ft_dataset[:test_data_size[ds_index]] # get test data set from total data set pool\n",
    "  remaining_data_set = ft_dataset[test_data_size[ds_index]:] # rest data from total data pool is used for training\n",
    "\n",
    "  mentored_data_sets, unlabeled_data_sets = data_loader(remaining_data_set, size)\n",
    "\n",
    "  # mentored_data_sets ==> three sets: [d1, d2, d3] ==> eg: [0-40, 320-360, 640-680] three epoc\n",
    "  for epoch, mentored_data in enumerate(mentored_data_sets):\n",
    "    global_count += 1\n",
    "    print(f\"\\n{'='*20} {global_count}/{len(mentored_size) * len(mentored_data_sets)} {'='*20}\")\n",
    "    data_frame_mistake = {\n",
    "      \"Image name\": [],\n",
    "      \"Mistake index\": [],\n",
    "      \"Mistake ID\": [],\n",
    "      \"Original label\": [],\n",
    "      \"Predicted label\": []\n",
    "    }\n",
    "\n",
    "    neg_img, pos_img = 0, 0\n",
    "\n",
    "    # collect the ground truth (label) of all the predicting images =>> key: 0 & 1 (class), value: tuple (data['id'], data['label']), required to calulate TP, FP, FN, TN\n",
    "    label_gt = {0: [], 1: []}\n",
    "    # collect the ground truth (id) of all the predicting images =>> key: 0 & 1 (class), value: ground truth id\n",
    "    id_gt = {0: [], 1: []}\n",
    "\n",
    "    # collect the predicted label for all the images =>> key: 0 & 1 (class), value: tuple(query['id'], decision_list.count(1)/n_neighbours)\n",
    "    # Percentage of predicted positive class, required to calculate AUC/ROC value\n",
    "    label_pred = {0: [], 1: []}\n",
    "    # collect the predicted id for all the images =>> key: 0 & 1 (class), value: predicted ids, required to calulate TP, FP, FN, TN\n",
    "    id_pred = {0: [],  1: []}\n",
    "\n",
    "    # feature label =>> key: 0 & 1 (class), value: deep feature of image\n",
    "    cluster_centers_dict = {0: [], 1: []}\n",
    "\n",
    "    unlabeled_data = unlabeled_data_sets[epoch]\n",
    "\n",
    "    print(f\"mentored data: {len(mentored_data)}, test data: {len(test_data_set)}, unlabled data: {len(unlabeled_data)}\")\n",
    "\n",
    "    neg_mentored_img, pos_mentored_img = 0, 0\n",
    "    for data in mentored_data:\n",
    "      if data['label'] == 0:\n",
    "          neg_mentored_img += 1\n",
    "      else:\n",
    "          pos_mentored_img += 1\n",
    "\n",
    "    # defining Human Expert\n",
    "    human_expert = HumanExpert(mentored_data)\n",
    "\n",
    "    # select balanced mentored data (50% from positive and 50% from negative)\n",
    "    fnegative = data_separation(mentored_data, 0, int(INITAL_SAMPLE_SIZE/2))  # Get the 'INITAL_SAMPLE_SIZE/2' negative features from 'mentored_data'\n",
    "    fpositive = data_separation(mentored_data, 1, int(INITAL_SAMPLE_SIZE/2))  # Get the 'INITAL_SAMPLE_SIZE/2' positive features from 'mentored_data'\n",
    "\n",
    "    print(f\"balanced data: {INITAL_SAMPLE_SIZE}, fpositive: {len(fpositive)}, fnegative: {len(fnegative)}\")\n",
    "    print(f\"mentored data: {len(mentored_data)}\")\n",
    "\n",
    "    n_subclusters = math.ceil(5 * DATASET_SIZE) if DATASET_SIZE > 0.5 else 2\n",
    "    print(\"Number of subclusters: {}\".format(n_subclusters))\n",
    "\n",
    "    # fit the feature data to the model\n",
    "    model_cluster = fit_to_clusters(fnegative + fpositive, 2)\n",
    "\n",
    "    cc_neg_features, c_neg_features = sub_clusters(fnegative, n_subclusters)  # Get the cluster center and negative clusters (Using K-means algorithm)\n",
    "    cc_pos_features, c_pos_features = sub_clusters(fpositive, n_subclusters)  # Get the cluster center and positive clusters (Using K-means algorithm)\n",
    "\n",
    "    # initial model\n",
    "    model_cluster.update(cc_neg_features, cc_pos_features, c_neg_features, c_pos_features)\n",
    "\n",
    "    n_obs, human_corrected_count, mistake_index, agent_action_count = 0, 0, INITAL_SAMPLE_SIZE, 0\n",
    "\n",
    "    # demonstrate to the RL Agent from labeled data by human\n",
    "    fdemo_data = [(init_fn, 0) for init_fn in fnegative]\n",
    "    pdemo_data = [(init_fp, 1) for init_fp in fpositive]\n",
    "    demo_data = fdemo_data + pdemo_data\n",
    "\n",
    "    shuffle(demo_data)\n",
    "\n",
    "    print(f\"Initial Demo data to agent: {len(demo_data)}\")\n",
    "\n",
    "    # initialize dagger data by initial demo data\n",
    "    dagger_data = copy.deepcopy(demo_data)\n",
    "\n",
    "    # demonstrate agent by initial demo data\n",
    "    agent.imitate_human(dagger_data)\n",
    "\n",
    "    print(f\"{'#' * 15} Mentoring and Demonstrating {len(mentored_data)} examples to Agent by Human Expert {'#' * 15}\")\n",
    "\n",
    "    agent.reset_reward()\n",
    "\n",
    "    # loop is for the mentored data as well as imitation learning --> Notice mentored_data=True in argument of the function call distance.\n",
    "    for state in tqdm(mentored_data):\n",
    "      n_obs += 1\n",
    "      cluster_centers_dict = {0: cc_neg_features, 1: cc_pos_features}\n",
    "\n",
    "      (data_frame_mistake,\n",
    "       human_corrected_count,\n",
    "       _,\n",
    "       label_pred,\n",
    "       c_neg_features,\n",
    "       c_pos_features) = distance(state,\n",
    "                                 cluster_centers_dict,\n",
    "                                 distance_type,\n",
    "                                 id_pred, # not being used in this case\n",
    "                                 label_pred,\n",
    "                                 c_neg_features,\n",
    "                                 c_pos_features,\n",
    "                                 n_neighbours,\n",
    "                                 human_corrected_count,\n",
    "                                 mistake_index,\n",
    "                                 data_frame_mistake,\n",
    "                                 human_expert,\n",
    "                                 agent=None,\n",
    "                                 mentored_data=True)\n",
    "\n",
    "      cc_neg_features, cc_pos_features = get_updated_cc(c_neg_features, c_pos_features)  # Get the mean of the features\n",
    "      mistake_index += 1\n",
    "\n",
    "      # Updating the kmeans model (including labels and clusters) every 10 data [this update is optional]\n",
    "      if n_obs%10 == 0 or n_obs == len(mentored_data):\n",
    "        model_cluster.update(cc_neg_features, cc_pos_features, c_neg_features, c_pos_features)\n",
    "\n",
    "      # dagger step\n",
    "\n",
    "      # query to human for actions\n",
    "      state_id = state['id']\n",
    "      state_obs = state['image']\n",
    "      expert_actions = human_expert.policy(state_id) # ground truth from human expert\n",
    "\n",
    "      # actions from agent itself\n",
    "      agent_actions = agent.policy(state_obs.reshape(1, -1)) # changing (n, ) to (1, n) and send to agent model\n",
    "      agent.update_reward(agent_actions[0], expert_actions[0], n_obs)\n",
    "\n",
    "      # if agent start to perform bad, use expert's label in new dataset to train the agent.\n",
    "      if agent.avg_reward < 0.95: # (max reward is 1, worst is 0)\n",
    "        actions = expert_actions # actions from human expert\n",
    "      else:\n",
    "        actions = agent_actions # action from agent\n",
    "\n",
    "      # aggregate the data\n",
    "      dagger_data.append((state_obs, actions[0]))\n",
    "\n",
    "      # train model with aggregated data\n",
    "      agent.imitate_human(dagger_data)\n",
    "\n",
    "\n",
    "    print(f\"{'#' * 15} Mentoring and Demonstrating {len(mentored_data)} examples to Agent by Human Expert DONE!!! {'#' * 15}\")\n",
    "\n",
    "    agent.update_reward_list()\n",
    "    print(\"Reward_list:\", agent.reward_list)\n",
    "\n",
    "    data_f_mistake = pd.DataFrame.from_dict(data_frame_mistake)\n",
    "    data_f_mistake.to_csv(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}/mistake_{size}_d{epoch + 1}.csv\", index=False)\n",
    "\n",
    "    test_obs = []\n",
    "    gt_actions = []\n",
    "    for data in test_data_set:\n",
    "      test_obs.append(data['image'])\n",
    "      gt_actions.append(data['label'])\n",
    "\n",
    "    # predict on test observations\n",
    "    test_preds = agent.policy(np.array(test_obs))\n",
    "\n",
    "    agent_accuracy = accuracy_score(gt_actions, test_preds)\n",
    "    agent_precision = precision_score(gt_actions, test_preds, average='weighted')\n",
    "    agent_recall = recall_score(gt_actions, test_preds, average='weighted')\n",
    "    agent_f1 = f1_score(gt_actions, test_preds, average='weighted')\n",
    "    agent_conf_matrix = confusion_matrix(gt_actions, test_preds)\n",
    "\n",
    "    data_frame_agent[\"Accuracy\"].append(round(agent_accuracy, 3))\n",
    "    data_frame_agent[\"Precision\"].append(round(agent_precision, 3))\n",
    "    data_frame_agent[\"Recall\"].append(round(agent_recall, 3))\n",
    "    data_frame_agent[\"F1 Score\"].append(round(agent_f1, 3))\n",
    "    data_frame_agent[\"Confusion Matrix\"].append(agent_conf_matrix)\n",
    "    data_frame_agent[\"Agent Reward\"].append(agent.avg_reward)\n",
    "\n",
    "\n",
    "    data_f_agent = pd.DataFrame.from_dict(data_frame_agent)\n",
    "    data_f_agent.to_csv(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}/agent_evaluation.csv\", index=False)\n",
    "\n",
    "    agent_corrected_count = 0\n",
    "    print(f\" {'#' * 15} Labeling {len(unlabeled_data)} unlabeled data with the help of Agent {'#' * 15}\")\n",
    "    # loop is for the unlabled data --> Notice mentored_data=False in argument of the function call distance.\n",
    "    for new_data in tqdm(unlabeled_data):\n",
    "      if new_data[\"label\"] == 1:\n",
    "        id_gt[1].append(new_data['id'])\n",
    "        label_gt[1].append((new_data['id'], new_data['label'])) # Required to calulate TP, FP, FN, TN\n",
    "      else:\n",
    "        id_gt[0].append(new_data['id'])\n",
    "        label_gt[0].append((new_data['id'], new_data['label']))\n",
    "\n",
    "      cluster_centers_dict = {0: cc_neg_features, 1: cc_pos_features}\n",
    "\n",
    "      (_,\n",
    "       agent_corrected_count,\n",
    "       id_pred,\n",
    "       label_pred,\n",
    "       c_neg_features,\n",
    "       c_pos_features) = distance(new_data,\n",
    "                                 cluster_centers_dict,\n",
    "                                 distance_type,\n",
    "                                 id_pred,\n",
    "                                 label_pred,\n",
    "                                 c_neg_features,\n",
    "                                 c_pos_features,\n",
    "                                 n_neighbours,\n",
    "                                 agent_corrected_count,\n",
    "                                 mistake_index,\n",
    "                                 data_frame_mistake,\n",
    "                                 human_expert,\n",
    "                                 agent=agent,\n",
    "                                 mentored_data=False)\n",
    "\n",
    "      cc_neg_features, cc_pos_features = get_updated_cc(c_neg_features, c_pos_features)   # Get the mean of the features\n",
    "\n",
    "    print(f\" {'#' * 15} Labeling {len(unlabeled_data)} unlabeled data with the help of Agent DONE!!! {'#' * 15}\")\n",
    "\n",
    "    accuracy, specificity, sensitivity, TP, TN, FP, FN = classification_metrices(id_gt, id_pred)\n",
    "\n",
    "    # Flattened as required to calculate clustering indices\n",
    "    flattened_neg_features = flatten_features(c_neg_features)\n",
    "    flattened_pos_features = flatten_features(c_pos_features)\n",
    "\n",
    "    dunn_index, davies_bouldin_index, silhouette__score = cluster_metrices(flattened_neg_features, flattened_pos_features)\n",
    "    cl_auc = roc_auc_curve(label_gt, label_pred)\n",
    "\n",
    "    data_frame_metrix[\"Dataset\"].append(f\"M_{ds_index + 1}_D_{epoch + 1}\")\n",
    "    data_frame_metrix[\"Mentored Data\"].append(size)\n",
    "    data_frame_metrix[\"Mentored Covid+ve\"].append(neg_mentored_img)\n",
    "    data_frame_metrix[\"Mentored Covid-ve\"].append(pos_mentored_img)\n",
    "    data_frame_metrix[\"Unlabeled Data\"].append(len(unlabeled_data))\n",
    "    data_frame_metrix[\"Accuracy\"].append(accuracy)\n",
    "    data_frame_metrix[\"Specificity\"].append(specificity)\n",
    "    data_frame_metrix[\"Sensitivity\"].append(sensitivity)\n",
    "    data_frame_metrix[\"AUC\"].append(cl_auc)\n",
    "    data_frame_metrix[\"Dunn Index\"].append(dunn_index)\n",
    "    data_frame_metrix[\"Davies Bouldin Index\"].append(davies_bouldin_index)\n",
    "    data_frame_metrix[\"Silhouette Score\"].append(silhouette__score)\n",
    "    data_frame_metrix[\"TP\"].append(TP)\n",
    "    data_frame_metrix[\"TN\"].append(TN)\n",
    "    data_frame_metrix[\"FP\"].append(FP)\n",
    "    data_frame_metrix[\"FN\"].append(FN)\n",
    "    data_frame_metrix[\"Human Corrected Count\"].append(human_corrected_count)\n",
    "    data_frame_metrix[\"Agent Corrected Count\"].append(agent_corrected_count)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"AUC: {cl_auc}\")\n",
    "    data_f_matrix = pd.DataFrame.from_dict(data_frame_metrix)\n",
    "    data_f_matrix.to_csv(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}/cluster_model_evaluation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
