{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "from statistics import mode\n",
    "\n",
    "# OpenCV\n",
    "import cv2\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import manhattan_distances, cosine_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 2 #int(input(\"Enter the number for: \\n 1) VGGNET16 \\n 2) Resnet101  \\n 3) Densenet169 \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For traning speed, define DATASET_SHRINK_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SHRINK_FACTOR = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open extracted feature in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../pickle_files/al/ct_scan/\"\n",
    "if selected_model == 1:\n",
    "  filename = \"ct_scan_vggnet16.pickle\"\n",
    "elif selected_model == 2:\n",
    "  filename = \"ct_scan_resnet101.pickle\"\n",
    "elif selected_model == 3:\n",
    "  filename = \"ct_scan_densenet169.pickle\"\n",
    "\n",
    "file = filepath + filename\n",
    "with open(file, 'rb') as handle:\n",
    "  all_ft_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the data and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset: 10000\n",
      "Sample feature dataset Covid: {'id': 3771, 'filepath': './ct_scan_dataset/3A_images/NCP_509_2175_0024.png', 'image': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'label': 1}\n",
      "Sample feature dataset Non-Covid: {'id': 9791, 'filepath': './ct_scan_dataset/3A_images/radiopaedia-50806-56291-0-0034.png', 'image': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'label': 0}\n",
      "Selected Dataset: 100\n"
     ]
    }
   ],
   "source": [
    "# suffle the data\n",
    "random.seed(42)\n",
    "shuffle(all_ft_dataset)\n",
    "\n",
    "print(\"Total Dataset: {}\".format(len(all_ft_dataset)))\n",
    "print(\"Sample feature dataset Covid: {}\".format(all_ft_dataset[0]))\n",
    "print(\"Sample feature dataset Non-Covid: {}\".format(all_ft_dataset[5001]))\n",
    "\n",
    "# shrink for minimize training time\n",
    "original_data_size = len(all_ft_dataset)\n",
    "new_data_size = int(original_data_size * DATASET_SHRINK_FACTOR)\n",
    "ft_dataset = all_ft_dataset[:new_data_size]\n",
    "\n",
    "print(\"Selected Dataset: {}\".format(len(ft_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to correct mispredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_mispredictions(query, feat_label, train_label, train_id, index_data, decision, data_frame_mistake, count):\n",
    "  if mode(decision) != query[\"label\"]:\n",
    "    count += 1\n",
    "    data_frame_mistake[\"Image name\"].append(query[\"filepath\"].split(\"/\")[-1])\n",
    "    data_frame_mistake[\"Mistake ID\"].append(query['id'])\n",
    "    data_frame_mistake[\"Original label\"].append(query['label'])\n",
    "    data_frame_mistake[\"Predicted label\"].append(mode(decision))\n",
    "    data_frame_mistake[\"Mistake index\"].append(index_data)\n",
    "    feat_label[query['label']].append(query[\"image\"])\n",
    "    train_label[query['label']].append(query[\"label\"])\n",
    "    train_id[query['label']].append(query['id'])\n",
    "\n",
    "  else:\n",
    "    feat_label[query['label']].append(query[\"image\"])\n",
    "    train_label[query['label']].append(query[\"label\"])\n",
    "    train_id[query['label']].append(query['id'])\n",
    "  return count, data_frame_mistake, feat_label, train_label, train_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calcuate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: raw dictionary (from pickle file)\n",
    "# feat_label: dictionary of {0: [], 1:[]}\n",
    "# distance_type: 1. Eucliddean, 2. Manhattan, 3. Cosine\n",
    "# id_pred: predicted id\n",
    "# label_pred: predicted label\n",
    "# n_neighbours: no. of neighbour\n",
    "# count:\n",
    "# train_label: training label\n",
    "# train_id: trainin id\n",
    "# index_data: index of data to track the mistaken data\n",
    "# data_frame_mistake: to save data_frame in CSV\n",
    "# supervised_data: if mentored data or not\n",
    "def distance(query, feat_label, distance_type, id_pred, label_pred, n_neighbours, count, train_label, train_id, index_data, data_frame_mistake, supervised_data):\n",
    "  expnd_query = np.expand_dims(query['image'], axis=0)\n",
    "  pos_tup, neg_tup = [], []\n",
    "  pos_dist, neg_dist = [], []\n",
    "\n",
    "  # Calculating the distance using numpy (axis=1) to calculate all at ones\n",
    "  if distance_type == 1: # Euclidean distance\n",
    "    if len(feat_label[0]) > 1:\n",
    "      neg_dist = np.linalg.norm(query['image'] - feat_label[0], axis=1)\n",
    "    elif len(feat_label[0]) == 1:\n",
    "      neg_dist = [np.linalg.norm(query['image'] - feat_label[0], axis=1)]\n",
    "    if len(feat_label[1]) > 1:\n",
    "      pos_dist = np.linalg.norm(query['image'] - feat_label[1], axis=1)\n",
    "    elif len(feat_label[1]) == 1:\n",
    "      pos_dist = [np.linalg.norm(query['image'] - feat_label[1], axis=1)]\n",
    "\n",
    "  elif distance_type == 2: # Manhattan distance\n",
    "    if len(feat_label[0]) > 1:\n",
    "      neg_dist = np.squeeze(manhattan_distances(feat_label[0], expnd_query))\n",
    "    elif len(feat_label[0]) == 1:\n",
    "      neg_dist = [np.squeeze(manhattan_distances(feat_label[0], expnd_query))]\n",
    "    if len(feat_label[1]) > 1:\n",
    "      pos_dist = np.squeeze(manhattan_distances(feat_label[1], expnd_query))\n",
    "    elif len(feat_label[1]) == 1:\n",
    "      pos_dist = [np.squeeze(manhattan_distances(feat_label[1], expnd_query))]\n",
    "\n",
    "  elif distance_type == 3: # Cosine distance\n",
    "    if len(feat_label[0]) > 1:\n",
    "      neg_dist = np.squeeze(cosine_distances(expnd_query, feat_label[0]))\n",
    "    elif len(feat_label[0]) == 1:\n",
    "      neg_dist = [np.squeeze(cosine_distances(expnd_query, feat_label[0]))]\n",
    "    if len(feat_label[1]) > 1:\n",
    "      pos_dist = np.squeeze(cosine_distances(expnd_query, feat_label[1]))\n",
    "    elif len(feat_label[1]) == 1:\n",
    "      pos_dist = [np.squeeze(cosine_distances(expnd_query, feat_label[1]))]\n",
    "\n",
    "  for dist_single in pos_dist:\n",
    "    pos_tup.append((dist_single, 1))\n",
    "\n",
    "  for dist_single in neg_dist:\n",
    "    neg_tup.append((dist_single, 0))\n",
    "\n",
    "  pos_tup.extend(neg_tup)\n",
    "  tup_dist = sorted(pos_tup)[:n_neighbours]\n",
    "\n",
    "  decision = [y for (x,y) in tup_dist]\n",
    "  if supervised_data:\n",
    "    (count,\n",
    "     data_frame_mistake,\n",
    "     feat_label,\n",
    "     train_label,\n",
    "     train_id) = correct_mispredictions(query,\n",
    "                                        feat_label,\n",
    "                                        train_label,\n",
    "                                        train_id,\n",
    "                                        index_data,\n",
    "                                        decision,\n",
    "                                        data_frame_mistake,\n",
    "                                        count)\n",
    "\n",
    "  else:\n",
    "    if len(decision) > 0 and mode(decision) == 0:\n",
    "      feat_label[0].append(query[\"image\"])\n",
    "      id_pred[0].append(query[\"id\"])\n",
    "      label_pred[0].append((query['id'], decision.count(1)/n_neighbours))\n",
    "    else:\n",
    "      feat_label[1].append(query[\"image\"])\n",
    "      id_pred[1].append(query[\"id\"])\n",
    "      label_pred[1].append((query['id'],decision.count(1)/n_neighbours))\n",
    "\n",
    "  return feat_label, id_pred, label_pred, data_frame_mistake, count, train_label, train_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate the classification metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrices(id_gt, id_pred):\n",
    "  TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "  # TP --> when correctly classified covid\n",
    "  for tp in id_pred[1]:\n",
    "    if tp in id_gt[1]:\n",
    "      TP += 1\n",
    "\n",
    "  # TN --> when correctly classified healthy (non-covid)\n",
    "  for tn in id_pred[0]:\n",
    "    if tn in id_gt[0]:\n",
    "      TN += 1\n",
    "\n",
    "  # FP --> when incorrectly classified healthy (Classified healthy as covid)\n",
    "  for fp in id_pred[1]:\n",
    "    if fp in id_gt[0]:\n",
    "      FP += 1\n",
    "\n",
    "  # FN --> when missed covid classification (Covid cases missed)\n",
    "  for fn in id_pred[0]:\n",
    "    if fn in id_gt[1]:\n",
    "      FN += 1\n",
    "\n",
    "  accuracy = round((TP + TN) / (TP + TN + FP + FN), 3)\n",
    "  if (TN + FP) > 0:\n",
    "    specificity = round(TN / (TN + FP), 3)\n",
    "  else:\n",
    "    specificity = 0 # Infinity\n",
    "\n",
    "  if (TP + FN) > 0:\n",
    "    sensitivity = round((TP) / (TP + FN), 3)\n",
    "  else:\n",
    "    sensitivity = 0 # Infinity\n",
    "\n",
    "  # f1_score = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "  print(\"TP: {}  FP: {}\".format(TP, FP))\n",
    "  print(\"FN: {}  TN: {}\".format(FN, TN))\n",
    "\n",
    "  return accuracy, specificity, sensitivity, TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_curve(label_gt, label_pred):\n",
    "  # contains (id, labels) tuple of binary class\n",
    "  gt_labels = sorted(label_gt[0] + label_gt[1])\n",
    "\n",
    "  # contains (id, labels) tuple of binary class --> sorted to match each element in gt_labels and pred_labels\n",
    "  pred_labels = sorted(label_pred[0] + label_pred[1])\n",
    "  y_test = [y for (x,y) in gt_labels] # Get only the labels\n",
    "  y_scores = [y for (x,y) in pred_labels]\n",
    "  fpr, tpr, threshold = roc_curve(y_test, y_scores)\n",
    "  roc_auc = round(auc(fpr, tpr), 3)\n",
    "  return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate Cluster metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_metrices(feat_label, train_label, id_pred):\n",
    "  print(\"Calculating Dunn's index...\")\n",
    "  dunn_index, davies_bouldin_index, silhouette_index = \"NA\", \"NA\", \"NA\"\n",
    "  if len(feat_label[0]) > 0 and len(feat_label[1]) > 0:\n",
    "    intra_dist1 = euclidean_distances(feat_label[0]).max()\n",
    "    intra_dist2 = euclidean_distances(feat_label[1]).max()\n",
    "    inter_dist = euclidean_distances(feat_label[0], feat_label[1]).min()\n",
    "\n",
    "    if intra_dist1 > intra_dist2:\n",
    "      max_intra_dist= intra_dist1\n",
    "    else:\n",
    "      max_intra_dist = intra_dist2\n",
    "\n",
    "    dunn_index = round(inter_dist / max_intra_dist, 3)\n",
    "\n",
    "  print(\"dunn_index: \", dunn_index)\n",
    "\n",
    "  # Davies Bouldin and Silhouette score from sklearn library.\n",
    "  class_0 = np.concatenate((np.zeros(shape=(len(train_label[0])), dtype=int), np.zeros(shape=(len(id_pred[0])), dtype=int)))\n",
    "  class_1 = np.concatenate((np.ones(shape=(len(train_label[1])), dtype=int), np.ones(shape=(len(id_pred[1])), dtype=int)))\n",
    "  class_all = np.concatenate((class_0, class_1))\n",
    "  if len(feat_label[0]) > 0 and len(feat_label[1]) > 0:\n",
    "    feature_all = np.concatenate((feat_label[0], feat_label[1]))\n",
    "    print(\"Calculating Davies Bouldin index...\")\n",
    "    davies_bouldin_index = round(davies_bouldin_score(feature_all, class_all), 3)\n",
    "    print(\"davies_bouldin_index: \", davies_bouldin_index)\n",
    "\n",
    "    print(\"Calculating Silhouette index...\")\n",
    "    silhouette_index = round(silhouette_score(feature_all, class_all), 3)\n",
    "    print(\"silhouette_index: \", silhouette_index)\n",
    "\n",
    "\n",
    "  return dunn_index, davies_bouldin_index, silhouette_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to load dataset into three different segment (k-way n-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to return three sets (n, 3200, 6400)  of labeled dataset for experiment\n",
    "def data_loader(dataset, n):\n",
    "  labeled_data, unlabeled_data = [], []\n",
    "\n",
    "  l_data = dataset[:n]                          # First case (0-40) // labeled + mentored\n",
    "  ul_data = dataset[n:]                         # First case (40-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[0] => dataset[0-40]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[0] => dataset[40-1000]\n",
    "\n",
    "  size_second_set = int(3200 * DATASET_SHRINK_FACTOR) # 3200 * 0.1 = 320\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Second case (320-360) // labeled + mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Second case (0-320) + (360-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[1] => dataset[320-360]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[1] => dataset[0-320] + dataset[360-1000]\n",
    "\n",
    "  size_second_set = int(6400 * DATASET_SHRINK_FACTOR) # 6400 * 0.1 = 640\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Third case (640-680) // labeled + mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Third case (0-640) + (680-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[2] => dataset[640-680]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[2] => dataset[0-640] + dataset[680-1000]\n",
    "  return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to separate data into positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that selects number of data samples and removes the selected data from the dataset. (required to select balanced positive and negative samples)\n",
    "def data_separation(dataset, taken_data_idx, label=None, data_sample=100):\n",
    "  add_data = []\n",
    "  for i, data in enumerate(dataset):\n",
    "    if i in taken_data_idx:\n",
    "      continue\n",
    "    if label > -1 and dataset[i][\"label\"] == label:\n",
    "      add_data.append(data)\n",
    "      taken_data_idx.append(i)\n",
    "    else:\n",
    "      add_data.append(data)\n",
    "      taken_data_idx.append(i)\n",
    "    if len(add_data) == data_sample:\n",
    "      break\n",
    "  return add_data, taken_data_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance type (Euclidean Manhattan or Consine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_type = 3 # int(input(\"Enter the number for: \\n 1) Euclidean  \\n 2) Manhattan \\n 3) Cosine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Distance Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model == 1:\n",
    "  s_model = 'vggnet16'\n",
    "elif selected_model == 2:\n",
    "  s_model = 'resnet101'\n",
    "elif selected_model == 3:\n",
    "  s_model = 'densenet169'\n",
    "\n",
    "if distance_type == 1:\n",
    "  s_distance = 'euclidean'\n",
    "elif distance_type == 2:\n",
    "  s_distance = 'manhattan'\n",
    "elif distance_type == 3:\n",
    "  s_distance = 'cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== 1/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 1\n",
      "feature pos: 1\n",
      "supervised data: 0\n",
      "unlabeled data: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:01<00:00, 65.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 0  FP: 0\n",
      "FN: 54  TN: 44\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.605\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  0.929\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.029\n",
      "Dataset: d_0 \t\t Labeled image: 2 \t\t Corrected count: 0\n",
      "Accuracy: 0.449 \t\t Specificity: 1.0 \t\t Sensitivity: 0.0\n",
      "Dunn index: 0.6050000190734863 \t Davies Bouldin: 0.929 \t Silhouette index: 0.028999999165534973\n",
      "AUC: 0.549\n",
      "============================== 2/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 0\n",
      "feature pos: 2\n",
      "supervised data: 0\n",
      "unlabeled data: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:01<00:00, 77.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 53  FP: 45\n",
      "FN: 0  TN: 0\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  NA\n",
      "Dataset: d_1 \t\t Labeled image: 2 \t\t Corrected count: 0\n",
      "Accuracy: 0.541 \t\t Specificity: 0.0 \t\t Sensitivity: 1.0\n",
      "Dunn index: NA \t Davies Bouldin: NA \t Silhouette index: NA\n",
      "AUC: 0.512\n",
      "============================== 3/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 1\n",
      "feature pos: 1\n",
      "supervised data: 0\n",
      "unlabeled data: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:01<00:00, 76.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 0  FP: 0\n",
      "FN: 54  TN: 44\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.515\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  1.245\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  -0.1\n",
      "Dataset: d_2 \t\t Labeled image: 2 \t\t Corrected count: 0\n",
      "Accuracy: 0.449 \t\t Specificity: 1.0 \t\t Sensitivity: 0.0\n",
      "Dunn index: 0.5149999856948853 \t Davies Bouldin: 1.245 \t Silhouette index: -0.10000000149011612\n",
      "AUC: 0.541\n",
      "============================== 4/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 1\n",
      "feature pos: 1\n",
      "supervised data: 2\n",
      "unlabeled data: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 77.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 53  FP: 43\n",
      "FN: 0  TN: 0\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.614\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  1.953\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.05\n",
      "Dataset: d_0 \t\t Labeled image: 4 \t\t Corrected count: 1\n",
      "Accuracy: 0.552 \t\t Specificity: 0.0 \t\t Sensitivity: 1.0\n",
      "Dunn index: 0.6140000224113464 \t Davies Bouldin: 1.953 \t Silhouette index: 0.05000000074505806\n",
      "AUC: 0.475\n",
      "============================== 5/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 0\n",
      "feature pos: 2\n",
      "supervised data: 2\n",
      "unlabeled data: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 72.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 52  FP: 44\n",
      "FN: 0  TN: 0\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.591\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  0.973\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.003\n",
      "Dataset: d_1 \t\t Labeled image: 4 \t\t Corrected count: 1\n",
      "Accuracy: 0.542 \t\t Specificity: 0.0 \t\t Sensitivity: 1.0\n",
      "Dunn index: 0.5910000205039978 \t Davies Bouldin: 0.973 \t Silhouette index: 0.003000000026077032\n",
      "AUC: 0.49\n",
      "============================== 6/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 1\n",
      "feature pos: 1\n",
      "supervised data: 2\n",
      "unlabeled data: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 78.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 53  FP: 43\n",
      "FN: 0  TN: 0\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.575\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.599\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  -0.006\n",
      "Dataset: d_2 \t\t Labeled image: 4 \t\t Corrected count: 2\n",
      "Accuracy: 0.552 \t\t Specificity: 0.0 \t\t Sensitivity: 1.0\n",
      "Dunn index: 0.574999988079071 \t Davies Bouldin: 2.599 \t Silhouette index: -0.006000000052154064\n",
      "AUC: 0.573\n",
      "============================== 7/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 1\n",
      "feature pos: 1\n",
      "supervised data: 6\n",
      "unlabeled data: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:01<00:00, 72.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 50  FP: 41\n",
      "FN: 0  TN: 1\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.614\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.162\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.069\n",
      "Dataset: d_0 \t\t Labeled image: 8 \t\t Corrected count: 3\n",
      "Accuracy: 0.554 \t\t Specificity: 0.024 \t\t Sensitivity: 1.0\n",
      "Dunn index: 0.6140000224113464 \t Davies Bouldin: 3.162 \t Silhouette index: 0.0689999982714653\n",
      "AUC: 0.506\n",
      "============================== 8/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 0\n",
      "feature pos: 2\n",
      "supervised data: 6\n",
      "unlabeled data: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:01<00:00, 71.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 49  FP: 43\n",
      "FN: 0  TN: 0\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.591\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.172\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.007\n",
      "Dataset: d_1 \t\t Labeled image: 8 \t\t Corrected count: 2\n",
      "Accuracy: 0.533 \t\t Specificity: 0.0 \t\t Sensitivity: 1.0\n",
      "Dunn index: 0.5910000205039978 \t Davies Bouldin: 2.172 \t Silhouette index: 0.007000000216066837\n",
      "AUC: 0.49\n",
      "============================== 9/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 1\n",
      "feature pos: 1\n",
      "supervised data: 6\n",
      "unlabeled data: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 92/92 [00:01<00:00, 74.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 50  FP: 36\n",
      "FN: 1  TN: 5\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.593\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.984\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.047\n",
      "Dataset: d_2 \t\t Labeled image: 8 \t\t Corrected count: 4\n",
      "Accuracy: 0.598 \t\t Specificity: 0.122 \t\t Sensitivity: 0.98\n",
      "Dunn index: 0.5929999947547913 \t Davies Bouldin: 3.984 \t Silhouette index: 0.04699999839067459\n",
      "AUC: 0.626\n",
      "============================== 10/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 1\n",
      "feature pos: 1\n",
      "supervised data: 13\n",
      "unlabeled data: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 85/85 [00:01<00:00, 70.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 45  FP: 39\n",
      "FN: 1  TN: 0\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.494\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  4.325\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.034\n",
      "Dataset: d_0 \t\t Labeled image: 15 \t\t Corrected count: 5\n",
      "Accuracy: 0.529 \t\t Specificity: 0.0 \t\t Sensitivity: 0.978\n",
      "Dunn index: 0.49399998784065247 \t Davies Bouldin: 4.325 \t Silhouette index: 0.03400000184774399\n",
      "AUC: 0.572\n",
      "============================== 11/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 0\n",
      "feature pos: 2\n",
      "supervised data: 13\n",
      "unlabeled data: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 85/85 [00:01<00:00, 67.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 44  FP: 40\n",
      "FN: 0  TN: 1\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.585\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  3.436\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  -0.005\n",
      "Dataset: d_1 \t\t Labeled image: 15 \t\t Corrected count: 4\n",
      "Accuracy: 0.529 \t\t Specificity: 0.024 \t\t Sensitivity: 1.0\n",
      "Dunn index: 0.5849999785423279 \t Davies Bouldin: 3.436 \t Silhouette index: -0.004999999888241291\n",
      "AUC: 0.523\n",
      "============================== 12/12 ==============================\n",
      "training data: 2\n",
      "feature neg: 1\n",
      "feature pos: 1\n",
      "supervised data: 13\n",
      "unlabeled data: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 85/85 [00:01<00:00, 72.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 44  FP: 27\n",
      "FN: 4  TN: 10\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.568\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  4.203\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.047\n",
      "Dataset: d_2 \t\t Labeled image: 15 \t\t Corrected count: 6\n",
      "Accuracy: 0.635 \t\t Specificity: 0.27 \t\t Sensitivity: 0.917\n",
      "Dunn index: 0.5680000185966492 \t Davies Bouldin: 4.203 \t Silhouette index: 0.04699999839067459\n",
      "AUC: 0.664\n"
     ]
    }
   ],
   "source": [
    "n_neighbours = 5 # 31\n",
    "labeled_size = [200, 400, 800, 1550]\n",
    "labeled_size = [int(size * DATASET_SHRINK_FACTOR) for size in labeled_size]\n",
    "data_frame_metrix = {\n",
    "  \"Labeled data\": [],\n",
    "  \"Dataset\": [],\n",
    "  \"Accuracy\": [],\n",
    "  \"Specificity\": [],\n",
    "  \"Sensitivity\": [],\n",
    "  \"AUC\":[],\n",
    "  \"Dunn index\": [],\n",
    "  \"Davies Bouldin\": [],\n",
    "  \"Silhouette index\":[],\n",
    "  \"TP\":[],\n",
    "  \"TN\":[],\n",
    "  \"FP\":[],\n",
    "  \"FN\":[],\n",
    "  \"pos_labeled_img\":[],\n",
    "  \"neg_labeled_img\":[],\n",
    "  \"corrected_count\":[]\n",
    "}\n",
    "\n",
    "global_count = 0\n",
    "\n",
    "for size in labeled_size:\n",
    "  labeled_data_sets, unlabeled_data_sets = data_loader(ft_dataset, size)\n",
    "\n",
    "  # labeled_data_sets ==> three sets: [d1, d2, d3] ==> eg: [0-40, 320-360, 640-680]\n",
    "  for dataset_type, labeled_data in enumerate(labeled_data_sets):\n",
    "    global_count += 1\n",
    "    print(f\"============================== {global_count}/{len(labeled_size) * len(labeled_data_sets)} ==============================\")\n",
    "    data_frame_mistake = {\n",
    "      \"Image name\": [],\n",
    "      \"Mistake index\": [],\n",
    "      \"Mistake ID\": [],\n",
    "      \"Original label\": [],\n",
    "      \"Predicted label\": []\n",
    "    }\n",
    "\n",
    "    pos_img, neg_img = 0, 0\n",
    "\n",
    "    # collect the ground truth (label) of all the predicting images =>> key: 0 & 1 (class), value: tuple (data['id'], data['label']), required to calulate TP, FP, FN, TN\n",
    "    label_gt = {0: [], 1: []}\n",
    "    # collect the ground truth (id) of all the predicting images =>> key: 0 & 1 (class), value: ground truth id\n",
    "    id_gt = {0: [], 1: []}\n",
    "\n",
    "    # collect the predicted label for all the images =>> key: 0 & 1 (class), value: tuple(query['id'], decision.count(1)/n_neighbours) --> decision.count(1)/n_neighbours: Percentage of predicted positive class, required to calculate AUC/ROC value\n",
    "    label_pred = {0: [], 1: []}\n",
    "    # collect the predicted id for all the images =>> key: 0 & 1 (class), value: predicted ids, required to calulate TP, FP, FN, TN\n",
    "    id_pred = {0: [],  1: []}\n",
    "\n",
    "    # feature label =>> key: 0 & 1 (class), value: deep feature of image\n",
    "    feat_label = {0: [], 1: []}\n",
    "\n",
    "    # train id =>> key: 0 & 1 (class), value: id of images --> Not required for, but scared to delete. \n",
    "    train_id = {0: [], 1: []}\n",
    "    # train lable =>> key: 0 & 1 (class), value: ground truth labels\n",
    "    train_label = {0: [], 1: []}\n",
    "\n",
    "    # select balanced labeled data (50% from positive and 50% from negative) and initialize training data from a few sample and rest data as supervised data\n",
    "    sample_data_size = int(200 * DATASET_SHRINK_FACTOR) # 200\n",
    "    training_data_pos, taken_data_idx = data_separation(labeled_data, [], 0, int(sample_data_size/2))\n",
    "    training_data_neg, taken_data_idx = data_separation(labeled_data, taken_data_idx, 1, int(sample_data_size/2))\n",
    "    training_data = training_data_pos + training_data_neg\n",
    "\n",
    "    sample_data_size_supervised = len(labeled_data) - sample_data_size\n",
    "    supervised_data, taken_data_idx = data_separation(labeled_data, taken_data_idx, -1, sample_data_size_supervised)\n",
    "\n",
    "    print(f\"training data: {len(training_data)}\")\n",
    "\n",
    "    for data in training_data:\n",
    "      if data[\"label\"] == 1:\n",
    "        feat_label[1].append(data['image'])\n",
    "        train_id[1].append(data['id'])\n",
    "        train_label[1].append((data['id'],data['label']))\n",
    "        pos_img += 1\n",
    "      else:\n",
    "        feat_label[0].append(data['image'])\n",
    "        train_id[0].append(data['id'])\n",
    "        train_label[0].append((data['id'],data['label']))\n",
    "        neg_img += 1\n",
    "\n",
    "    print(f\"feature neg: {neg_img}\")\n",
    "    print(f\"feature pos: {pos_img}\")\n",
    "\n",
    "    count, index_data = 0, 200\n",
    "\n",
    "    print(f\"supervised data: {len(supervised_data)}\")\n",
    "\n",
    "    # loop is for the mentored data --> Notice mentored_data=True in argument of the function call distance.\n",
    "    for data in supervised_data:\n",
    "      (feat_label,\n",
    "       id_pred,\n",
    "       label_pred,\n",
    "       data_frame_mistake,\n",
    "       count, train_label,\n",
    "       train_id) = distance(data,\n",
    "                           feat_label,\n",
    "                           distance_type,\n",
    "                           id_pred,\n",
    "                           label_pred,\n",
    "                           n_neighbours,\n",
    "                           count,\n",
    "                           train_label,\n",
    "                           train_id,\n",
    "                           index_data,\n",
    "                           data_frame_mistake,\n",
    "                           supervised_data=True)\n",
    "      index_data += 1\n",
    "\n",
    "    data_f_mistake = pd.DataFrame.from_dict(data_frame_mistake)\n",
    "    data_f_mistake.to_csv(f\"../test/ct_scan/{s_model}_{s_distance}_mistake_{size}_{dataset_type}.csv\", index=False)\n",
    "\n",
    "    print(f\"unlabeled data: {len(unlabeled_data_sets[dataset_type])}\")\n",
    "\n",
    "    # loop is for the test data --> Notice mentored_data=False in argument of the function call distance.\n",
    "    for data in tqdm(unlabeled_data_sets[dataset_type]):\n",
    "      if data[\"label\"] == 1:\n",
    "        id_gt[1].append(data['id'])\n",
    "        label_gt[1].append((data['id'], data['label']))\n",
    "      else:\n",
    "        id_gt[0].append(data['id'])\n",
    "        label_gt[0].append((data['id'], data['label']))\n",
    "\n",
    "      feat_label, id_pred, label_pred, _, _, _, _ = distance(data,\n",
    "                                                             feat_label,\n",
    "                                                             distance_type,\n",
    "                                                             id_pred,\n",
    "                                                             label_pred,\n",
    "                                                             n_neighbours,\n",
    "                                                             count=None,\n",
    "                                                             train_label=None,\n",
    "                                                             train_id=None,\n",
    "                                                             index_data=None,\n",
    "                                                             data_frame_mistake=None,\n",
    "                                                             supervised_data=False)\n",
    "\n",
    "    accuracy, specificity, sensitivity, TP, TN, FP, FN = classification_metrices(id_gt, id_pred)\n",
    "    dunn_index, davies_bouldin_index, silhouette_index = cluster_metrices(feat_label, train_label, id_pred)\n",
    "    cl_auc = roc_auc_curve(label_gt, label_pred)\n",
    "\n",
    "    data_frame_metrix[\"Labeled data\"].append(size)\n",
    "    data_frame_metrix[\"Dataset\"].append(f\"d_{dataset_type}\")\n",
    "    data_frame_metrix[\"Accuracy\"].append(accuracy)\n",
    "    data_frame_metrix[\"Specificity\"].append(specificity)\n",
    "    data_frame_metrix[\"Sensitivity\"].append(sensitivity)\n",
    "    data_frame_metrix[\"AUC\"].append(cl_auc)\n",
    "    data_frame_metrix[\"Dunn index\"].append(dunn_index)\n",
    "    data_frame_metrix[\"Davies Bouldin\"].append(davies_bouldin_index)\n",
    "    data_frame_metrix[\"Silhouette index\"].append(silhouette_index)\n",
    "    data_frame_metrix[\"TP\"].append(TP)\n",
    "    data_frame_metrix[\"TN\"].append(TN)\n",
    "    data_frame_metrix[\"FP\"].append(FP)\n",
    "    data_frame_metrix[\"FN\"].append(FN)\n",
    "    data_frame_metrix[\"pos_labeled_img\"].append(pos_img)\n",
    "    data_frame_metrix[\"neg_labeled_img\"].append(neg_img)\n",
    "    data_frame_metrix[\"corrected_count\"].append(count)\n",
    "\n",
    "    print(f\"Dataset: d_{dataset_type} \\t\\t Labeled image: {size} \\t\\t Corrected count: {count}\")\n",
    "    print(f\"Accuracy: {accuracy} \\t\\t Specificity: {specificity} \\t\\t Sensitivity: {sensitivity}\")\n",
    "    print(f\"Dunn index: {dunn_index} \\t Davies Bouldin: {davies_bouldin_index} \\t Silhouette index: {silhouette_index}\")\n",
    "    print(f\"AUC: {cl_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f_matrix = pd.DataFrame.from_dict(data_frame_metrix)\n",
    "data_f_matrix.to_csv(f\"../test/ct_scan/{s_model}_{s_distance}_dist.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
