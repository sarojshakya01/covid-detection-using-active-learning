{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required python libraries         \n",
    "import numpy as np         \n",
    "import os                  \n",
    "from random import shuffle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "# OpenCV and scikit-learn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import manhattan_distances, cosine_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2 \n",
    "\n",
    "# Pandas \n",
    "# import pandas as pd\n",
    "\n",
    "# Tensorflow\n",
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "# from tensorflow.keras.applications.densenet import DenseNet169\n",
    "# from tensorflow.keras.applications.vgg16 import VGG16\n",
    "# from tensorflow.keras.applications.resnet import ResNet101 \n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laoading the pickle file (list of dictionary) consisting of features of image obtained from the mentioned model\n",
    "with open('../../pickle_files/al/ct_scan/ct_scan_resnet101.pickle','rb') as handle:\n",
    "   t_dataset  = pickle.load(handle)  # Type of t_dataset: python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the loaded list\n",
    "random.seed(42)\n",
    "shuffle(t_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3771,\n",
       " 'filepath': './ct_scan_dataset/3A_images/NCP_509_2175_0024.png',\n",
       " 'image': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first element\n",
    "t_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sub_clusters(features):\n",
    "#     sum_of_squared_distances = []\n",
    "#     no_cluster=[]\n",
    "#     K=range(2,70,2)\n",
    "#     for num_clusters in K:\n",
    "#         kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_init=\"auto\")\n",
    "#         kmeans.fit(features)\n",
    "#         no_cluster.append(num_clusters)\n",
    "#         sum_of_squared_distances.append(kmeans.inertia_)\n",
    "#     print(sum_of_squared_distances)\n",
    "#     output= kmeans.labels_\n",
    "#     clusters = [np.squeeze(np.array(features)[[np.where(output==i)[0]]],axis=0) for i in range(len(np.unique(output)))]\n",
    "#     return kmeans.cluster_centers_, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create subclusters \n",
    "def sub_clusters(features):\n",
    "    kmeans = KMeans(n_clusters=70, random_state=0, n_init=\"auto\").fit(features) # Number of cluster defined from elbow method.\n",
    "    output= kmeans.labels_       # list of labels for elements occuring in each cluster \n",
    "    clusters = [np.squeeze(np.array(features)[[np.where(output==i)[0]]],axis=0) for i in range(len(np.unique(output)))] # Form clusters of deep features of image\n",
    "    return kmeans.cluster_centers_, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that selects 100 data samples and removes the selected data from the dataset. (required to select balanced positive and negative samples)\n",
    "def data_separation(dataset,label):\n",
    "    add_data= []\n",
    "    i=0\n",
    "    while len(add_data)!=100:\n",
    "        if dataset[i][\"label\"]==label:\n",
    "            add_data.append(dataset[i]['image'])\n",
    "            del dataset[i]\n",
    "        i+=1\n",
    "    return add_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the mean of each subclusters. (required as mean is the representative of that subcluster)\n",
    "def mean_features(positive, negative):\n",
    "    # print(f\"pure_pf: {positive}\")\n",
    "    # print(f\"p_type: {type(positive)}\")\n",
    "    # print(f\"len_p: {len(positive)}\")\n",
    "    mpos_features=np.array([np.mean(i,axis=0) for i in positive])  # Mean of all positive sub clusters \n",
    "    mneg_features=np.array([np.mean(i,axis=0) for i in negative])  # Mean of all negative sub clusters\n",
    "    # print(mpos_features)\n",
    "    return mpos_features, mneg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that updates the subcluster by concatenating the test data sample to the most similar subcluster.\n",
    "def update_subclusters(all_dist, query, fea_label, id_pred, label_pred, features, decision, n_neighbours, cluster):\n",
    "    min_ind=np.argmin(all_dist)    # Get the index of the minimum distance to update the most similar (or nearest) subcluster.\n",
    "    features[min_ind]=np.concatenate((features[min_ind],np.expand_dims(query[\"image\"], axis=0)),axis=0)  # Concatenate on the most similar subcluster\n",
    "    # fea_label[cluster].append(np.expand_dims(query[\"image\"], axis=0))  # Have doubt here\n",
    "    id_pred[cluster].append(query[\"id\"])\n",
    "    label_pred[cluster].append((query['id'],decision.count(1)/n_neighbours))  # decision.count(1)/n_neighbours --> Percentage that the model predict the data as positive (required to calculate AUC ROC value)\n",
    "    return features, fea_label, id_pred, label_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the model predicted label with the ground truth and corrects only if it is a mistake\n",
    "def correct_mispredictions(query, fea_label,train_label, train_id, ind_data, decision,data_frame_1, count, pos_dist, neg_dist, pos_features, neg_features):\n",
    "    \n",
    "    if mode(decision) != query[\"label\"]:  # Misclassification: if model's decision is different than the ground truth.\n",
    "        # print(\"here\")\n",
    "        count +=1    \n",
    "        data_frame_1[\"Image name\"].append(query[\"filepath\"].split(\"/\")[-1])  # Recording to save it as csv file\n",
    "        data_frame_1[\"Mistake ID\"].append(query['id'])\n",
    "        data_frame_1[\"Original label\"].append(query['label'])\n",
    "        data_frame_1[\"Predicted label\"].append(mode(decision))\n",
    "        data_frame_1[\"Mistake index\"].append(ind_data)\n",
    "        if query[\"label\"]==1:\n",
    "            pos_features_list= list(pos_features)  # converting numpy array into list to append \n",
    "            pos_features_list.append(np.expand_dims(query[\"image\"], axis=0))   # Appending positive image to positive cluster (can use numpy function to do it without converting to list)\n",
    "            pos_features = np.array(pos_features_list)\n",
    "            # pos_features= np.concatenate((pos_features,np.expand_dims(query[\"image\"], axis=0)),axis=0)\n",
    "        else:\n",
    "            neg_features_list= list(neg_features) # Same as above, but for negative class.\n",
    "            neg_features_list.append(np.expand_dims(query[\"image\"], axis=0))\n",
    "            neg_features = np.array(neg_features_list)\n",
    "            # neg_features = np.concatenate((neg_features,np.expand_dims(query[\"image\"], axis=0)),axis=0)\n",
    "        train_label[query['label']].append(query[\"label\"])\n",
    "        train_id[query['label']].append(query['id'])\n",
    "\n",
    "    else:\n",
    "        if query['label'] == 0:  # Correct classification by model: concatenating the feature to the closest subsample. \n",
    "            min_ind = np.argmin(neg_dist)  # Measuring the minimum distance with the representation (or mean) of subcluster, recording index of the nearest subcluster.\n",
    "            neg_features[min_ind] = np.concatenate((neg_features[min_ind],np.expand_dims(query[\"image\"], axis=0)),axis=0)\n",
    "            # fea_label[query['label']].append(np.concatenate((fea_label[query['label']],np.expand_dims(query[\"image\"], axis=0)),axis=0))\n",
    "        else:\n",
    "            min_ind = np.argmin(pos_dist)\n",
    "\n",
    "            pos_features[min_ind] = np.concatenate((pos_features[min_ind],np.expand_dims(query[\"image\"], axis=0)),axis=0)\n",
    "            # fea_label[query['label']].append(np.concatenate((mpos_features,np.expand_dims(query[\"image\"], axis=0)),axis=0))\n",
    "        train_label[query['label']].append(query[\"label\"])\n",
    "        train_id[query['label']].append(query['id'])\n",
    "    return count,data_frame_1,fea_label,train_label,train_id,pos_features,neg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure distance of test data sample with each element in the training dataset (comparision occurs either with representative of subcluster or the element itself).\n",
    "def distance2(query, fea_label, select_distance, id_pred, label_pred, n_neighbours, count, train_label, train_id, ind_data, data_frame_1, pos_features, neg_features, mentored_data): # Query is the raw dictionary (from pickle file) // fea_label is dictionary of {0: [], 1:[]} (distance) // select distance is int\n",
    "  exp_query = np.expand_dims(query['image'], axis=0)\n",
    "  pos_tup, neg_tup = [], []\n",
    "\n",
    "  if select_distance==1: # Euclidean distance\n",
    "    # print(f\"Type: {type(fea_label[0])}\")\n",
    "    # print(f\"Shape: {fea_label[0].shape}\")\n",
    "    neg_dist = np.linalg.norm(query['image']- fea_label[0], axis=1)  # Calculating the Euclidean distance using numpy (axis=1) to calculate all at ones   \n",
    "    pos_dist = np.linalg.norm(query['image']- fea_label[1], axis=1)\n",
    "\n",
    "  elif select_distance==2: # Manhattan distance\n",
    "    neg_dist = np.squeeze(manhattan_distances(fea_label[0],exp_query))  # convert (1,n) to (,n), where n is the number of element in the list (or numpy array)\n",
    "    pos_dist=np.squeeze(manhattan_distances(fea_label[1],exp_query))\n",
    "\n",
    "  elif select_distance==3: # Cosine distance\n",
    "    neg_dist = np.squeeze(cosine_distances(exp_query,fea_label[0]))  # convert (1,n) to (,n), where n is the number of element in the list (or numpy array)\n",
    "    pos_dist=np.squeeze(cosine_distances(exp_query,fea_label[1]))\n",
    "  \n",
    "  for dist_single in pos_dist:  # Creating tuple (distance, label) to calculate AUC ROC value\n",
    "    # print(dist_single)\n",
    "    pos_tup.append((dist_single,1))\n",
    "\n",
    "  for dist_single in neg_dist:    # Creating tuple (distance, label) to calculate AUC ROC value\n",
    "    neg_tup.append((dist_single,0))\n",
    "\n",
    "  pos_tup.extend(neg_tup)    # Extending the positive and negative tuple \n",
    "  tup_dist = sorted(pos_tup)[:n_neighbours]  # sorting the tuple according to distance to proceed with K Nearest Neighbor\n",
    "  \n",
    "  decision = [y for (x,y) in tup_dist]  # Decision contains list of labels\n",
    "\n",
    "  if mentored_data:  # Check/compare the output of the model for mentored data. \n",
    "    count,data_frame_1,fea_label,train_label,train_id, pos_features,neg_features=correct_mispredictions(query, fea_label,train_label,train_id, ind_data, decision,data_frame_1, count, pos_dist, neg_dist, pos_features, neg_features)\n",
    "    \n",
    "  else: # For data sample other than the mentored, proceed with the following code. \n",
    "    if decision.count(0) > decision.count(1): \n",
    "      neg_features, fea_label, id_pred, label_pred = update_subclusters(neg_dist,query,fea_label,id_pred,label_pred,neg_features, decision, n_neighbours, cluster=0)\n",
    "      \n",
    "    else:\n",
    "      pos_features, fea_label, id_pred, label_pred = update_subclusters(pos_dist,query,fea_label,id_pred,label_pred,pos_features, decision,n_neighbours, cluster=1)\n",
    "  \n",
    "  return id_pred, label_pred, data_frame_1, count, train_label, train_id, pos_features, neg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the metrices.\n",
    "def classification_metrics(label_gt,id_pred):\n",
    "  TP,FP,FN,TN = 0,0,0,0\n",
    "\n",
    "  for tp in id_pred[1]:   # TP --> When correctly classified covid #id_pred: dictionary\n",
    "    if tp in label_gt[1]:\n",
    "      TP +=1\n",
    "\n",
    "  for tn in id_pred[0]:  # TN --> When correctly classified healthy (non-covid)\n",
    "    if tn in label_gt[0]:\n",
    "      TN +=1\n",
    "\n",
    "  for fp in id_pred[1]: # FP --> When incorrectly classified healthy (Classified healthy as covid)\n",
    "    if fp in label_gt[0]:\n",
    "      FP +=1\n",
    "\n",
    "  for fn in id_pred[0]: # FN --> When missed covid classification (Covid cases missed)\n",
    "    if fn in label_gt[1]:\n",
    "      FN +=1\n",
    "\n",
    "  accuracy= (TP+TN)/(TP+TN+FP+FN)\n",
    "  specificity = TN/(TN+FP)\n",
    "  sensitivity = (TP)/(TP+FN)\n",
    "  # f1_score = (2*precision*recall)/(precision + recall)\n",
    "  \n",
    "  print(\"TP: \", TP)\n",
    "  print(\"FP: \", FP)\n",
    "  print(\"FN: \", FN)\n",
    "  print(\"TN: \", TN)\n",
    "\n",
    "  return accuracy, specificity, sensitivity,TP,TN,FP,FN\n",
    "\n",
    "def roc_auc_curve(label_gt,label_pred):\n",
    "  gt_labels= sorted(label_gt[0]+ label_gt[1])  # Contains (id,labels) tuple of binary class \n",
    "  pred_labels = sorted(label_pred[0]+label_pred[1]) # Contains (id,labels) tuple of binary class --> sorted to match each element in gt_labels and pred_labels\n",
    "  y_test = [y for (x,y) in gt_labels]   # Get only the labels\n",
    "  y_scores = [y for (x,y) in pred_labels]\n",
    "  fpr, tpr, threshold = roc_curve(y_test, y_scores)\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  return roc_auc\n",
    "\n",
    "def cluster_metrics(pos_features, neg_features, train_label,id_pred):\n",
    "  print(\"Calculating Dunn's index...\")\n",
    "  intra_dist1 = euclidean_distances(neg_features).max()\n",
    "  intra_dist2 = euclidean_distances(pos_features).max()\n",
    "  inter_dist = euclidean_distances(neg_features,pos_features).min()\n",
    "\n",
    "  if intra_dist1>intra_dist2:\n",
    "    max_intra_dist= intra_dist1  \n",
    "  else:\n",
    "    max_intra_dist = intra_dist2 \n",
    "\n",
    "  Dunn_index = inter_dist/max_intra_dist\n",
    "\n",
    "  print(\"Calculating Davies Bouldin index...\")\n",
    "\n",
    "  # Davies Bouldin and Silhouette score from sklearn library.\n",
    "  # class_0 =np.concatenate((np.zeros(shape=(len(train_label[0])),dtype=int),np.zeros(shape=(len(id_pred[0])),dtype=int),np.zeros(shape=(20),dtype=int)))\n",
    "  # class_1 = np.concatenate((np.ones(shape=(len(train_label[1])),dtype=int),np.ones(shape=(len(id_pred[1])),dtype=int),np.zeros(shape=(20),dtype=int)))\n",
    "  class_0 =np.zeros(shape=(len(neg_features)),dtype=int)\n",
    "  class_1 = np.ones(shape=(len(pos_features)),dtype=int)\n",
    "  class_all = np.concatenate((class_0,class_1))\n",
    "  feature_all = np.concatenate((neg_features,pos_features))\n",
    "\n",
    "  davies_bouldin_index = davies_bouldin_score(feature_all,class_all)\n",
    "  silhouette_index = silhouette_score(feature_all,class_all)\n",
    "\n",
    "  print(\"davies: \", davies_bouldin_index)\n",
    "  print(\"silhouette_sklearn: \", silhouette_index)\n",
    "  \n",
    "  return Dunn_index,davies_bouldin_index, silhouette_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_size = [400,800,1600,3200] # Labeled size consists of different mentored tiers (including the initial labeled data, e.g, labeled size of 400 consists of 200 labeled and 200 mentored data). \n",
    "\n",
    "# labeled_size = [3200]\n",
    "def data_loader(dataset,n): # Method to return three sets of labeled dataset for experiment\n",
    "  labeled_data, unlabeled_data = [], [] \n",
    "\n",
    "  l_data = dataset[:n]    # First case // labeled + mentored\n",
    "  ul_data = dataset[n:]   # First case // unlabeled\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "\n",
    "  l_data = dataset[3200:3200+n]    # Second case // labeled + mentored\n",
    "  ul_data = dataset[:3200]+dataset[3200+n:]   # Second case // unlabeled\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "\n",
    "  l_data = dataset[6400:6400+n]    # Third case // labeled + mentored\n",
    "  ul_data = dataset[:6400]+dataset[6400+n:]     # Third case // unlabeled\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "  return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to calculate the clustering indices\n",
    "def flatten_features(features):\n",
    "    all_features = []\n",
    "    for i in features:\n",
    "        for j in i:\n",
    "            all_features.append(j)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (71,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 101\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m labeled_data[select]:\n\u001b[0;32m     98\u001b[0m     fea_label\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: mneg_features,\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;241m1\u001b[39m: mpos_features}\n\u001b[1;32m--> 101\u001b[0m     id_pred, label_pred, data_frame_1, count, train_label, train_id, pos_features, neg_features\u001b[38;5;241m=\u001b[39m \u001b[43mdistance2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfea_label\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mid_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_neighbours\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_frame_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmentored_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     mpos_features, mneg_features \u001b[38;5;241m=\u001b[39m mean_features(pos_features, neg_features)    \u001b[38;5;66;03m# Get the mean of the features\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     ind_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m, in \u001b[0;36mdistance2\u001b[1;34m(query, fea_label, select_distance, id_pred, label_pred, n_neighbours, count, train_label, train_id, ind_data, data_frame_1, pos_features, neg_features, mentored_data)\u001b[0m\n\u001b[0;32m     30\u001b[0m decision \u001b[38;5;241m=\u001b[39m [y \u001b[38;5;28;01mfor\u001b[39;00m (x,y) \u001b[38;5;129;01min\u001b[39;00m tup_dist]  \u001b[38;5;66;03m# Decision contains list of labels\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mentored_data:  \u001b[38;5;66;03m# Check/compare the output of the model for mentored data. \u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m   count,data_frame_1,fea_label,train_label,train_id, pos_features,neg_features\u001b[38;5;241m=\u001b[39m\u001b[43mcorrect_mispredictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfea_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_frame_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# For data sample other than the mentored, proceed with the following code. \u001b[39;00m\n\u001b[0;32m     36\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m decision\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m decision\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;241m1\u001b[39m): \n",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m, in \u001b[0;36mcorrect_mispredictions\u001b[1;34m(query, fea_label, train_label, train_id, ind_data, decision, data_frame_1, count, pos_dist, neg_dist, pos_features, neg_features)\u001b[0m\n\u001b[0;32m     18\u001b[0m     neg_features_list\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(neg_features) \u001b[38;5;66;03m# Same as above, but for negative class.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     neg_features_list\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mexpand_dims(query[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 20\u001b[0m     neg_features \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_features_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# neg_features = np.concatenate((neg_features,np.expand_dims(query[\"image\"], axis=0)),axis=0)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m train_label[query[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mappend(query[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (71,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "n_neighbours=25\n",
    "\n",
    "# Dataframe (dictionary) to save the result in csv format.\n",
    "data_frame = {\"Labeled data\": [],\n",
    "              \"Dataset\": [],\n",
    "              \"Accuracy\": [],\n",
    "              \"Specificity\": [],\n",
    "              \"Sensitivity\": [],\n",
    "              \"AUC\":[],\n",
    "              \"Dunn index\": [],\n",
    "              \"Davies Bouldin\": [],\n",
    "              \"Silhouette index\":[],\n",
    "              \"TP\":[],\n",
    "              \"TN\":[],\n",
    "              \"FP\":[],\n",
    "              \"FN\":[],\n",
    "              \"pos_labeled_img\":[],\n",
    "              \"neg_labeled_img\":[],\n",
    "              \"corrected_count\":[]\n",
    "    \n",
    "}\n",
    "# fea_label1={0: [],\n",
    "#             1:[]}\n",
    "\n",
    "\n",
    "for size in labeled_size: # Select each size [400, 800,1600, 3200]\n",
    "  labeled_data, unlabeled_data = data_loader(t_dataset, size)\n",
    "#   print(f\"labeled data length {len(labeled_data)}\")\n",
    "#   print(f\"Unlabeled data length {len(unlabeled_data)}\")\n",
    "  select=0         # To select the dataset out of three cases ==> three cases: [d1, d2, d3] ==> eg: [3200,3200,3200]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  while(select < 3): # Save the mistake information as csv file\n",
    "    data_frame_1 = {  \"Image name\": [],\n",
    "                  \"Mistake index\": [],\n",
    "                  \"Mistake ID\": [],\n",
    "                  \"Original label\": [],\n",
    "                  \"Predicted label\": []\n",
    "                  \n",
    "    }\n",
    "    pos_img, neg_img=0, 0\n",
    "\n",
    "    fpos, fneg= [], []   \n",
    "\n",
    "    label_gt = {0: [],    # key: 0 & 1 (class), value: tuple (data['id'],data['label']), required to calulate TP, FP, FN, TN\n",
    "        1 :[]}    \n",
    "                            \n",
    "    train_label = {0: [],    # # key: 0 & 1 (class), value: ground truth labels\n",
    "        1 :[]}    \n",
    "\n",
    "    label_pred = {0: [],  # key: 0 & 1 (class), value: tuple(query['id'],decision.count(1)/n_neighbours) --> decision.count(1)/n_neighbours: Percentage of predicted positive class, required to calculate AUC/roc value\n",
    "        1 :[]}              \n",
    "\n",
    "    id_gt = {0: [], #  key: 0 & 1 (class), value: ground truth id  (Collect the ground truth (id) of all the predicting images)\n",
    "            1: [] }         \n",
    "    \n",
    "    id_pred = {0: [],  #  key: 0 & 1 (class), value: predicted ids, required to calulate TP, FP, FN, TN\n",
    "               1: []\n",
    "               }\n",
    "    fea_label = {0: [],  #  key: 0 & 1 (class), value: deep feature of image\n",
    "            1: []}\n",
    "\n",
    "    train_id ={0: [],  #  key: 0 & 1 (class), value: id of images   --> Not required for , but scared to delete. \n",
    "            1:[]}\n",
    "        \n",
    "    # print(type(labeled_data[0][0]))\n",
    "    # for data in labeled_data[select]:\n",
    "    #     if data[\"label\"] == 1:\n",
    "    #         fpos.append(data['image'])\n",
    "    #         train_id[1].append(data['id'])\n",
    "    #         train_label[1].append((data['id'],data['label']))\n",
    "    #         pos_img +=1\n",
    "\n",
    "    #     else:\n",
    "    #         fneg.append(data['image'])\n",
    "    #         train_id[0].append(data['id'])\n",
    "    #         train_label[0].append((data['id'],data['label']))\n",
    "    #         neg_img +=1\n",
    "\n",
    "    # print(f\"Blen: {len(labeled_data[select])}\")\n",
    "    \n",
    "    # Select 200 labeled data\n",
    "    fpositive = data_separation(labeled_data[select],1)    # Get the balanced positive features\n",
    "    fnegative = data_separation(labeled_data[select],0)  # Get the balanced negative features\n",
    "\n",
    "\n",
    "    mneg_features,neg_features= sub_clusters(fnegative)  # Get the subclusters (Using K-means algorithm)\n",
    "    mpos_features,pos_features= sub_clusters(fpositive)  # Get the subclusters (Using K-means algorithm)\n",
    "\n",
    "        \n",
    "\n",
    "    count, ind_data=0, 200    # Count to record the number of mistakes // ind_data: required to keep track of the indexes the model makes mistakes.\n",
    "    \n",
    "    # This loop is for the mentored data --> Notice mentored_data = True in argument of the function call distance2().\n",
    "    for data in labeled_data[select]:\n",
    "        fea_label={0: mneg_features,\n",
    "            1: mpos_features}\n",
    "        \n",
    "        id_pred, label_pred, data_frame_1, count, train_label, train_id, pos_features, neg_features= distance2(data,fea_label,3,id_pred,label_pred,n_neighbours, count, train_label, train_id, ind_data, data_frame_1, pos_features, neg_features, mentored_data=True)\n",
    "        mpos_features, mneg_features = mean_features(pos_features, neg_features)    # Get the mean of the features\n",
    "        ind_data +=1\n",
    "\n",
    "    data_f_1 = pd.DataFrame.from_dict(data_frame_1)\n",
    "    data_f_1.to_csv(f\"../subclusters_csv/ct-scans/mistake/densenet169_cosine_mistake_{size}_{select}.csv\",index=False) # Save the misclassiciation records\n",
    "\n",
    "   # This loop is for the test data --> Notice mentored_data = False in argument of the function call distance2().\n",
    "    for data in tqdm(unlabeled_data[select]):\n",
    "      if data[\"label\"]==1:\n",
    "        id_gt[1].append(data['id'])\n",
    "        label_gt[1].append((data['id'],data['label']))   # Required to calulate TP, FP, FN, TN\n",
    "      \n",
    "      else:\n",
    "        id_gt[0].append(data['id'])\n",
    "        label_gt[0].append((data['id'],data['label']))\n",
    "      \n",
    "      fea_label={0: mneg_features,\n",
    "            1: mpos_features}\n",
    "\n",
    "      id_pred, label_pred, _, _, _, _, pos_features, neg_features = distance2(data,fea_label,3,id_pred,label_pred,n_neighbours, count, train_label, train_id, ind_data, data_frame_1, pos_features, neg_features,mentored_data=False) # ind_data is the index of misclassification\n",
    "      mpos_features, mneg_features = mean_features(pos_features, neg_features)    # Get the mean of the features\n",
    "\n",
    "    accuracy, specificity, sensitivity,TP,TN,FP,FN= classification_metrics(id_gt,id_pred)\n",
    "    flattened_pos_features = flatten_features(pos_features) # Flattened as required to calculate clustering indices.\n",
    "    flattened_neg_features = flatten_features(neg_features)\n",
    "    dunn_index, davies_bouldin_index, silhouette_index = cluster_metrics(flattened_pos_features, flattened_neg_features, train_label,id_pred)\n",
    "    cl_auc = roc_auc_curve(label_gt,label_pred)\n",
    "    \n",
    "    data_frame[\"Labeled data\"].append(size)\n",
    "    data_frame[\"Dataset\"].append(f\"d_{select}\")\n",
    "    data_frame[\"Accuracy\"].append(accuracy)\n",
    "    data_frame[\"Specificity\"].append(specificity)\n",
    "    data_frame[\"Sensitivity\"].append(sensitivity)\n",
    "    data_frame[\"AUC\"].append(cl_auc)\n",
    "    data_frame[\"Dunn index\"].append(dunn_index)\n",
    "    data_frame[\"Davies Bouldin\"].append(davies_bouldin_index)\n",
    "    data_frame[\"Silhouette index\"].append(silhouette_index)\n",
    "    data_frame[\"TP\"].append(TP)\n",
    "    data_frame[\"TN\"].append(TN)\n",
    "    data_frame[\"FP\"].append(FP)\n",
    "    data_frame[\"FN\"].append(FN)\n",
    "    data_frame[\"pos_labeled_img\"].append(pos_img)\n",
    "    data_frame[\"neg_labeled_img\"].append(neg_img)\n",
    "    data_frame[\"corrected_count\"].append(count)\n",
    "\n",
    "    print(f\"Labeled image: {size} \\t Dataset: d_{select} \\t Accuracy: {accuracy} \\t Specificity: {specificity} \\t Sensitivity: {sensitivity} \\t Dunn index: {dunn_index}  \\t Davies Bouldin: {davies_bouldin_index} \\t Silhouette index: {silhouette_index} \\t AUC: {cl_auc} \\t Corrected count: {count}\")\n",
    "    select +=1 \n",
    "\n",
    "    data_f=pd.DataFrame.from_dict(data_frame)\n",
    "    data_f.to_csv(f\"../subclusters_csv/ct-scans/densenet169_cosine_dist.csv\",index=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
