{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os, math, random\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from statistics import mode\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n",
    "from sklearn.metrics import auc, roc_curve, davies_bouldin_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = 1 #int(input(\"Enter the number for: \\n 1) VGG16 \\n 2) Resnet101  \\n 3) Densenet169 \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define DATASET_SIZE (max: 1, min: 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open extracted feature in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../pickle_files/al/x_ray/\"\n",
    "if selected_model == 1:\n",
    "  filename = \"x_ray_pca_vgg16.pickle\"\n",
    "elif selected_model == 2:\n",
    "  filename = \"x_ray_pca_resnet101.pickle\"\n",
    "elif selected_model == 3:\n",
    "  filename = \"x_ray_pca_densenet169.pickle\"\n",
    "\n",
    "file = filepath + filename\n",
    "with open(file, 'rb') as handle:\n",
    "  all_ft_dataset = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the data and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset: 4400\n",
      "Sample feature dataset Covid: {'id': 3716, 'filepath': '../../dataset/xray/resized\\\\Covid\\\\MIDRC-RICORD-1C-419639-000906-14379-0.png', 'image': array([-3.92914429e+02, -1.63180954e+02,  6.10871468e+01, ...,\n",
      "        3.14289234e-10,  1.02662885e-10,  1.25120081e-10], dtype=float32), 'label': 1}\n",
      "Sample feature dataset Non-Covid: {'id': 1910, 'filepath': '../../dataset/xray/resized\\\\Noncovid\\\\PNEUMONIA(393).jpg', 'image': array([ 1.10929192e+02, -2.91740017e+01, -4.43288635e+02, ...,\n",
      "        3.12765036e-10,  1.02398943e-10,  1.25293845e-10], dtype=float32), 'label': 0}\n",
      "Selected Dataset: 4400\n"
     ]
    }
   ],
   "source": [
    "# suffle the data\n",
    "random.seed(42)\n",
    "shuffle(all_ft_dataset)\n",
    "\n",
    "print(\"Total Dataset: {}\".format(len(all_ft_dataset)))\n",
    "print(\"Sample feature dataset Covid: {}\".format(all_ft_dataset[0]))\n",
    "print(\"Sample feature dataset Non-Covid: {}\".format(all_ft_dataset[2200]))\n",
    "\n",
    "# shrink for minimize training time\n",
    "original_data_size = len(all_ft_dataset)\n",
    "new_data_size = int(original_data_size * DATASET_SIZE)\n",
    "ft_dataset = all_ft_dataset[:new_data_size]\n",
    "\n",
    "print(\"Selected Dataset: {}\".format(len(ft_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the mean of each subclusters. (required as mean is the representative of that subcluster)\n",
    "def mean_features(c_neg_features, c_pos_features):\n",
    "  mneg_features = np.array([np.mean(i, axis=0) for i in c_neg_features])  # Mean of all negative subclusters\n",
    "  mpos_features = np.array([np.mean(i, axis=0) for i in c_pos_features])  # Mean of all positive subclusters\n",
    "  return mneg_features, mpos_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update subcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that updates the subcluster by concatenating the test data sample to the most similar subcluster.\n",
    "def update_subclusters(query, closest_labels_from_model, id_pred, label_pred, n_neighbours, c_features, distances_of_data_to_cc, cluster_index):\n",
    "  nearest_subcluster_index = np.argmin(distances_of_data_to_cc) # find nearest subcluster of the query\n",
    "  # add query data to the nearest/most-similar subcluster\n",
    "  c_features[nearest_subcluster_index] = np.concatenate((c_features[nearest_subcluster_index], np.expand_dims(query[\"image\"], axis=0)), axis=0)\n",
    "\n",
    "  id_pred[cluster_index].append(query[\"id\"])\n",
    "  # closest_labels_from_model.count(1)/n_neighbours --> Percentage that the model predict the data as positive (required to calculate AUC ROC value)\n",
    "  label_pred[cluster_index].append((query['id'], closest_labels_from_model.count(1)/n_neighbours))\n",
    "  return c_features, id_pred, label_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the class (pos or neg) from max frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_with_max_freq(closest_labels_from_model):\n",
    "    return mode(closest_labels_from_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to correct mispredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the model predicted label with the ground truth and corrects only if it is a mistake\n",
    "def correct_mispredictions(query, closest_labels_from_model, c_neg_features, c_pos_features, distances_of_data_to_neg_cc, distances_of_data_to_pos_cc, data_frame_mistake, mistake_index, corrected_count):\n",
    "  if get_label_with_max_freq(closest_labels_from_model) != query[\"label\"]:  # Misclassification: if model's decision is different than the ground truth.\n",
    "    corrected_count += 1\n",
    "    data_frame_mistake[\"Image name\"].append(query[\"filepath\"].split(\"/\")[-1])  # Recording to save it as csv file\n",
    "    data_frame_mistake[\"Mistake ID\"].append(query['id'])\n",
    "    data_frame_mistake[\"Original label\"].append(query['label'])\n",
    "    data_frame_mistake[\"Predicted label\"].append(get_label_with_max_freq(closest_labels_from_model))\n",
    "    data_frame_mistake[\"Mistake index\"].append(mistake_index)\n",
    "    if query[\"label\"] == 0:\n",
    "      c_neg_features.append(np.expand_dims(query[\"image\"], axis=0))  # Appending negative image to negative cluster\n",
    "    else:\n",
    "      c_pos_features.append(np.expand_dims(query[\"image\"], axis=0))  # Appending positive image to positive cluster\n",
    "\n",
    "  else: # Correct classification by model: concatenating the feature to the closest subsample.\n",
    "    if query['label'] == 0:\n",
    "      nearest_subcluster_index = np.argmin(distances_of_data_to_neg_cc)  # find nearest subcluster of the query\n",
    "      # add query data to the nearest/most-similar subcluster\n",
    "      c_neg_features[nearest_subcluster_index] = np.concatenate((c_neg_features[nearest_subcluster_index], np.expand_dims(query[\"image\"], axis=0)), axis=0)\n",
    "    else:\n",
    "      nearest_subcluster_index = np.argmin(distances_of_data_to_pos_cc)  # find nearest subcluster of the query\n",
    "      # add query data to the nearest/most-similar subcluster\n",
    "      c_pos_features[nearest_subcluster_index] = np.concatenate((c_pos_features[nearest_subcluster_index], np.expand_dims(query[\"image\"], axis=0)), axis=0)\n",
    "\n",
    "  return corrected_count, data_frame_mistake, c_neg_features, c_pos_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calcuate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: raw dictionary (from pickle file)\n",
    "# cluster_centers_dict: dictionary of {0: [], 1:[]} ==> {0: cc_neg_features, 1: cc_pos_features}\n",
    "# distance_type: 1. Eucliddean, 2. Manhattan, 3. Cosine\n",
    "# label_pred: predicted label\n",
    "# c_neg_features\n",
    "# c_pos_features\n",
    "# n_neighbours: no. of neighbour\n",
    "# corrected_count: count of coorection of mispredictions\n",
    "# mistake_index: index of data to track the mistaken data\n",
    "# data_frame_mistake: to save data_frame in CSV\n",
    "# mentored_data: if mentored data or not\n",
    "def distance(query,\n",
    "             cluster_centers_dict,\n",
    "             distance_type, id_pred,\n",
    "             label_pred,\n",
    "             c_neg_features,\n",
    "             c_pos_features,\n",
    "             n_neighbours,\n",
    "             corrected_count,\n",
    "             mistake_index,\n",
    "             data_frame_mistake,\n",
    "             mentored_data):\n",
    "  expnd_query = np.expand_dims(query['image'], axis=0)\n",
    "  distances_of_data_to_neg_cc, distances_of_data_to_pos_cc = [], []\n",
    "\n",
    "  # len(cluster_centers_dict[0]) should have at least the number of sub-cluster\n",
    "  # Calculating the distance using numpy (axis=1) to calculate all at ones\n",
    "  if distance_type == 1: # Euclidean distance\n",
    "    if len(cluster_centers_dict[0]) > 1:\n",
    "      distances_of_data_to_neg_cc = np.linalg.norm(query['image'] - cluster_centers_dict[0], axis=1)\n",
    "    elif len(cluster_centers_dict[0]) == 1:\n",
    "      distances_of_data_to_neg_cc = [np.linalg.norm(query['image'] - cluster_centers_dict[0], axis=1)]\n",
    "    if len(cluster_centers_dict[1]) > 1:\n",
    "      distances_of_data_to_pos_cc = np.linalg.norm(query['image'] - cluster_centers_dict[1], axis=1)\n",
    "    elif len(cluster_centers_dict[1]) == 1:\n",
    "      distances_of_data_to_pos_cc = [np.linalg.norm(query['image'] - cluster_centers_dict[1], axis=1)]\n",
    "\n",
    "  elif distance_type == 2: # Manhattan distance\n",
    "    if len(cluster_centers_dict[0]) > 1:\n",
    "      distances_of_data_to_neg_cc = np.squeeze(manhattan_distances(cluster_centers_dict[0], expnd_query))\n",
    "    elif len(cluster_centers_dict[0]) == 1:\n",
    "      distances_of_data_to_neg_cc = [np.squeeze(manhattan_distances(cluster_centers_dict[0], expnd_query))]\n",
    "    if len(cluster_centers_dict[1]) > 1:\n",
    "      distances_of_data_to_pos_cc = np.squeeze(manhattan_distances(cluster_centers_dict[1], expnd_query))\n",
    "    elif len(cluster_centers_dict[1]) == 1:\n",
    "      distances_of_data_to_pos_cc = [np.squeeze(manhattan_distances(cluster_centers_dict[1], expnd_query))]\n",
    "\n",
    "  elif distance_type == 3: # Cosine distance\n",
    "    if len(cluster_centers_dict[0]) > 1:\n",
    "      distances_of_data_to_neg_cc = np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[0]))\n",
    "    elif len(cluster_centers_dict[0]) == 1:\n",
    "      distances_of_data_to_neg_cc = [np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[0]))]\n",
    "    if len(cluster_centers_dict[1]) > 1:\n",
    "      distances_of_data_to_pos_cc = np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[1]))\n",
    "    elif len(cluster_centers_dict[1]) == 1:\n",
    "      distances_of_data_to_pos_cc = [np.squeeze(cosine_distances(expnd_query, cluster_centers_dict[1]))]\n",
    "\n",
    "  neg_distances_tup_list, pos_ditances_tup_list = [], []\n",
    "\n",
    "  for dist_single in distances_of_data_to_neg_cc:\n",
    "    neg_distances_tup_list.append((dist_single, 0))\n",
    "\n",
    "  for dist_single in distances_of_data_to_pos_cc:\n",
    "    pos_ditances_tup_list.append((dist_single, 1))\n",
    "\n",
    "\n",
    "  # concat all distances\n",
    "  pos_ditances_tup_list.extend(neg_distances_tup_list)\n",
    "  # sort distances from min to max result: ((0.1, 1), (0.2, 1), (0.3, 0), (0.4, 0), (0.5, 1))\n",
    "  all_distances_tup = sorted(pos_ditances_tup_list)[:n_neighbours]\n",
    "\n",
    "  # filter only n_neighbours elements\n",
    "  # all_distances_tup = all_distances_tup[:n_neighbours]\n",
    "\n",
    "  closest_labels_from_model = [label for (distance, label) in all_distances_tup]\n",
    "\n",
    "  if mentored_data:\n",
    "    (corrected_count,\n",
    "     data_frame_mistake,\n",
    "     c_neg_features,\n",
    "     c_pos_features) = correct_mispredictions(query,\n",
    "                                              closest_labels_from_model,\n",
    "                                              c_neg_features,\n",
    "                                              c_pos_features,\n",
    "                                              distances_of_data_to_neg_cc,\n",
    "                                              distances_of_data_to_pos_cc,\n",
    "                                              data_frame_mistake,\n",
    "                                              mistake_index,\n",
    "                                              corrected_count\n",
    "                                              )\n",
    "\n",
    "  else:\n",
    "    # label from model is negative\n",
    "    if len(closest_labels_from_model) > 0 and get_label_with_max_freq(closest_labels_from_model) == 0:\n",
    "      c_neg_features, id_pred, label_pred = update_subclusters(query,\n",
    "                                                               closest_labels_from_model,\n",
    "                                                               id_pred,\n",
    "                                                               label_pred,\n",
    "                                                               n_neighbours,\n",
    "                                                               # neg params\n",
    "                                                               c_neg_features,\n",
    "                                                               distances_of_data_to_neg_cc,\n",
    "                                                               cluster_index=0\n",
    "                                                              )\n",
    "    else:\n",
    "      # label from model is positive\n",
    "      c_pos_features, id_pred, label_pred = update_subclusters(query,\n",
    "                                                               closest_labels_from_model,\n",
    "                                                               id_pred,\n",
    "                                                               label_pred,\n",
    "                                                               n_neighbours,\n",
    "                                                               # pos params\n",
    "                                                               c_pos_features,\n",
    "                                                               distances_of_data_to_pos_cc,\n",
    "                                                               cluster_index=1\n",
    "                                                              )\n",
    "\n",
    "  return data_frame_mistake, corrected_count, id_pred, label_pred, c_neg_features, c_pos_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate the classification metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrices(id_gt, id_pred):\n",
    "  TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "  # TP --> when correctly classified covid\n",
    "  for tp in id_pred[1]:\n",
    "    if tp in id_gt[1]:\n",
    "      TP += 1\n",
    "\n",
    "  # TN --> when correctly classified healthy (non-covid)\n",
    "  for tn in id_pred[0]:\n",
    "    if tn in id_gt[0]:\n",
    "      TN += 1\n",
    "\n",
    "  # FP --> when incorrectly classified healthy (Classified healthy as covid)\n",
    "  for fp in id_pred[1]:\n",
    "    if fp in id_gt[0]:\n",
    "      FP += 1\n",
    "\n",
    "  # FN --> when missed covid classification (Covid cases missed)\n",
    "  for fn in id_pred[0]:\n",
    "    if fn in id_gt[1]:\n",
    "      FN += 1\n",
    "\n",
    "  accuracy = round((TP + TN) / (TP + TN + FP + FN), 3)\n",
    "  if (TN + FP) > 0:\n",
    "    specificity = round(TN / (TN + FP), 3)\n",
    "  else:\n",
    "    specificity = 0 # Infinity\n",
    "\n",
    "  if (TP + FN) > 0:\n",
    "    sensitivity = round((TP) / (TP + FN), 3)\n",
    "  else:\n",
    "    sensitivity = 0 # Infinity\n",
    "\n",
    "  # f1_score = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "  print(\"TP: {}  FP: {}\".format(TP, FP))\n",
    "  print(\"FN: {}  TN: {}\".format(FN, TN))\n",
    "\n",
    "  return accuracy, specificity, sensitivity, TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_curve(label_gt, label_pred):\n",
    "  # contains (id, labels) tuple of binary class\n",
    "  gt_labels = sorted(label_gt[0] + label_gt[1])\n",
    "\n",
    "  # contains (id, labels) tuple of binary class --> sorted to match each element in gt_labels and pred_labels\n",
    "  pred_labels = sorted(label_pred[0] + label_pred[1])\n",
    "  y_test = [y for (x,y) in gt_labels] # Get only the labels\n",
    "  y_scores = [y for (x,y) in pred_labels]\n",
    "  fpr, tpr, threshold = roc_curve(y_test, y_scores)\n",
    "  roc_auc = round(auc(fpr, tpr), 3)\n",
    "  return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to calculate Cluster metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_metrices(neg_features, pos_features):\n",
    "  print(\"Calculating Dunn's index...\")\n",
    "  dunn_index, davies_bouldin_index, silhouette_index = \"NA\", \"NA\", \"NA\"\n",
    "  if len(neg_features) > 0 and len(pos_features) > 0:\n",
    "    intra_dist1 = euclidean_distances(neg_features).max()\n",
    "    intra_dist2 = euclidean_distances(pos_features).max()\n",
    "    inter_dist = euclidean_distances(neg_features, pos_features).min()\n",
    "\n",
    "    if intra_dist1 > intra_dist2:\n",
    "      max_intra_dist= intra_dist1\n",
    "    else:\n",
    "      max_intra_dist = intra_dist2\n",
    "\n",
    "    dunn_index = round(inter_dist / max_intra_dist, 3)\n",
    "\n",
    "  print(\"dunn_index: \", dunn_index)\n",
    "\n",
    "  # Davies Bouldin and Silhouette score from sklearn library.\n",
    "  if len(neg_features) > 0 and len(pos_features) > 0:\n",
    "    feature_all = np.concatenate((neg_features, pos_features))\n",
    "    neg_labels = np.zeros(shape=(len(neg_features)),dtype=int)\n",
    "    pos_labels = np.ones(shape=(len(pos_features)),dtype=int)\n",
    "\n",
    "    label_all = np.concatenate((neg_labels, pos_labels))\n",
    "    print(\"Calculating Davies Bouldin index...\")\n",
    "    davies_bouldin_index = round(davies_bouldin_score(feature_all, label_all), 3)\n",
    "    print(\"davies_bouldin_index: \", davies_bouldin_index)\n",
    "\n",
    "    print(\"Calculating Silhouette index...\")\n",
    "    silhouette_index = round(silhouette_score(feature_all, label_all), 3)\n",
    "    print(\"silhouette_index: \", silhouette_index)\n",
    "\n",
    "  return dunn_index, davies_bouldin_index, silhouette_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to flatter the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to calculate the clustering indices\n",
    "def flatten_features(features):\n",
    "  all_features = []\n",
    "  for feature in features:\n",
    "    for index in feature:\n",
    "      all_features.append(index)\n",
    "  return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to find sub cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create subclusters\n",
    "def sub_clusters(features, n_clusters=5):\n",
    "  # Number of cluster defined from elbow method\n",
    "  # kmeans = KMeans(n_clusters=int(70*DATASET_SIZE), random_state=0, n_init=\"auto\").fit(features)\n",
    "  kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(features)\n",
    "  # list of labels for elements occuring in each cluster\n",
    "  out_labels = kmeans.labels_\n",
    "  # Form clusters of deep features of image\n",
    "  # clusters = [np.squeeze(np.array(features)[[np.where(out_labels == i)[0]]], axis=0) for i in range(len(np.unique(out_labels)))]\n",
    "  clusters = [np.array(features)[np.where(out_labels == i)[0]] for i in range(len(np.unique(out_labels)))]\n",
    "  return kmeans.cluster_centers_, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to load dataset into three different segment (k-way n-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to return three sets (n, 1500, 3000)  of labeled dataset for experiment\n",
    "def data_loader(dataset, n):\n",
    "  labeled_data, unlabeled_data = [], []\n",
    "\n",
    "  l_data = dataset[:n]                          # First case (0-40) // labeled + mentored\n",
    "  ul_data = dataset[n:]                         # First case (40-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[0] => dataset[0-40]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[0] => dataset[40-1000]\n",
    "\n",
    "  size_second_set = int(1500 * DATASET_SIZE)    # 1500 * 0.1 = 150\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Second case (150-190) // labeled + mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Second case (0-150) + (190-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[1] => dataset[150-190]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[1] => dataset[0-150] + dataset[190-1000]\n",
    "\n",
    "  size_second_set = int(3000 * DATASET_SIZE)    # 3000 * 0.1 = 300\n",
    "  sss = size_second_set\n",
    "  l_data = dataset[sss: n + sss]                # Third case (300-340) // labeled + mentored\n",
    "  ul_data = dataset[:sss] + dataset[n + sss:]   # Third case (0-300) + (340-1000) // unlabeled\n",
    "  labeled_data.append(l_data)                   # labeled_data[2] => dataset[300-340]\n",
    "  unlabeled_data.append(ul_data)                # unlabeled_data[2] => dataset[0-300] + dataset[340-1000]\n",
    "  return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to separate data into positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that selects number of data samples and removes the selected data from the dataset.\n",
    "# (required to select balanced positive and negative samples)\n",
    "def data_separation(dataset, label, data_sample=100):\n",
    "  add_data = []\n",
    "  for i, data in enumerate(dataset):\n",
    "    if data[\"label\"] == label:\n",
    "      add_data.append(data['image'])\n",
    "      del dataset[i]\n",
    "    if len(add_data) == data_sample:\n",
    "      break\n",
    "\n",
    "  return add_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance type (Euclidean Manhattan or Consine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_type = 3 # int(input(\"Enter the number for: \\n 1) Euclidean  \\n 2) Manhattan \\n 3) Cosine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Distance Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model == 1:\n",
    "  s_model = 'vgg16'\n",
    "elif selected_model == 2:\n",
    "  s_model = 'resnet101'\n",
    "elif selected_model == 3:\n",
    "  s_model = 'densenet169'\n",
    "\n",
    "if distance_type == 1:\n",
    "  s_distance = 'euclidean'\n",
    "elif distance_type == 2:\n",
    "  s_distance = 'manhattan'\n",
    "elif distance_type == 3:\n",
    "  s_distance = 'cosine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** training with 200 size of labled data***************\n",
      "\n",
      "\n",
      "============================== 1/12 ==============================\n",
      "labeled data: 200, unlabled data: 4200\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 160\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 160 data ###############\n",
      " ############### Mentoring 160 data DONE!!! ###############\n",
      " ############### Training 4200 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4200/4200 [00:25<00:00, 166.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4200 unlabeled data DONE!!! ###############\n",
      "TP: 2006  FP: 150\n",
      "FN: 33  TN: 2011\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.0\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.626\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.119\n",
      "Dataset: d_1 \t\t\t Labeled image: 200 \t\t Corrected count: 9\n",
      "Accuracy: 0.956 \t\t Specificity: 0.931 \t\t Sensitivity: 0.984\n",
      "Dunn index: 0.0\n",
      "Davies Bouldin Index: 2.626\n",
      "Silhouette Index: 0.11900000274181366\n",
      "AUC: 0.963\n",
      "\n",
      "\n",
      "============================== 2/12 ==============================\n",
      "labeled data: 200, unlabled data: 4200\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 160\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 160 data ###############\n",
      " ############### Mentoring 160 data DONE!!! ###############\n",
      " ############### Training 4200 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4200/4200 [00:25<00:00, 161.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4200 unlabeled data DONE!!! ###############\n",
      "TP: 2003  FP: 261\n",
      "FN: 16  TN: 1920\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.111\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.598\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.125\n",
      "Dataset: d_2 \t\t\t Labeled image: 200 \t\t Corrected count: 7\n",
      "Accuracy: 0.934 \t\t Specificity: 0.88 \t\t Sensitivity: 0.992\n",
      "Dunn index: 0.11100000143051147\n",
      "Davies Bouldin Index: 2.598\n",
      "Silhouette Index: 0.125\n",
      "AUC: 0.937\n",
      "\n",
      "\n",
      "============================== 3/12 ==============================\n",
      "labeled data: 200, unlabled data: 4200\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 160\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 160 data ###############\n",
      " ############### Mentoring 160 data DONE!!! ###############\n",
      " ############### Training 4200 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4200/4200 [00:25<00:00, 164.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4200 unlabeled data DONE!!! ###############\n",
      "TP: 1990  FP: 167\n",
      "FN: 41  TN: 2002\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.0\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.626\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.12\n",
      "Dataset: d_3 \t\t\t Labeled image: 200 \t\t Corrected count: 10\n",
      "Accuracy: 0.95 \t\t Specificity: 0.923 \t\t Sensitivity: 0.98\n",
      "Dunn index: 0.0\n",
      "Davies Bouldin Index: 2.626\n",
      "Silhouette Index: 0.11999999731779099\n",
      "AUC: 0.959\n",
      "*************** training with 400 size of labled data***************\n",
      "\n",
      "\n",
      "============================== 4/12 ==============================\n",
      "labeled data: 400, unlabled data: 4000\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 360\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 360 data ###############\n",
      " ############### Mentoring 360 data DONE!!! ###############\n",
      " ############### Training 4000 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:26<00:00, 153.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4000 unlabeled data DONE!!! ###############\n",
      "TP: 1912  FP: 101\n",
      "FN: 22  TN: 1965\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.0\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.645\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.117\n",
      "Dataset: d_1 \t\t\t Labeled image: 400 \t\t Corrected count: 16\n",
      "Accuracy: 0.969 \t\t Specificity: 0.951 \t\t Sensitivity: 0.989\n",
      "Dunn index: 0.0\n",
      "Davies Bouldin Index: 2.645\n",
      "Silhouette Index: 0.11699999868869781\n",
      "AUC: 0.993\n",
      "\n",
      "\n",
      "============================== 5/12 ==============================\n",
      "labeled data: 400, unlabled data: 4000\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 360\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 360 data ###############\n",
      " ############### Mentoring 360 data DONE!!! ###############\n",
      " ############### Training 4000 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:26<00:00, 152.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4000 unlabeled data DONE!!! ###############\n",
      "TP: 1882  FP: 128\n",
      "FN: 37  TN: 1953\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.028\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.647\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.117\n",
      "Dataset: d_2 \t\t\t Labeled image: 400 \t\t Corrected count: 12\n",
      "Accuracy: 0.959 \t\t Specificity: 0.938 \t\t Sensitivity: 0.981\n",
      "Dunn index: 0.02800000086426735\n",
      "Davies Bouldin Index: 2.647\n",
      "Silhouette Index: 0.11699999868869781\n",
      "AUC: 0.989\n",
      "\n",
      "\n",
      "============================== 6/12 ==============================\n",
      "labeled data: 400, unlabled data: 4000\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 360\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 360 data ###############\n",
      " ############### Mentoring 360 data DONE!!! ###############\n",
      " ############### Training 4000 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:25<00:00, 156.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 4000 unlabeled data DONE!!! ###############\n",
      "TP: 1909  FP: 130\n",
      "FN: 20  TN: 1941\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.0\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.635\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.118\n",
      "Dataset: d_3 \t\t\t Labeled image: 400 \t\t Corrected count: 17\n",
      "Accuracy: 0.963 \t\t Specificity: 0.937 \t\t Sensitivity: 0.99\n",
      "Dunn index: 0.0\n",
      "Davies Bouldin Index: 2.635\n",
      "Silhouette Index: 0.11800000071525574\n",
      "AUC: 0.993\n",
      "*************** training with 800 size of labled data***************\n",
      "\n",
      "\n",
      "============================== 7/12 ==============================\n",
      "labeled data: 800, unlabled data: 3600\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 760\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 760 data ###############\n",
      " ############### Mentoring 760 data DONE!!! ###############\n",
      " ############### Training 3600 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3600/3600 [00:25<00:00, 141.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 3600 unlabeled data DONE!!! ###############\n",
      "TP: 1713  FP: 43\n",
      "FN: 34  TN: 1810\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.111\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.667\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.113\n",
      "Dataset: d_1 \t\t\t Labeled image: 800 \t\t Corrected count: 25\n",
      "Accuracy: 0.979 \t\t Specificity: 0.977 \t\t Sensitivity: 0.981\n",
      "Dunn index: 0.11100000143051147\n",
      "Davies Bouldin Index: 2.667\n",
      "Silhouette Index: 0.11299999803304672\n",
      "AUC: 0.996\n",
      "\n",
      "\n",
      "============================== 8/12 ==============================\n",
      "labeled data: 800, unlabled data: 3600\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 760\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 760 data ###############\n",
      " ############### Mentoring 760 data DONE!!! ###############\n",
      " ############### Training 3600 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3600/3600 [00:26<00:00, 137.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 3600 unlabeled data DONE!!! ###############\n",
      "TP: 1706  FP: 36\n",
      "FN: 35  TN: 1823\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.035\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.661\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.113\n",
      "Dataset: d_2 \t\t\t Labeled image: 800 \t\t Corrected count: 23\n",
      "Accuracy: 0.98 \t\t Specificity: 0.981 \t\t Sensitivity: 0.98\n",
      "Dunn index: 0.03500000014901161\n",
      "Davies Bouldin Index: 2.661\n",
      "Silhouette Index: 0.11299999803304672\n",
      "AUC: 0.997\n",
      "\n",
      "\n",
      "============================== 9/12 ==============================\n",
      "labeled data: 800, unlabled data: 3600\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 760\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 760 data ###############\n",
      " ############### Mentoring 760 data DONE!!! ###############\n",
      " ############### Training 3600 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3600/3600 [00:25<00:00, 141.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 3600 unlabeled data DONE!!! ###############\n",
      "TP: 1711  FP: 59\n",
      "FN: 17  TN: 1813\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.0\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.657\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.115\n",
      "Dataset: d_3 \t\t\t Labeled image: 800 \t\t Corrected count: 25\n",
      "Accuracy: 0.979 \t\t Specificity: 0.968 \t\t Sensitivity: 0.99\n",
      "Dunn index: 0.0\n",
      "Davies Bouldin Index: 2.657\n",
      "Silhouette Index: 0.11500000208616257\n",
      "AUC: 0.996\n",
      "*************** training with 1550 size of labled data***************\n",
      "\n",
      "\n",
      "============================== 10/12 ==============================\n",
      "labeled data: 1550, unlabled data: 2850\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 1510\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 1510 data ###############\n",
      " ############### Mentoring 1510 data DONE!!! ###############\n",
      " ############### Training 2850 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2850/2850 [00:23<00:00, 119.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 2850 unlabeled data DONE!!! ###############\n",
      "TP: 1362  FP: 19\n",
      "FN: 18  TN: 1451\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.0\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.668\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.112\n",
      "Dataset: d_1 \t\t\t Labeled image: 1550 \t\t Corrected count: 46\n",
      "Accuracy: 0.987 \t\t Specificity: 0.987 \t\t Sensitivity: 0.987\n",
      "Dunn index: 0.0\n",
      "Davies Bouldin Index: 2.668\n",
      "Silhouette Index: 0.1120000034570694\n",
      "AUC: 0.998\n",
      "\n",
      "\n",
      "============================== 11/12 ==============================\n",
      "labeled data: 1550, unlabled data: 2850\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 1510\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 1510 data ###############\n",
      " ############### Mentoring 1510 data DONE!!! ###############\n",
      " ############### Training 2850 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2850/2850 [00:24<00:00, 118.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 2850 unlabeled data DONE!!! ###############\n",
      "TP: 1365  FP: 31\n",
      "FN: 14  TN: 1440\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.084\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.682\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.112\n",
      "Dataset: d_2 \t\t\t Labeled image: 1550 \t\t Corrected count: 37\n",
      "Accuracy: 0.984 \t\t Specificity: 0.979 \t\t Sensitivity: 0.99\n",
      "Dunn index: 0.08399999886751175\n",
      "Davies Bouldin Index: 2.682\n",
      "Silhouette Index: 0.1120000034570694\n",
      "AUC: 0.997\n",
      "\n",
      "\n",
      "============================== 12/12 ==============================\n",
      "labeled data: 1400, unlabled data: 3000\n",
      "balanced data: 40, fpositive: 20, fnegative: 20\n",
      "mentored data: 1360\n",
      "Number of subclusters: 5\n",
      " ############### Mentoring 1360 data ###############\n",
      " ############### Mentoring 1360 data DONE!!! ###############\n",
      " ############### Training 3000 unlabeled data ###############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:23<00:00, 125.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ############### Training 3000 unlabeled data DONE!!! ###############\n",
      "TP: 1420  FP: 41\n",
      "FN: 21  TN: 1518\n",
      "Calculating Dunn's index...\n",
      "dunn_index:  0.0\n",
      "Calculating Davies Bouldin index...\n",
      "davies_bouldin_index:  2.659\n",
      "Calculating Silhouette index...\n",
      "silhouette_index:  0.114\n",
      "Dataset: d_3 \t\t\t Labeled image: 1550 \t\t Corrected count: 34\n",
      "Accuracy: 0.979 \t\t Specificity: 0.974 \t\t Sensitivity: 0.985\n",
      "Dunn index: 0.0\n",
      "Davies Bouldin Index: 2.659\n",
      "Silhouette Index: 0.11400000005960464\n",
      "AUC: 0.998\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME =  str(round(datetime.datetime.now().timestamp()))\n",
    "os.mkdir(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}\")\n",
    "n_neighbours = int(15 * DATASET_SIZE)\n",
    "labeled_size = [200, 400, 800, 1550]\n",
    "labeled_size = [int(size * DATASET_SIZE) for size in labeled_size]\n",
    "data_frame_metrix = {\n",
    "  \"Dataset\": [],\n",
    "  \"Labeled Data\": [],\n",
    "  \"Unlabeled Data\": [],\n",
    "  \"Accuracy\": [],\n",
    "  \"Specificity\": [],\n",
    "  \"Sensitivity\": [],\n",
    "  \"AUC\": [],\n",
    "  \"Dunn index\": [],\n",
    "  \"Davies Bouldin Index\": [],\n",
    "  \"Silhouette Index\": [],\n",
    "  \"TP\": [],\n",
    "  \"TN\": [],\n",
    "  \"FP\": [],\n",
    "  \"FN\": [],\n",
    "  \"Labeled Covid+ve\": [],\n",
    "  \"Labeled Covid-ve\": [],\n",
    "  \"Corrected Count\": []\n",
    "}\n",
    "\n",
    "global_count = 0\n",
    "\n",
    "for size in labeled_size:\n",
    "  print(\"{} training with {} size of labled data{}\".format('*'*15, size, '*'*15))\n",
    "  labeled_data_sets, unlabeled_data_sets = data_loader(ft_dataset, size)\n",
    "\n",
    "  # labeled_data_sets ==> three sets: [d1, d2, d3] ==> eg: [0-40, 320-360, 640-680]\n",
    "  for dataset_type, labeled_data in enumerate(labeled_data_sets):\n",
    "    global_count += 1\n",
    "    print(f\"\\n\\n============================== {global_count}/{len(labeled_size) * len(labeled_data_sets)} ==============================\")\n",
    "    data_frame_mistake = {\n",
    "      \"Image name\": [],\n",
    "      \"Mistake index\": [],\n",
    "      \"Mistake ID\": [],\n",
    "      \"Original label\": [],\n",
    "      \"Predicted label\": []\n",
    "    }\n",
    "\n",
    "    neg_img, pos_img = 0, 0\n",
    "\n",
    "    # collect the ground truth (label) of all the predicting images =>> key: 0 & 1 (class), value: tuple (data['id'], data['label']), required to calulate TP, FP, FN, TN\n",
    "    label_gt = {0: [], 1: []}\n",
    "    # collect the ground truth (id) of all the predicting images =>> key: 0 & 1 (class), value: ground truth id\n",
    "    id_gt = {0: [], 1: []}\n",
    "\n",
    "    # collect the predicted label for all the images =>> key: 0 & 1 (class), value: tuple(query['id'], decision_list.count(1)/n_neighbours)\n",
    "    # Percentage of predicted positive class, required to calculate AUC/ROC value\n",
    "    label_pred = {0: [], 1: []}\n",
    "    # collect the predicted id for all the images =>> key: 0 & 1 (class), value: predicted ids, required to calulate TP, FP, FN, TN\n",
    "    id_pred = {0: [],  1: []}\n",
    "\n",
    "    # feature label =>> key: 0 & 1 (class), value: deep feature of image\n",
    "    cluster_centers_dict = {0: [], 1: []}\n",
    "\n",
    "    print(f\"labeled data: {len(labeled_data)}, unlabled data: {len(unlabeled_data_sets[dataset_type])}\")\n",
    "\n",
    "    neg_labeled_img, pos_labeled_img = 0, 0\n",
    "    for data in labeled_data:\n",
    "        if data['label'] == 0:\n",
    "            neg_labeled_img += 1\n",
    "        else:\n",
    "            pos_labeled_img += 1\n",
    "\n",
    "    # select balanced labeled data (50% from positive and 50% from negative)\n",
    "    sample_size = int(20 * DATASET_SIZE) # sample size of balanced_data\n",
    "    fpositive = data_separation(labeled_data, 1, sample_size)  # Get the 'sample_size' positive features from 'labeled_data'\n",
    "    fnegative = data_separation(labeled_data, 0, sample_size)  # Get the 'sample_size' negative features from 'labeled_data'\n",
    "\n",
    "    print(f\"balanced data: {2 * sample_size}, fpositive: {len(fpositive)}, fnegative: {len(fnegative)}\")\n",
    "    print(f\"mentored data: {len(labeled_data)}\")\n",
    "\n",
    "    n_sub_clusters = math.ceil(5 * DATASET_SIZE) if DATASET_SIZE > 0.5 else 2\n",
    "    print(\"Number of subclusters: {}\".format(n_sub_clusters))\n",
    "    cc_neg_features, c_neg_features = sub_clusters(fnegative, n_sub_clusters)  # Get the cluster center and negative clusters (Using K-means algorithm)\n",
    "    cc_pos_features, c_pos_features = sub_clusters(fpositive, n_sub_clusters)  # Get the cluster center and positive clusters (Using K-means algorithm)\n",
    "\n",
    "    corrected_count, mistake_index = 0, 2 * sample_size\n",
    "\n",
    "    print(f\" {'#' * 15} Mentoring {len(labeled_data)} data {'#' * 15}\")\n",
    "    # loop is for the mentored data --> Notice mentored_data=True in argument of the function call distance.\n",
    "    for data in labeled_data:\n",
    "      cluster_centers_dict = {0: cc_neg_features, 1: cc_pos_features}\n",
    "\n",
    "      (data_frame_mistake,\n",
    "       corrected_count,\n",
    "       _,\n",
    "       label_pred,\n",
    "       c_neg_features,\n",
    "       c_pos_features) = distance(data,\n",
    "                                 cluster_centers_dict,\n",
    "                                 distance_type,\n",
    "                                 id_pred, # not being used in this case\n",
    "                                 label_pred,\n",
    "                                 c_neg_features,\n",
    "                                 c_pos_features,\n",
    "                                 n_neighbours,\n",
    "                                 corrected_count,\n",
    "                                 mistake_index,\n",
    "                                 data_frame_mistake,\n",
    "                                 mentored_data=True)\n",
    "\n",
    "      cc_neg_features, cc_pos_features = mean_features(c_neg_features, c_pos_features)  # Get the mean of the features\n",
    "      mistake_index += 1\n",
    "\n",
    "    print(f\" {'#' * 15} Mentoring {len(labeled_data)} data DONE!!! {'#' * 15}\")\n",
    "\n",
    "    data_f_mistake = pd.DataFrame.from_dict(data_frame_mistake)\n",
    "    data_f_mistake.to_csv(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}/mistake_{size}_d{dataset_type + 1}.csv\", index=False)\n",
    "\n",
    "    print(f\" {'#' * 15} Training {len(unlabeled_data_sets[dataset_type])} unlabeled data {'#' * 15}\")\n",
    "    # loop is for the test data --> Notice mentored_data=False in argument of the function call distance.\n",
    "    for data in tqdm(unlabeled_data_sets[dataset_type]):\n",
    "      if data[\"label\"] == 1:\n",
    "        id_gt[1].append(data['id'])\n",
    "        label_gt[1].append((data['id'], data['label'])) # Required to calulate TP, FP, FN, TN\n",
    "      else:\n",
    "        id_gt[0].append(data['id'])\n",
    "        label_gt[0].append((data['id'], data['label']))\n",
    "\n",
    "      cluster_centers_dict = {0: cc_neg_features, 1: cc_pos_features}\n",
    "\n",
    "      (_,\n",
    "       _,\n",
    "       id_pred,\n",
    "       label_pred,\n",
    "       c_neg_features,\n",
    "       c_pos_features) = distance(data,\n",
    "                                 cluster_centers_dict,\n",
    "                                 distance_type,\n",
    "                                 id_pred,\n",
    "                                 label_pred,\n",
    "                                 c_neg_features,\n",
    "                                 c_pos_features,\n",
    "                                 n_neighbours,\n",
    "                                 corrected_count, # not being used in this case\n",
    "                                 mistake_index,\n",
    "                                 data_frame_mistake,\n",
    "                                 mentored_data=False)\n",
    "\n",
    "      cc_neg_features, cc_pos_features = mean_features(c_neg_features, c_pos_features)   # Get the mean of the features\n",
    "\n",
    "    print(f\" {'#' * 15} Training {len(unlabeled_data_sets[dataset_type])} unlabeled data DONE!!! {'#' * 15}\")\n",
    "\n",
    "    accuracy, specificity, sensitivity, TP, TN, FP, FN = classification_metrices(id_gt, id_pred)\n",
    "\n",
    "    # Flattened as required to calculate clustering indices\n",
    "    flattened_neg_features = flatten_features(c_neg_features)\n",
    "    flattened_pos_features = flatten_features(c_pos_features)\n",
    "\n",
    "    dunn_index, davies_bouldin_index, silhouette_index = cluster_metrices(flattened_neg_features, flattened_pos_features)\n",
    "    cl_auc = roc_auc_curve(label_gt, label_pred)\n",
    "\n",
    "    data_frame_metrix[\"Dataset\"].append(f\"d_{dataset_type + 1}\")\n",
    "    data_frame_metrix[\"Labeled Data\"].append(size)\n",
    "    data_frame_metrix[\"Unlabeled Data\"].append(len(unlabeled_data_sets[dataset_type]))\n",
    "    data_frame_metrix[\"Accuracy\"].append(accuracy)\n",
    "    data_frame_metrix[\"Specificity\"].append(specificity)\n",
    "    data_frame_metrix[\"Sensitivity\"].append(sensitivity)\n",
    "    data_frame_metrix[\"AUC\"].append(cl_auc)\n",
    "    data_frame_metrix[\"Dunn index\"].append(dunn_index)\n",
    "    data_frame_metrix[\"Davies Bouldin Index\"].append(davies_bouldin_index)\n",
    "    data_frame_metrix[\"Silhouette Index\"].append(silhouette_index)\n",
    "    data_frame_metrix[\"TP\"].append(TP)\n",
    "    data_frame_metrix[\"TN\"].append(TN)\n",
    "    data_frame_metrix[\"FP\"].append(FP)\n",
    "    data_frame_metrix[\"FN\"].append(FN)\n",
    "    data_frame_metrix[\"Labeled Covid-ve\"].append(neg_labeled_img)\n",
    "    data_frame_metrix[\"Labeled Covid+ve\"].append(pos_labeled_img)\n",
    "    data_frame_metrix[\"Corrected Count\"].append(corrected_count)\n",
    "\n",
    "    print(f\"Dataset: d_{dataset_type + 1} \\t\\t\\t Labeled image: {size} \\t\\t Corrected count: {corrected_count}\")\n",
    "    print(f\"Accuracy: {accuracy} \\t\\t Specificity: {specificity} \\t\\t Sensitivity: {sensitivity}\")\n",
    "    print(f\"Dunn index: {dunn_index}\")\n",
    "    print(f\"Davies Bouldin Index: {davies_bouldin_index}\")\n",
    "    print(f\"Silhouette Index: {silhouette_index}\")\n",
    "    print(f\"AUC: {cl_auc}\")\n",
    "\n",
    "data_f_matrix = pd.DataFrame.from_dict(data_frame_metrix)\n",
    "data_f_matrix.to_csv(f\"./test_{s_model}_{s_distance}_{CURRENT_TIME}/model_evaluation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
